<!DOCTYPE html>
<html lang="zh">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>DP-600 198题 在线刷题系统（完整版）</title>
<style>
:root {
  --bg:#f6f7f9; --card:#fff; --text:#111; --muted:#666; --border:#e6e6e6;
  --primary:#0078d4;
}
* { box-sizing:border-box; }
body { margin:0; font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Arial; background:var(--bg); color:var(--text);}
header { position:sticky; top:0; z-index:10; background:#111; color:#fff; padding:12px 12px; }
.header-row { display:flex; gap:10px; align-items:center; justify-content:space-between; flex-wrap:wrap; }
.badge { background:#222; border:1px solid #333; padding:6px 10px; border-radius:999px; font-size:14px; }
.container { max-width:980px; margin:0 auto; padding:14px; }
.card { background:var(--card); border:1px solid var(--border); border-radius:12px; padding:14px; }
.row { display:flex; gap:10px; flex-wrap:wrap; }
button { border:0; border-radius:10px; padding:10px 12px; font-size:16px; }
button.primary { background:var(--primary); color:#fff; }
button.secondary { background:#eee; }
button.ghost { background:transparent; color:#fff; border:1px solid #444; }
button:disabled { opacity:.5; }
.meta { color:var(--muted); font-size:14px; }
.qtitle { margin:0 0 6px 0; font-size:18px; }
.stem { white-space:pre-wrap; line-height:1.45; }
.options { margin-top:10px; display:flex; flex-direction:column; gap:8px; }
.opt { display:flex; align-items:flex-start; gap:10px; padding:10px; border:1px solid var(--border); border-radius:10px; background:#fff; }
.opt input { margin-top:3px; transform:scale(1.1); }
hr { border:0; border-top:1px solid var(--border); margin:14px 0; }
.learn { display:none; }
.learn .answer { font-weight:700; }
.learn .explain { white-space:pre-wrap; color:#1f3b1f; }
.learn .ref { white-space:pre-wrap; color:#0b5394; }
.progress { height:8px; background:#2a2a2a; border-radius:999px; overflow:hidden; flex:1; min-width:160px; }
.progress > div { height:100%; background:var(--primary); width:0%; }
.small { font-size:13px; color:var(--muted); }
input[type="text"], textarea, select { width:100%; padding:10px; border:1px solid var(--border); border-radius:10px; font-size:16px; }
textarea { min-height:90px; resize:vertical; }
.grid { display:grid; grid-template-columns:1fr; gap:10px; }
@media (min-width: 700px) { .grid.two { grid-template-columns:1fr 1fr; } }

.qimg-wrap{ margin-top:12px; }
.qimg{ max-width:100%; border:1px solid var(--border); border-radius:10px; cursor:zoom-in; }
.qimg-note{ margin-top:6px; }

</style>
</head>
<body>
<header>
  <div class="header-row">
    <div class="badge">DP-600 · 198题</div>
    <div class="badge">剩余时间：<span id="timer">120:00</span></div>
    <div class="badge">进度：<span id="progressText">0/198</span></div>
    <div class="progress" aria-label="progress"><div id="progressBar"></div></div>
    <button class="ghost" id="learnBtn" onclick="toggleLearn()">学习模式：关闭</button>
  </div>
</header>

<div class="container">
  <div class="card" id="questionCard"></div>

  <div class="row" style="margin-top:12px; align-items:center; justify-content:space-between;">
    <div class="row">
      <button class="secondary" id="prevBtn" onclick="prevQ()">上一题</button>
      <button class="secondary" id="nextBtn" onclick="nextQ()">下一题</button>
    </div>
    <div class="row">
      <button class="secondary" onclick="jumpPrompt()">跳题</button>
      <button class="primary" onclick="submitExam()">提交统计</button>
    </div>
  </div>

  <p class="small" style="margin-top:10px;">
    说明：本文件为本地离线刷题系统。考试模式不显示答案；开启学习模式后可显示正确答案与解析。
  </p>
</div>

<script>
const QUESTIONS = [{"id": 1, "shown": "1", "type": "hotspot", "stem": "You have a Fabric tenant.You plan to create a Fabric notebook that will use Spark DataFr ames to generate Microsoft Power BI\nvisuals.\nYou run the following code.\nFor each of the following statements, select Yes if the stateme nt is true. Otherwise, select No.\nNOTE:  Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: No\nCreate and render a quick visualize instance\n\nCreate a QuickVisualize instance from the DataFrame you created . If you're using a pandas DataFrame,\nyou can use our utility function as shown in the following code snippet to create the report. If you're using a\nDataFrame other than pandas, parse the data yourself.\n# Create a Power BI report from your data\nPBI_visualize = QuickVisualize(get_dataset_config(df), auth=dev ice_auth)\n# Render new report\nPBI_visualize\nBox 2: Yes\nBox 3: Yes", "reference": "Explanation:\nBox 1: No\nCreate and render a quick visualize instance\n\nCreate a QuickVisualize instance from the DataFrame you created . If you're using a pandas DataFrame,\nyou can use our utility function as shown in the following code snippet to create the report. If you're using a\nDataFrame other than pandas, parse the data yourself.\n# Create a Power BI report from your data\nPBI_visualize = QuickVisualize(get_dataset_config(df), auth=dev ice_auth)\n# Render new report\nPBI_visualize\nBox 2: Yes\nBox 3: YesReference:\nhttps://learn.microsoft.com/en-us/power-bi/create-reports/jupyt er-quick-report", "boxes": [{"box": 1, "value": "No"}, {"box": 2, "value": "Yes"}, {"box": 3, "value": "YesReference:"}], "image": "images/p002.jpg"}, {"id": 2, "shown": "2", "type": "mc", "stem": "You are analyzing the data in a Fabric notebook.\nYou have a Spark DataFrame assigned to a variable named df.You need to use the Chart view in the notebook to explore the d ata manually.\nWhich function should you run to make the data available in the  Chart view?", "options": [{"key": "A", "text": "displayHTML"}, {"key": "B", "text": "show"}, {"key": "C", "text": "write"}, {"key": "D", "text": "display"}], "correct": "D", "multi": false, "explanation": "Built-in visualization command - display() functionThe Fabric built-in visualization function allows you to turn A pache Spark DataFrames, Pandas\nDataFrames and SQL query results into rich format data visualiz ations.\nYou can use the display function on dataframes that created in PySpark and Scala on Spark DataFrames\n\nor Resilient Distributed Datasets (RDD) functions to produce th e rich dataframe table view and chart view.\nThe output of SQL statement appears in the rendered table view by default.", "reference": "Explanation:\nBuilt-in visualization command - display() functionThe Fabric built-in visualization function allows you to turn A pache Spark DataFrames, Pandas\nDataFrames and SQL query results into rich format data visualiz ations.\nYou can use the display function on dataframes that created in PySpark and Scala on Spark DataFrames\n\nor Resilient Distributed Datasets (RDD) functions to produce th e rich dataframe table view and chart view.\nThe output of SQL statement appears in the rendered table view by default.\nReference:\nhttps://learn.microsoft.com/en-us/fabric/data-engineering/noteb ook-visualization", "boxes": [], "image": "images/p003.jpg"}, {"id": 3, "shown": "3", "type": "mc", "stem": "You have a Fabric notebook that has the Python code and output shown in the following exhibit.\nWhich type of analytics are you performing?", "options": [{"key": "A", "text": "descriptive"}, {"key": "B", "text": "diagnostic"}, {"key": "C", "text": "prescriptive"}, {"key": "D", "text": "predictive"}], "correct": "A", "multi": false, "explanation": "This is a histogram. Histogram are used in relation to descript ive statistics calculations.", "reference": "Explanation:\n\nThis is a histogram. Histogram are used in relation to descript ive statistics calculations.\nReference:\nhttps://www.advantive.com/solutions/spc-software/quality-adviso r/data-analysis-tools/histogram-calculate-\ndescriptive-statistics/", "boxes": [], "image": "images/p004.jpg"}, {"id": 4, "shown": "4", "type": "mc", "stem": "You have a Fabric tenant that contains customer churn data stor ed as Parquet files in OneLake. The data\ncontains details about customer demographics and product usage.\nYou create a Fabric notebook to read the data into a Spark Data Frame. You then create column charts in\nthe notebook that show the distribution of retained customers a s compared to lost customers based on\ngeography, the number of products purchased, age, and customer tenure.\nWhich type of analytics are you performing?", "options": [{"key": "A", "text": "diagnostic"}, {"key": "B", "text": "descriptive"}, {"key": "C", "text": "prescriptive"}, {"key": "D", "text": "predictive"}], "correct": "B", "multi": false, "explanation": "What is Customer Retention Analytics?Customer retention analytics provide predictive metrics of whic h customers may churn, allowing\nbusinesses to prevent this from happening. Let us understand th is by an example, by using customer\nretention analytics, companies can reduce churn and increase pr ofits, as evidenced by a McKinsey report\nsuggesting that extensive use of customer data analytics can dr ive profit. Customer retention metrics,\nincluding the customer retention rate, are used to measure the likelihood of retaining and attracting\ncustomers to a business. This is how data analytics helps in cu stomer retention.\nDescriptive Analytics\nDescriptive analytics provide you with granular insights based on historical data. This includes tracking\npast purchases, customer complaints, customer service reviews, and more. In order to implement\ndescriptive customer retention analytics, your cloud engineers would need to make sure all customer data\nis on-premise and up-to-date and backed up on a regular basis. Because it uses historical data to create\nretention strategies and personalize customer experiences, all historical data must be accessible for\nanalysis.\nIncorrect:\n* Predictive AnalyticsThis works in tandеm with dеscriptivе analytics, which allows y ou to forеcast the behavior of your\ncustomers based on past data. This allows you to prеparе for sp ecific customеr intеractions and improvе\ncustomеr rеtеntion. For еxamplе, you can usе historical transac tions to prеdict how likely a customer is to\nrеnеw their subscription at a music plan. Thе nеxt timе that cu stomеr walks into thе studio, your staff will\nrеcеivе an alеrt to offеr additional incеntivеs to pеrsuadе thе m to rеnеw.\n* Prescriptive Analytics\nPrescriptive analytics finds solutions based on insights from d escriptive analytics. For example, you can\ncollect data about remedial solutions to improve retention and see how well they performed. Prescriptive\nanalytics forces you to retrospectively evaluate all strategies to improve them. For example, a bank might\nuse Fraud Detection. An algorithm evaluates historical data aft er making a purchase to see if it matches\nthe typical level of spending. If it detects an anomaly, the ba nk will be notified and will recommend a\ncourse of action, such as cancelling the bank card.\n* Diagnostic Analytics\nDiagnostic analytics involves the collection and examination of data pertaining to a particular issue or\noccurrence in order to comprehend the underlying causes. Consid er a scenario where a fitness app,\nGymFit, observes a significant drop in user engagement during a specific period. Unraveling the factors\ncontributing to this decline becomes the focal point of diagnos tic analytics. In this context, GymFit delves\n\ninto the data to uncover reasons why users might be disengaging , such as changes in workout\npreferences, dissatisfaction with features, or scheduling confl icts. Through careful analysis, GymFit\nidentifies patterns and root causes behind the drop in user eng agement. Armed with this knowledge, the\nfitness app can then implement targeted improvements, addressin g concerns and enhancing the overall\nuser experience to prevent further disengagement and attract ne w users.", "reference": "Explanation:\nWhat is Customer Retention Analytics?Customer retention analytics provide predictive metrics of whic h customers may churn, allowing\nbusinesses to prevent this from happening. Let us understand th is by an example, by using customer\nretention analytics, companies can reduce churn and increase pr ofits, as evidenced by a McKinsey report\nsuggesting that extensive use of customer data analytics can dr ive profit. Customer retention metrics,\nincluding the customer retention rate, are used to measure the likelihood of retaining and attracting\ncustomers to a business. This is how data analytics helps in cu stomer retention.\nDescriptive Analytics\nDescriptive analytics provide you with granular insights based on historical data. This includes tracking\npast purchases, customer complaints, customer service reviews, and more. In order to implement\ndescriptive customer retention analytics, your cloud engineers would need to make sure all customer data\nis on-premise and up-to-date and backed up on a regular basis. Because it uses historical data to create\nretention strategies and personalize customer experiences, all historical data must be accessible for\nanalysis.\nIncorrect:\n* Predictive AnalyticsThis works in tandеm with dеscriptivе analytics, which allows y ou to forеcast the behavior of your\ncustomers based on past data. This allows you to prеparе for sp ecific customеr intеractions and improvе\ncustomеr rеtеntion. For еxamplе, you can usе historical transac tions to prеdict how likely a customer is to\nrеnеw their subscription at a music plan. Thе nеxt timе that cu stomеr walks into thе studio, your staff will\nrеcеivе an alеrt to offеr additional incеntivеs to pеrsuadе thе m to rеnеw.\n* Prescriptive Analytics\nPrescriptive analytics finds solutions based on insights from d escriptive analytics. For example, you can\ncollect data about remedial solutions to improve retention and see how well they performed. Prescriptive\nanalytics forces you to retrospectively evaluate all strategies to improve them. For example, a bank might\nuse Fraud Detection. An algorithm evaluates historical data aft er making a purchase to see if it matches\nthe typical level of spending. If it detects an anomaly, the ba nk will be notified and will recommend a\ncourse of action, such as cancelling the bank card.\n* Diagnostic Analytics\nDiagnostic analytics involves the collection and examination of data pertaining to a particular issue or\noccurrence in order to comprehend the underlying causes. Consid er a scenario where a fitness app,\nGymFit, observes a significant drop in user engagement during a specific period. Unraveling the factors\ncontributing to this decline becomes the focal point of diagnos tic analytics. In this context, GymFit delves\n\ninto the data to uncover reasons why users might be disengaging , such as changes in workout\npreferences, dissatisfaction with features, or scheduling confl icts. Through careful analysis, GymFit\nidentifies patterns and root causes behind the drop in user eng agement. Armed with this knowledge, the\nfitness app can then implement targeted improvements, addressin g concerns and enhancing the overall\nuser experience to prevent further disengagement and attract ne w users.\nReference:\nhttps://emergingindiagroup.com/data-analytics-for-customer-rete ntion/", "boxes": []}, {"id": 5, "shown": "5", "type": "mc", "stem": "You have a Fabric workspace named Workspace1 that contains a da taflow named Dataflow1. Dataflow1\nreturns 500 rows of data.\nYou need to identify the min and max values for each column in the query results.\nWhich three Data view options should you select? Each correct a nswer presents part of the solution.\nNOTE: Each correct answer is worth one point.", "options": [{"key": "A", "text": "Show column value distribution"}, {"key": "B", "text": "Enable column profile"}, {"key": "C", "text": "Show column profile in details pane"}, {"key": "D", "text": "Show column quality details"}, {"key": "E", "text": "Enable details pane"}], "correct": "BCE", "multi": true, "explanation": "", "reference": "", "boxes": [], "image": "images/p006.jpg"}, {"id": 6, "shown": "6", "type": "mc", "stem": "You have a Fabric tenant that contains a Microsoft Power BI rep ort.\nYou are exploring a new semantic model.You need to display the following column statistics:\nCount\nAverage\nNull count\nDistinct count\nStandard deviation\nWhich Power Query function should you run?", "options": [{"key": "A", "text": "Table.schema"}, {"key": "B", "text": "Table.view"}, {"key": "C", "text": "Table.FuzzyGroup"}, {"key": "D", "text": "Table.Profile"}], "correct": "D", "multi": false, "explanation": "Power Query M, Table.ProfileSyntaxTable.Profile(table as table, optional additionalAggregates as nullable list) as table\nAbout\n\nReturns a profile for the columns in table.\nThe following information is returned for each column (when app licable):\nminimum\nmaximumaveragestandard deviationcountnull countdistinct count", "reference": "Explanation:\nPower Query M, Table.ProfileSyntaxTable.Profile(table as table, optional additionalAggregates as nullable list) as table\nAbout\n\nReturns a profile for the columns in table.\nThe following information is returned for each column (when app licable):\nminimum\nmaximumaveragestandard deviationcountnull countdistinct count\nReference:\nhttps://learn.microsoft.com/en-us/powerquery-m/table-profile\n\nMaintain a data analytics solution\nTestlet 1Case studyThis is a case study. Case studies are not timed separately. You can use as much exam time as you\nwould like to complete each case. However, there may be additional case studies and sections on this\nexam. You must manage your time to ensure that you are able to complete all questions included on this\nexam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in\nthe case study. Case studies might contain exhibits and other r esources that provide more information\nabout the scenario that is described in the case study. Each qu estion is independent of the other questions\nin this case study.\nAt the end of this case study, a review screen will appear. Thi s screen allows you to review your answers\nand to make changes before you move to the next section of the exam. After you begin a new section, you\ncannot return to this section.\nTo start the case study\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to\nexplore the content of the case study before you answer the que stions. Clicking these buttons displays\ninformation such as business requirements, existing environment , and problem statements. If the case\nstudy has an All Information tab, note that the information displayed is identical to the i nformation\ndisplayed on the subsequent tabs. When you are ready to answer a question, click the Question button to\nreturn to the question.\nOverview\nContoso, Ltd. is a US-based health supplements company. Contoso has two divisions named Sales and\nResearch. The Sales division contains two departments named Onl ine Sales and Retail Sales. The\nResearch division assigns internally developed product lines to individual teams of researchers and\nanalysts.\nExisting Environment\nIdentity Environment\nContoso has a Microsoft Entra tenant named contoso.com. The ten ant contains two groups named\nResearchReviewersGroup1 and ResearchReviewersGroup2.\nData Environment\nContoso has the following data environment:\nThe Sales division uses a Microsoft Power BI Premium capacity.\nThe semantic model of the Online Sales department includes a fa ct table named Orders that uses\nImport mode. In the system of origin, the OrderID value represe nts the sequence in which orders are\ncreated.\nThe Research department uses an on-premises, third-party data w arehousing product.\nFabric is enabled for contoso.com.\nAn Azure Data Lake Storage Gen2 storage account named storage1 contains Research division data\nfor a product line named Productline1. The data is in the delta format.\nA Data Lake Storage Gen2 storage account named storage2 contain s Research division data for a\nproduct line named Productline2. The data is in the CSV format.\nRequirements\nPlanned Changes\nContoso plans to make the following changes:\n\nEnable support for Fabric in the Power BI Premium capacity used by the Sales division.\nMake all the data for the Sales division and the Research divis ion available in Fabric.\nFor the Research division, create two Fabric workspaces named P roductline1ws and Productline2ws.\nIn Productline1ws, create a lakehouse named Lakehouse1.\nIn Lakehouse1, create a shortcut to storage1 named ResearchProd uct.\nData Analytics Requirements\nContoso identifies the following data analytics requirements:\nAll the workspaces for the Sales division and the Research divi sion must support all Fabric\nexperiences.\nThe Research division workspaces must use a dedicated, on-deman d capacity that has per-minute\nbilling.\nThe Research division workspaces must be grouped together logic ally to support OneLake data hub\nfiltering based on the department name.\nFor the Research division workspaces, the members of ResearchRe viewersGroup1 must be able to\nread lakehouse and warehouse data and shortcuts by using SQL en dpoints.\nFor the Research division workspaces, the members of ResearchRe viewersGroup2 must be able to\nread lakehouse data by using Lakehouse explorer.\nAll the semantic models and reports for the Research division m ust use version control that supports\nbranching.\nData Preparation Requirements\nContoso identifies the following data preparation requirements:\nThe Research division data for Productline2 must be retrieved f rom Lakehouse1 by using Fabric\nnotebooks.\nAll the Research division data in the lakehouses must be presen ted as managed tables in Lakehouse\nexplorer.\nSemantic Model Requirements\nContoso identifies the following requirements for implementing and managing semantic models:\nThe number of rows added to the Orders table during refreshes m ust be minimized.\nThe semantic models in the Research division workspaces must us e Direct Lake mode.\nGeneral Requirements\nContoso identifies the following high-level requirements that m ust be considered for all solutions:\nFollow the principle of least privilege when applicable.\nMinimize implementation and maintenance effort when possible.", "boxes": [], "images": ["images/p006.jpg", "images/p008.jpg"]}, {"id": 7, "shown": "1", "type": "mc", "stem": "You need to recommend which type of Fabric capacity SKU meets t he data analytics requirements for the\nResearch division.\nWhat should you recommend?", "options": [{"key": "A", "text": "A"}, {"key": "B", "text": "EM"}, {"key": "C", "text": "P"}, {"key": "D", "text": "F"}], "correct": "D", "multi": false, "explanation": "Use F SKU for Fabric.\n\nNote: Power BI embedded analytics requires a capacity (A, EM, P , or F SKU) in order to publish\nembedded Power BI content.\nMicrosoft Fabric\nMicrosoft Fabric is an Azure offering that brings together new and existing components from Power BI,\nAzure Synapse, and Azure Data Explorer into a single integrated environment. Fabric uses F SKUs and\nsupports embedding Power BI items.\nScenario:\nData Analytics Requirements\nContoso identifies the following data analytics requirements:\n*-> All the workspaces for the Sales division and the Research division must support all Fabric\nexperiences.\nThe Research division workspaces must use a dedicated, on-deman d capacity that has per-minute\nbilling.\nThe Research division workspaces must be grouped together logic ally to support OneLake data hub\nfiltering based on the department name.\nFor the Research division workspaces, the members of ResearchRe viewersGroup1 must be able to\nread lakehouse and warehouse data and shortcuts by using SQL en dpoints.\nFor the Research division workspaces, the members of ResearchRe viewersGroup2 must be able to\nread lakehouse data by using Lakehouse explorer.\nAll the semantic models and reports for the Research division m ust use version control that supports\nbranching.", "reference": "Explanation:\nUse F SKU for Fabric.\n\nNote: Power BI embedded analytics requires a capacity (A, EM, P , or F SKU) in order to publish\nembedded Power BI content.\nMicrosoft Fabric\nMicrosoft Fabric is an Azure offering that brings together new and existing components from Power BI,\nAzure Synapse, and Azure Data Explorer into a single integrated environment. Fabric uses F SKUs and\nsupports embedding Power BI items.\nScenario:\nData Analytics Requirements\nContoso identifies the following data analytics requirements:\n*-> All the workspaces for the Sales division and the Research division must support all Fabric\nexperiences.\nThe Research division workspaces must use a dedicated, on-deman d capacity that has per-minute\nbilling.\nThe Research division workspaces must be grouped together logic ally to support OneLake data hub\nfiltering based on the department name.\nFor the Research division workspaces, the members of ResearchRe viewersGroup1 must be able to\nread lakehouse and warehouse data and shortcuts by using SQL en dpoints.\nFor the Research division workspaces, the members of ResearchRe viewersGroup2 must be able to\nread lakehouse data by using Lakehouse explorer.\nAll the semantic models and reports for the Research division m ust use version control that supports\nbranching.\nReference:\nhttps://learn.microsoft.com/en-us/power-bi/developer/embedded/e mbedded-capacity", "boxes": [], "image": "images/p009.jpg"}, {"id": 8, "shown": "2", "type": "hotspot", "stem": "Which workspace role assignments should you recommend for Resea rchReviewersGroup1 and\nResearchReviewersGroup2? To answer, select the appropriate opti ons in the answer area.\nNOTE : Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: Viewer\nResearchReviewersGroup1\nFor the Research division workspaces, the members of ResearchRe viewersGroup1 must be able to read\nlakehouse and warehouse data and shortcuts by using SQL endpoin ts.\nWorkspace roles in Lakehouse\nWorkspace roles define what user can do with Microsoft Fabric i tems. Roles can be assigned to individuals\nor security groups from workspace view. See, Give users access to workspaces.\nThe user can be assigned to the following roles:Admin\nMemberContributorViewerIn a lakehouse the users with Admin, Member, and Contributor ro les can perform all CRUD (CREATE,\nREAD, UPDATE and DELETE) operations on all data. A user with Vi ewer role can only read data stored in\nTables using the SQL analytics endpoint.\nBox 2: Contributor\nResearchReviewersGroup2\nFor the Research division workspaces, the members of ResearchRe viewersGroup2 must be able to read\nlakehouse data by using Lakehouse explorer.\nMicrosoft Fabric workspace roles\n\nEtc.\nIncorrect:\n* MemberMore permissions compared to Contributor\nScenario:\nIdentity Environment\nContoso has a Microsoft Entra tenant named contoso.com. The ten ant contains two groups named\nResearchReviewersGroup1 and ResearchReviewersGroup2.", "reference": "Explanation:\nBox 1: Viewer\nResearchReviewersGroup1\nFor the Research division workspaces, the members of ResearchRe viewersGroup1 must be able to read\nlakehouse and warehouse data and shortcuts by using SQL endpoin ts.\nWorkspace roles in Lakehouse\nWorkspace roles define what user can do with Microsoft Fabric i tems. Roles can be assigned to individuals\nor security groups from workspace view. See, Give users access to workspaces.\nThe user can be assigned to the following roles:Admin\nMemberContributorViewerIn a lakehouse the users with Admin, Member, and Contributor ro les can perform all CRUD (CREATE,\nREAD, UPDATE and DELETE) operations on all data. A user with Vi ewer role can only read data stored in\nTables using the SQL analytics endpoint.\nBox 2: Contributor\nResearchReviewersGroup2\nFor the Research division workspaces, the members of ResearchRe viewersGroup2 must be able to read\nlakehouse data by using Lakehouse explorer.\nMicrosoft Fabric workspace roles\n\nEtc.\nIncorrect:\n* MemberMore permissions compared to Contributor\nScenario:\nIdentity Environment\nContoso has a Microsoft Entra tenant named contoso.com. The ten ant contains two groups named\nResearchReviewersGroup1 and ResearchReviewersGroup2.\nReference:\nhttps://learn.microsoft.com/en-us/fabric/data-engineering/works pace-roles-lakehouse\nhttps://learn.microsoft.com/en-us/fabric/get-started/roles-work spaces", "boxes": [{"box": 1, "value": "Viewer"}, {"box": 2, "value": "Contributor"}], "images": ["images/p010.jpg", "images/p011.jpg"]}, {"id": 9, "shown": "3", "type": "mc", "stem": "You need to ensure that Contoso can use version control to meet  the data analytics requirements and the\ngeneral requirements.\nWhat should you do?", "options": [{"key": "A", "text": "Store all the semantic models and reports in Data Lake Gen2 s torage."}, {"key": "B", "text": "Modify the settings of the Research workspaces to use a GitHu b repository."}, {"key": "C", "text": "Modify the settings of the Research division workspaces to us e an Azure Repos repository."}, {"key": "D", "text": "Store all the semantic models and reports in Microsoft OneDri ve."}], "correct": "C", "multi": false, "explanation": "", "reference": "", "boxes": [], "image": "images/p012.jpg"}, {"id": 10, "shown": "4", "type": "hotspot", "stem": "You need to recommend a solution to group the Research division  workspaces.\nWhat should you include in the recommendation? To answer, selec t the appropriate options in the answer\narea.\nNOTE:  Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: Domain\nGrouping method\nWith the OneLake data hub users can see data across their busin ess domains and filter to see a specific\ndomain that they are interested in, see all authoritative endor sed data in one place and see all the data\nowned by users to make data management easy as possible in one central location.\nBox 2: OneLake data hub\nTool\nThe OneLake data hub is integrated into multiple experiences wi thin both Fabric service and Power BI\nDesktop. This integration ensures that users can quickly and ea sily find necessary data in any context and\nin a consistent manner. For instance, in Power BI Desktop, user s may access the OneLake data hub\nexperience to browse available items and connect with them, thu s avoiding the need to create new data\n\nsources. This approach fosters a culture of data reusability an d helps organizations meet their goals more\neffectively.\nScenario:\nData Analytics Requirements\nContoso identifies the following data analytics requirements:\n*-> The Research division workspaces must be grouped together l ogically to support OneLake data hub\nfiltering based on the department name.\nIdentity Environment\nContoso has a Microsoft Entra tenant named contoso.com. The ten ant contains two groups named\nResearchReviewersGroup1 and ResearchReviewersGroup2.", "reference": "Explanation:\nBox 1: Domain\nGrouping method\nWith the OneLake data hub users can see data across their busin ess domains and filter to see a specific\ndomain that they are interested in, see all authoritative endor sed data in one place and see all the data\nowned by users to make data management easy as possible in one central location.\nBox 2: OneLake data hub\nTool\nThe OneLake data hub is integrated into multiple experiences wi thin both Fabric service and Power BI\nDesktop. This integration ensures that users can quickly and ea sily find necessary data in any context and\nin a consistent manner. For instance, in Power BI Desktop, user s may access the OneLake data hub\nexperience to browse available items and connect with them, thu s avoiding the need to create new data\n\nsources. This approach fosters a culture of data reusability an d helps organizations meet their goals more\neffectively.\nScenario:\nData Analytics Requirements\nContoso identifies the following data analytics requirements:\n*-> The Research division workspaces must be grouped together l ogically to support OneLake data hub\nfiltering based on the department name.\nIdentity Environment\nContoso has a Microsoft Entra tenant named contoso.com. The ten ant contains two groups named\nResearchReviewersGroup1 and ResearchReviewersGroup2.\nReference:\nhttps://blog.fabric.microsoft.com/en-us/blog/microsoft-onelake- in-fabric-the-onedrive-for-data/\nhttps://learn.microsoft.com/en-us/fabric/get-started/onelake-da ta-hub\n\nMaintain a data analytics solution\nTestlet 2Case study\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you\nwould like to complete each case. However, there may be additional case studies and sections on this\nexam. You must manage your time to ensure that you are able to complete all questions included on this\nexam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in\nthe case study. Case studies might contain exhibits and other r esources that provide more information\nabout the scenario that is described in the case study. Each qu estion is independent of the other questions\nin this case study.\nAt the end of this case study, a review screen will appear. Thi s screen allows you to review your answers\nand to make changes before you move to the next section of the exam. After you begin a new section, you\ncannot return to this section.\nTo start the case study\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to\nexplore the content of the case study before you answer the que stions. Clicking these buttons displays\ninformation such as business requirements, existing environment , and problem statements. If the case\nstudy has an All Information tab, note that the information displayed is identical to the i nformation\ndisplayed on the subsequent tabs. When you are ready to answer a question, click the Question button to\nreturn to the question.\nOverview\nLitware, Inc. is a manufacturing company that has offices throu ghout North America. The analytics team at\nLitware contains data engineers, analytics engineers, data anal ysts, and data scientists.\nExisting Environment\nFabric Environment\nLitware has been using a Microsoft Power BI tenant for three ye ars. Litware has NOT enabled any Fabric\ncapacities and features.\nAvailable Data\nLitware has data that must be analyzed as shown in the followin g table.\nThe Product data contains a single table and the following colu mns.\n\nThe customer satisfaction data contains the following tables:\nSurvey\nQuestion\nResponse\nFor each survey submitted, the following occurs:\nOne row is added to the Survey table.\nOne row is added to the Response table for each question in the survey.\nThe Question table contains the text of each survey question. T he third question in each survey response\nis an overall satisfaction score. Customers can submit a survey after each purchase.\nUser Problems\nThe analytics team has large volumes of data, some of which is semi-structured. The team wants to use\nFabric to create a new data store.\nProduct data is often classified into three pricing groups: hig h, medium, and low. This logic is implemented\nin several databases and semantic models, but the logic does NOT always match across implementations.\nRequirements\nPlanned Changes\nLitware plans to enable Fabric features in the existing tenant. The analytics team will create a new data\nstore as a proof of concept (PoC). The remaining Litware users will only get access to the Fabric features\nonce the PoC is complete. The PoC will be completed by using a Fabric trial capacity.\nThe following three workspaces will be created:\nAnalyticsPOC: Will contain the data store, semantic models, rep orts pipelines, dataflow, and notebooks\nused to populate the data store\nDataEngPOC: Will contain all the pipelines, dataflows, and note books used to populate OneLake\nDataSciPOC: Will contain all the notebooks and reports created by the data scientists\nThe following will be created in the AnalyticsPOC workspace:\nA data store (type to be decided)\nA custom semantic model\nA default semantic model\nInteractive reports\nThe data engineers will create data pipelines to load data to O neLake either hourly or daily depending on\nthe data source. The analytics engineers will create processes to ingest, transform, and load the data to\nthe data store in the AnalyticsPOC workspace daily. Whenever po ssible, the data engineers will use low-\ncode tools for data ingestion. The choice of which data cleansi ng and transformation tools to use will be at\nthe data engineers’ discretion.\nAll the semantic models and reports in the Analytics POC worksp ace will use the data store as the sole\ndata source.\nTechnical Requirements\nThe data store must support the following:\nRead access by using T-SQL or Python\nSemi-structured and unstructured data\nRow-level security (RLS) for users executing T-SQL queries\nFiles loaded by the data engineers to OneLake will be stored in the Parquet format and will meet Delta\nLake specifications.\n\nData will be loaded without transformation in one area of the A nalyticsPOC data store. The data will then\nbe cleansed, merged, and transformed into a dimensional model.\nThe data load process must ensure that the raw and cleansed dat a is updated completely before\npopulating the dimensional model.\nThe dimensional model must contain a date dimension. There is n o existing data source for the date\ndimension. The Litware fiscal year matches the calendar year. T he date dimension must always contain\ndates from 2010 through the end of the current year.\nThe product pricing group logic must be maintained by the analy tics engineers in a single location. The\npricing group data must be made available in the data store for T-SQL queries and in the default semantic\nmodel. The following logic must be used:\nList prices that are less than or equal to 50 are in the low pr icing group.\nList prices that are greater than 50 and less than or equal to 1,000 are in the medium pricing group.\nList prices that are greater than 1,000 are in the high pricing group.\nSecurity Requirements\nOnly Fabric administrators and the analytics team must be able to see the Fabric items created as part of\nthe PoC.\nLitware identifies the following security requirements for the Fabric items in the AnalyticsPOC workspace:\nFabric administrators will be the workspace administrators.\nThe data engineers must be able to read from and write to the d ata store. No access must be granted\nto datasets or reports.\nThe analytics engineers must be able to read from, write to, an d create schemas in the data store. They\nalso must be able to create and share semantic models with the data analysts and view and modify all\nreports in the workspace.\nThe data scientists must be able to read from the data store, b ut not write to it. They will access the\ndata by using a Spark notebook\nThe data analysts must have read access to only the dimensional model objects in the data store. They\nalso must have access to create Power BI reports by using the s emantic models created by the\nanalytics engineers.\nThe date dimension must be available to all users of the data s tore.\nThe principle of least privilege must be followed.\nBoth the default and custom semantic models must include only t ables or views from the dimensional\nmodel in the data store. Litware already has the following Micr osoft Entra security groups:\nFabricAdmins: Fabric administrators\nAnalyticsTeam: All the members of the analytics team\nDataAnalysts: The data analysts on the analytics team\nDataScientists: The data scientists on the analytics team\nDataEngineers: The data engineers on the analytics team\nAnalyticsEngineers: The analytics engineers on the analytics te am\nReport Requirements\nThe data analysts must create a customer satisfaction report th at meets the following requirements:\nEnables a user to select a product to filter customer survey re sponses to only those who have\npurchased that product.\nDisplays the average overall satisfaction score of all the surv eys submitted during the last 12 months\nup to a selected date.\nShows data as soon as the data is updated in the data store.\nEnsures that the report and the semantic model only contain dat a from the current and previous year.\nEnsures that the report respects any table-level security speci fied in the source data store.\nMinimizes the execution time of report queries.", "boxes": [{"box": 1, "value": "Domain"}, {"box": 2, "value": "OneLake data hub"}], "images": ["images/p012.jpg", "images/p013.jpg", "images/p014.jpg", "images/p015.jpg", "images/p016.jpg", "images/p017.jpg"]}, {"id": 11, "shown": "1", "type": "hotspot", "stem": "You need to assign permissions for the data store in the Analyt icsPOC workspace. The solution must meet\nthe security requirements.\nWhich additional permissions should you assign when you share t he data store? To answer, select the\nappropriate options in the answer area.\nNOTE:  Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: Build Reports on the default dataset\nDataEngineers\nScenario: Litware identifies the following security requirement s for the Fabric items in the AnalyticsPOC\nworkspace:* The analytics engineers must be able to read from, write to, and create schemas in the data store. They\nalso must be able to create and share semantic models with the data analysts and view and modify all\nreports in the workspace.\nBox 2: Read All SQL analytics endpoint data\nDataAnalyst\n* The data analysts must have read access to only the dimension al model objects in the data store. They\nalso must have access to create Power BI reports by using the s emantic models created by the analytics\nengineers.\nBox 3: Read All Apache Spark\nDataScientists\n* The data scientists must be able to read from the data store, but not write to it. They will access the data\nby using a Spark notebook.\n\nMaintain a data analytics solution\nQuestion Set 3", "reference": "Explanation:\nBox 1: Build Reports on the default dataset\nDataEngineers\nScenario: Litware identifies the following security requirement s for the Fabric items in the AnalyticsPOC\nworkspace:* The analytics engineers must be able to read from, write to, and create schemas in the data store. They\nalso must be able to create and share semantic models with the data analysts and view and modify all\nreports in the workspace.\nBox 2: Read All SQL analytics endpoint data\nDataAnalyst\n* The data analysts must have read access to only the dimension al model objects in the data store. They\nalso must have access to create Power BI reports by using the s emantic models created by the analytics\nengineers.\nBox 3: Read All Apache Spark\nDataScientists\n* The data scientists must be able to read from the data store, but not write to it. They will access the data\nby using a Spark notebook.\n\nMaintain a data analytics solution\nQuestion Set 3", "boxes": [{"box": 1, "value": "Build Reports on the default dataset"}, {"box": 2, "value": "Read All SQL analytics endpoint data"}, {"box": 3, "value": "Read All Apache Spark"}], "images": ["images/p018.jpg", "images/p019.jpg"]}, {"id": 12, "shown": "1", "type": "mc", "stem": "You have a Fabric tenant named Tenant1 that contains a workspac e named WS1. WS1 uses a capacity\nnamed C1 and contains a dataset named DS1.\nYou need to ensure read-write access to DS1 is available by usi ng XMLA endpoint.\nWhat should be modified first?", "options": [{"key": "A", "text": "the DS1 settings"}, {"key": "B", "text": "the WS1 settings"}, {"key": "C", "text": "the C1 settings"}, {"key": "D", "text": "the Tenant1 settings"}], "correct": "C", "multi": false, "explanation": "Semantic model connectivity with the XMLA endpointRead-write operations using the endpoint can be enabled. Read-w rite provides more semantic model\nmanagement, governance, advanced semantic modeling, debugging, and monitoring. When enabled,\nsemantic models have more parity with Azure Analysis Services a nd SQL Server Analysis Services\nenterprise grade tabular modeling tools and processes.\nEnable XMLA read-write\nBy default, Premium capacity or Premium Per User semantic model workloads have the XMLA endpoint\nproperty setting enabled for read-only. This means applications can only query a semantic model. For\napplications to perform write operations, the XMLA Endpoint pro perty must be enabled for read-write.\nTo enable read-write for a Premium capacity\n1. Select Settings > Admin portal.\n2. In the Admin portal, select Capacity settings > Power BI Pre mium > capacity name.\n3. Expand Workloads. In the XMLA Endpoint setting, select Read Write. The XMLA Endpoint setting\napplies to all workspaces and semantic models assigned to the c apacity.", "reference": "Explanation:\nSemantic model connectivity with the XMLA endpointRead-write operations using the endpoint can be enabled. Read-w rite provides more semantic model\nmanagement, governance, advanced semantic modeling, debugging, and monitoring. When enabled,\nsemantic models have more parity with Azure Analysis Services a nd SQL Server Analysis Services\nenterprise grade tabular modeling tools and processes.\nEnable XMLA read-write\nBy default, Premium capacity or Premium Per User semantic model workloads have the XMLA endpoint\nproperty setting enabled for read-only. This means applications can only query a semantic model. For\napplications to perform write operations, the XMLA Endpoint pro perty must be enabled for read-write.\nTo enable read-write for a Premium capacity\n1. Select Settings > Admin portal.\n2. In the Admin portal, select Capacity settings > Power BI Pre mium > capacity name.\n3. Expand Workloads. In the XMLA Endpoint setting, select Read Write. The XMLA Endpoint setting\napplies to all workspaces and semantic models assigned to the c apacity.\nReference:\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-p remium-connect-tools", "boxes": [], "image": "images/p020.jpg"}, {"id": 13, "shown": "2", "type": "hotspot", "stem": "You have a Fabric tenant that contains a warehouse named Wareho use1. Warehouse1 contains three\nschemas named schemaA, schemaB, and schemaC.\nYou need to ensure that a user named User1 can truncate tables in schemaA only.\nHow should you complete the T-SQL statement? To answer, select the appropriate options in the answer\narea.\nNOTE:  Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: ALTER\nThe minimum permission required is ALTER on table_name. TRUNCAT E TABLE permissions default to\nthe table owner, members of the sysadmin fixed server role, and the db_owner and db_ddladmin fixed\ndatabase roles, and are not transferable.\nUsing DCL:\nGRANT ALTER on schema::schemaname to 'Azure User'\nBox 2: SCHEMA::schemaA", "reference": "Explanation:\nBox 1: ALTER\nThe minimum permission required is ALTER on table_name. TRUNCAT E TABLE permissions default to\nthe table owner, members of the sysadmin fixed server role, and the db_owner and db_ddladmin fixed\ndatabase roles, and are not transferable.\nUsing DCL:\nGRANT ALTER on schema::schemaname to 'Azure User'\nBox 2: SCHEMA::schemaAReference:\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/truncate -table-transact-sql\nhttps://learn.microsoft.com/en-us/answers/questions/757108/trun cate-table-permission-in-azure-synapse", "boxes": [{"box": 1, "value": "ALTER"}, {"box": 2, "value": "SCHEMA::schemaAReference:"}], "image": "images/p021.jpg"}, {"id": 14, "shown": "3", "type": "mc", "stem": "You plan to deploy Microsoft Power BI items by using Fabric dep loyment pipelines. You have a\ndeployment pipeline that contains three stages named Developmen t, Test, and Production. A workspace is\nassigned to each stage.\nYou need to provide Power BI developers with access to the pipe line. The solution must meet the following\nrequirements:\nEnsure that the developers can deploy items to the workspaces f or Development and Test.\nPrevent the developers from deploying items to the workspace fo r Production.\nEnsure that the developers can view items in Production.\nFollow the principle of least privilege.\nWhich three levels of access should you assign to the developer s? Each correct answer presents part of\nthe solution.\nNOTE:  Each correct answer is worth one point.", "options": [{"key": "A", "text": "Build permission to the production semantic models"}, {"key": "B", "text": "Admin access to the deployment pipeline"}, {"key": "C", "text": "Viewer access to the Development and Test workspaces"}, {"key": "D", "text": "Viewer access to the Production workspace"}, {"key": "E", "text": "Contributor access to the Development and Test workspaces"}, {"key": "F", "text": "Contributor access to the Production workspace"}], "correct": "BDE", "multi": true, "explanation": "", "reference": "", "boxes": [], "image": "images/p021.jpg"}, {"id": 15, "shown": "4", "type": "mc", "stem": "You have a Fabric tenant that contains a warehouse.\nSeveral times a day, the performance of all warehouse queries d egrades. You suspect that Fabric is\nthrottling the compute used by the warehouse.\nWhat should you use to identify whether throttling is occurring ?", "options": [{"key": "A", "text": "the Capacity settings"}, {"key": "B", "text": "the Monitoring hub"}, {"key": "C", "text": "dynamic management views (DMVs)"}, {"key": "D", "text": "the Microsoft Fabric Capacity Metrics app"}], "correct": "D", "multi": false, "explanation": "Monitor overload information with Fabric Capacity Metrics AppCapacity administrators can view overload information and drill down further via Microsoft Fabric Capacity\nMetrics app.\n\nNote: Throttling\nThrottling occurs when a customer's capacity consumes more CPU resources than what was purchased.\nAfter consumption is smoothed, capacity throttling policies wil l be checked based on the amount of future\ncapacity consumed. This results in a degraded end-user experien ce. When a capacity enters a throttled\nstate, it only affects operations that are requested after the capacity has begun throttling.\nThrottling policies are applied at a capacity level. If one cap acity, or set of workspaces, is experiencing\nreduced performance due to being overloaded, other capacities c an continue running normally.", "reference": "Explanation:\nMonitor overload information with Fabric Capacity Metrics AppCapacity administrators can view overload information and drill down further via Microsoft Fabric Capacity\nMetrics app.\n\nNote: Throttling\nThrottling occurs when a customer's capacity consumes more CPU resources than what was purchased.\nAfter consumption is smoothed, capacity throttling policies wil l be checked based on the amount of future\ncapacity consumed. This results in a degraded end-user experien ce. When a capacity enters a throttled\nstate, it only affects operations that are requested after the capacity has begun throttling.\nThrottling policies are applied at a capacity level. If one cap acity, or set of workspaces, is experiencing\nreduced performance due to being overloaded, other capacities c an continue running normally.\nReference:\nhttps://learn.microsoft.com/en-us/fabric/data-warehouse/compute -capacity-smoothing-throttling", "boxes": [], "image": "images/p022.jpg"}, {"id": 16, "shown": "5", "type": "dragdrop", "stem": "You have a Fabric tenant that contains a lakehouse named Lakeho use1.\nReadings from 100 IoT devices are appended to a Delta table in Lakehouse1. Each set of readings is\napproximately 25 KB. Approximately 10 GB of data is received da ily.\nAll the table and SparkSession settings are set to the default.You discover that queries are slow to execute. In addition, the  lakehouse storage contains data and log\nfiles that are no longer used.\nYou need to remove the files that are no longer used and combin e small files into larger files with a target\nsize of 1 GB per file.\nWhat should you do? To answer, drag the appropriate actions to the correct requirements. Each action\nmay be used once, more than once, or not at all. You may need t o drag the split bar between panes or\nscroll to view content.\nNOTE:  Each correct selection is worth one point.\nSelect and Place:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: Run the VACUUM command on a schedule.\nRemove the files.\nRemove old files with the Delta Lake Vacuum Command\nYou can remove files marked for deletion (aka “tombstoned files ”) from storage with the Delta Lake\nvacuum command. Delta Lake doesn't physically remove files from storage for operations that logically\ndelete the files. You need to use the vacuum command to physica lly remove files from storage that have\nbeen marked for deletion and are older than the retention perio d.\nThe main benefit of vacuuming is to save on storage costs. Vacu uming does not make your queries run\nany faster and can limit your ability to time travel to earlier Delta table versions. You need to weigh the\ncosts/benefits for each of your tables to develop an optimal va cuum strategy. Some tables should be\nvacuumed frequently. Other tables should never be vacuumed.\nBox 2: Run the OPTIMIZE command on a schedule.\nCombine the files.\nBest practices: Delta Lake\nCompact filesIf you continuously write data to a Delta table, it will over t ime accumulate a large number of files,\nespecially if you add data in small batches. This can have an a dverse effect on the efficiency of table\nreads, and it can also affect the performance of your file syst em. Ideally, a large number of small files\nshould be rewritten into a smaller number of larger files on a regular basis. This is known as compaction.\nYou can compact a table using the OPTIMIZE command.", "reference": "Explanation:\n\nBox 1: Run the VACUUM command on a schedule.\nRemove the files.\nRemove old files with the Delta Lake Vacuum Command\nYou can remove files marked for deletion (aka “tombstoned files ”) from storage with the Delta Lake\nvacuum command. Delta Lake doesn't physically remove files from storage for operations that logically\ndelete the files. You need to use the vacuum command to physica lly remove files from storage that have\nbeen marked for deletion and are older than the retention perio d.\nThe main benefit of vacuuming is to save on storage costs. Vacu uming does not make your queries run\nany faster and can limit your ability to time travel to earlier Delta table versions. You need to weigh the\ncosts/benefits for each of your tables to develop an optimal va cuum strategy. Some tables should be\nvacuumed frequently. Other tables should never be vacuumed.\nBox 2: Run the OPTIMIZE command on a schedule.\nCombine the files.\nBest practices: Delta Lake\nCompact filesIf you continuously write data to a Delta table, it will over t ime accumulate a large number of files,\nespecially if you add data in small batches. This can have an a dverse effect on the efficiency of table\nreads, and it can also affect the performance of your file syst em. Ideally, a large number of small files\nshould be rewritten into a smaller number of larger files on a regular basis. This is known as compaction.\nYou can compact a table using the OPTIMIZE command.Reference:\nhttps://delta.io/blog/remove-files-delta-lake-vacuum-command/\nhttps://docs.databricks.com/en/delta/best-practices.html", "boxes": [{"box": 1, "value": "Run the VACUUM command on a schedule."}, {"box": 2, "value": "Run the OPTIMIZE command on a schedule."}], "images": ["images/p023.jpg", "images/p024.jpg"]}, {"id": 17, "shown": "6", "type": "hotspot", "stem": "You have a Fabric workspace named Workspace1 and an Azure Data Lake Storage Gen2 account named\nstorage1. Workspace1 contains a lakehouse named Lakehouse1.\nYou need to create a shortcut to storage1 in Lakehouse1.Which protocol and endpoint should you specify? To answer, sele ct the appropriate options in the answer\narea.\nNOTE:  Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: abfss\nAccess Azure storageOnce you have properly configured credentials to access your Az ure storage container, you can interact\nwith resources in the storage account using URIs. Databricks re commends using the abfss driver for\ngreater security.\nspark.read.load(\"abfss://<container-name>@<storage-account-name >.dfs.core.windows.net/<path-to-\n\ndata>\")\ndbutils.fs.ls(\"abfss://<container-name>@<storage-account-name>. dfs.core.windows.net/<path-to-data>\")\n CREATE TABLE <database-name>.<table-name>;\nCOPY INTO <database-name>.<table-name>\nFROM 'abfss://container@storageAccount.dfs.core.windows.net/pat h/to/folder'\nFILEFORMAT = CSVCOPY_OPTIONS ('mergeSchema' = 'true');\nBox 2: dfs\ndfs is used for the endpoint:\ndbutils.fs.ls(\"abfss://<container-name>@<storage-account-name>. dfs.core.windows.net/<path-to-data>\")", "reference": "Explanation:\nBox 1: abfss\nAccess Azure storageOnce you have properly configured credentials to access your Az ure storage container, you can interact\nwith resources in the storage account using URIs. Databricks re commends using the abfss driver for\ngreater security.\nspark.read.load(\"abfss://<container-name>@<storage-account-name >.dfs.core.windows.net/<path-to-\n\ndata>\")\ndbutils.fs.ls(\"abfss://<container-name>@<storage-account-name>. dfs.core.windows.net/<path-to-data>\")\n CREATE TABLE <database-name>.<table-name>;\nCOPY INTO <database-name>.<table-name>\nFROM 'abfss://container@storageAccount.dfs.core.windows.net/pat h/to/folder'\nFILEFORMAT = CSVCOPY_OPTIONS ('mergeSchema' = 'true');\nBox 2: dfs\ndfs is used for the endpoint:\ndbutils.fs.ls(\"abfss://<container-name>@<storage-account-name>. dfs.core.windows.net/<path-to-data>\")\nReference:\nhttps://docs.databricks.com/en/connect/storage/azure-storage.ht ml", "boxes": [{"box": 1, "value": "abfss"}, {"box": 2, "value": "dfs"}], "image": "images/p026.jpg"}, {"id": 18, "shown": "7", "type": "mc", "stem": "You have an Azure Repos Git repository named Repo1 and a Fabric -enabled Microsoft Power BI Premium\ncapacity. The capacity contains two workspaces named Workspace1  and Workspace2. Git integration is\nenabled at the workspace level.\nYou plan to use Microsoft Power BI Desktop and Workspace1 to ma ke version-controlled changes to a\nsemantic model stored in Repo1. The changes will be built and d eployed to Workspace2 by using Azure\nPipelines.\nYou need to ensure that report and semantic model definitions a re saved as individual text files in a folder\nhierarchy. The solution must minimize development and maintenan ce effort.\nIn which file format should you save the changes?", "options": [{"key": "A", "text": "PBIP"}, {"key": "B", "text": "PBIDS"}, {"key": "C", "text": "PBIT"}, {"key": "D", "text": "PBIX"}], "correct": "A", "multi": false, "explanation": "Power BI Desktop projects (PREVIEW)Power BI Desktop introduces a new way to author, collaborate, a nd save your projects. You can now save\nyour work as a Power BI Project (PBIP). As a project, report an d semantic model item definitions are saved\nas individual plain text files in a simple, intuitive folder st ructure.", "reference": "Explanation:\nPower BI Desktop projects (PREVIEW)Power BI Desktop introduces a new way to author, collaborate, a nd save your projects. You can now save\nyour work as a Power BI Project (PBIP). As a project, report an d semantic model item definitions are saved\nas individual plain text files in a simple, intuitive folder st ructure.\nReference:\nhttps://learn.microsoft.com/en-us/power-bi/developer/projects/p rojects-overview", "boxes": []}, {"id": 19, "shown": "8", "type": "mc", "stem": "You have a Fabric tenant that contains a lakehouse named Lakeho use1. Lakehouse1 contains a Delta\ntable that has one million Parquet files.\nYou need to remove files that were NOT referenced by the table during the past 30 days. The solution\nmust ensure that the transaction log remains consistent, and th e ACID properties of the table are\nmaintained.\nWhat should you do?", "options": [{"key": "A", "text": "From OneLake file explorer, delete the files."}, {"key": "B", "text": "Run the OPTIMIZE command and specify the Z-order parameter."}, {"key": "C", "text": "Run the OPTIMIZE command and specify the V-order parameter."}, {"key": "D", "text": "Run the VACUUM command."}], "correct": "D", "multi": false, "explanation": "VACUUMApplies to: check marked yes Databricks SQL check marked yes Da tabricks Runtime\nRemove unused files from a table directory.\nVACUUM removes all files from the table directory that are not managed by Delta, as well as data files that\nare no longer in the latest state of the transaction log for th e table and are older than a retention threshold.\nIncorrect:\nNot B: What is Z order optimization?Z-ordering is a technique to colocate related information in th e same set of files. This co-locality is\nautomatically used by Delta Lake on Azure Databricks data-skipp ing algorithms. This behavior dramatically\nreduces the amount of data that Delta Lake on Azure Databricks needs to read.\nNot C: Delta Lake table optimization and V-Order\nV-Order is a write time optimization to the parquet file format that enables lightning-fast reads under the\nMicrosoft Fabric compute engines, such as Power BI, SQL, Spark, and others.\nPower BI and SQL engines make use of Microsoft Verti-Scan techn ology and V-Ordered parquet files to\nachieve in-memory like data access times. Spark and other non-V erti-Scan compute engines also benefit\nfrom the V-Ordered files with an average of 10% faster read tim es, with some scenarios up to 50%.\nV-Order works by applying special sorting, row group distributi on, dictionary encoding and compression on\nparquet files, thus requiring less network, disk, and CPU resou rces in compute engines to read it, providing\ncost efficiency and performance. V-Order sorting has a 15% impa ct on average write times but provides up\nto 50% more compression.", "reference": "Explanation:\nVACUUMApplies to: check marked yes Databricks SQL check marked yes Da tabricks Runtime\nRemove unused files from a table directory.\nVACUUM removes all files from the table directory that are not managed by Delta, as well as data files that\nare no longer in the latest state of the transaction log for th e table and are older than a retention threshold.\nIncorrect:\nNot B: What is Z order optimization?Z-ordering is a technique to colocate related information in th e same set of files. This co-locality is\nautomatically used by Delta Lake on Azure Databricks data-skipp ing algorithms. This behavior dramatically\nreduces the amount of data that Delta Lake on Azure Databricks needs to read.\nNot C: Delta Lake table optimization and V-Order\nV-Order is a write time optimization to the parquet file format that enables lightning-fast reads under the\nMicrosoft Fabric compute engines, such as Power BI, SQL, Spark, and others.\nPower BI and SQL engines make use of Microsoft Verti-Scan techn ology and V-Ordered parquet files to\nachieve in-memory like data access times. Spark and other non-V erti-Scan compute engines also benefit\nfrom the V-Ordered files with an average of 10% faster read tim es, with some scenarios up to 50%.\nV-Order works by applying special sorting, row group distributi on, dictionary encoding and compression on\nparquet files, thus requiring less network, disk, and CPU resou rces in compute engines to read it, providing\ncost efficiency and performance. V-Order sorting has a 15% impa ct on average write times but provides up\nto 50% more compression.\nReference:\nhttps://docs.databricks.com/en/sql/language-manual/delta-vacuum .html\nhttps://learn.microsoft.com/en-us/fabric/data-engineering/delta -optimization-and-v-order?", "boxes": []}, {"id": 20, "shown": "9", "type": "mc", "stem": "You have a Fabric tenant that contains a lakehouse named Lakeho use1.\nYou need to prevent new tables added to Lakehouse1 from being a dded automatically to the default\nsemantic model of the lakehouse.\nWhat should you configure?", "options": [{"key": "A", "text": "the SQL analytics endpoint settings"}, {"key": "B", "text": "the semantic model settings"}, {"key": "C", "text": "the workspace settings"}, {"key": "D", "text": "the Lakehouse1 settings"}], "correct": "A", "multi": false, "explanation": "Default Power BI semantic models in Microsoft Fabric\nSync the default Power BI semantic model\n\nPreviously we auto added all tables and views in the Warehouse to the default Power BI semantic model.\nBased on feedback, we have modified the default behavior to not automatically add tables and views to the\ndefault Power BI semantic model. This change will ensure the ba ckground sync will not get triggered. This\nwill also disable some actions like \"New Measure\", \"Create Repo rt\", \"Analyze in Excel\".\nIf you want to change this default behavior, you can:1. Manually enable the Sync the default Power BI semantic model setting for each Warehouse or SQL\nanalytics endpoint in the workspace. This will restart the back ground sync that will incur some consumption\ncosts.\n2. Manually pick tables and views to be added to semantic model through Manage default Power BI\nsemantic model in the ribbon or info bar.\nNote: Understand what's in the default Power BI semantic model\nWhen you create a Warehouse or SQL analytics endpoint, a defaul t Power BI semantic model is created.\nThe default semantic model is represented with the (default) su ffix.", "reference": "Explanation:\nDefault Power BI semantic models in Microsoft Fabric\nSync the default Power BI semantic model\n\nPreviously we auto added all tables and views in the Warehouse to the default Power BI semantic model.\nBased on feedback, we have modified the default behavior to not automatically add tables and views to the\ndefault Power BI semantic model. This change will ensure the ba ckground sync will not get triggered. This\nwill also disable some actions like \"New Measure\", \"Create Repo rt\", \"Analyze in Excel\".\nIf you want to change this default behavior, you can:1. Manually enable the Sync the default Power BI semantic model setting for each Warehouse or SQL\nanalytics endpoint in the workspace. This will restart the back ground sync that will incur some consumption\ncosts.\n2. Manually pick tables and views to be added to semantic model through Manage default Power BI\nsemantic model in the ribbon or info bar.\nNote: Understand what's in the default Power BI semantic model\nWhen you create a Warehouse or SQL analytics endpoint, a defaul t Power BI semantic model is created.\nThe default semantic model is represented with the (default) su ffix.\nReference:\nhttps://learn.microsoft.com/en-us/fabric/data-warehouse/semanti c-models", "boxes": []}, {"id": 21, "shown": "10", "type": "mc", "stem": "You have a Fabric tenant that contains JSON files in OneLake. T he files have one billion items.\nYou plan to perform time series analysis of the items.You need to transform the data, visualize the data to find insi ghts, perform anomaly detection, and share\nthe insights with other business users. The solution must meet the following requirements:\nUse parallel processing.\nMinimize the duplication of data.\nMinimize how long it takes to load the data.\nWhat should you use to transform and visualize the data?", "options": [{"key": "A", "text": "the PySpark library in a Fabric notebook"}, {"key": "B", "text": "the pandas library in a Fabric notebook"}, {"key": "C", "text": "a Microsoft Power BI report that uses core visuals"}], "correct": "A", "multi": false, "explanation": "PySpark vs Pandas PerformancePyspark has been created to help us work with big data on distr ibuted systems. On the other hand, the\npandas module is used to manipulate and analyze datasets up to a few GigaBytes (Less than 10 GB to be\nspecific). So, PySpark, when used with a distributed computing system, gives better performance than\npandas. Pyspark also uses resilient distributed datasets (RDDs) to work parallel on the data. Hence, it\nperforms better than pandas.\n\nNote: PySpark is a Python library that provides an interface fo r Apache Spark. Spark is an open-source\nframework for big data processing. Spark is built to process la rge amounts of data quickly by distributing\ncomputing tasks across a cluster of machines.\nPySpark allows us to use Apache Spark and its ecosystem of libr aries, such as Spark SQL for working\nwith structured data. We can also use Spark MLlib for machine learning and GraphX for graph processing using Pyspark in\nPython. PySpark supports many data sources, including Hadoop Distribute d File System (HDFS), Apache\nCassandra, and Amazon S3. Along with the data processing capabilities, we can also use py spark with popular Python libraries such as\nNumPy and Pandas.", "reference": "Explanation:\nPySpark vs Pandas PerformancePyspark has been created to help us work with big data on distr ibuted systems. On the other hand, the\npandas module is used to manipulate and analyze datasets up to a few GigaBytes (Less than 10 GB to be\nspecific). So, PySpark, when used with a distributed computing system, gives better performance than\npandas. Pyspark also uses resilient distributed datasets (RDDs) to work parallel on the data. Hence, it\nperforms better than pandas.\n\nNote: PySpark is a Python library that provides an interface fo r Apache Spark. Spark is an open-source\nframework for big data processing. Spark is built to process la rge amounts of data quickly by distributing\ncomputing tasks across a cluster of machines.\nPySpark allows us to use Apache Spark and its ecosystem of libr aries, such as Spark SQL for working\nwith structured data. We can also use Spark MLlib for machine learning and GraphX for graph processing using Pyspark in\nPython. PySpark supports many data sources, including Hadoop Distribute d File System (HDFS), Apache\nCassandra, and Amazon S3. Along with the data processing capabilities, we can also use py spark with popular Python libraries such as\nNumPy and Pandas.\nReference:\nhttps://www.codeconquest.com/blog/pyspark-vs-pandas-performance -memory-consumption-and-use-\ncases", "boxes": [], "image": "images/p029.jpg"}, {"id": 22, "shown": "11", "type": "mc", "stem": "You have a Fabric tenant that contains two workspaces named Wor kspace1 and Workspace2 and a user\nnamed User1.\nYou need to ensure that User1 can perform the following tasks:\nCreate a new domain.\nCreate two subdomains named subdomain1 and subdomain2.\nAssign Workspace1 to subdomain1.\nAssign Workspace2 to subdomain2.\nThe solution must follow the principle of least privilege.Which role should you assign to User1?", "options": [{"key": "A", "text": "domain admin"}, {"key": "B", "text": "domain contributor"}, {"key": "C", "text": "Fabric admin"}, {"key": "D", "text": "workspace Admin"}], "correct": "A", "multi": false, "explanation": "To achieve the tasks described, User1 needs permissions to mana ge domains and assign workspaces to\nsubdomains. Here’s a breakdown of the required tasks and the pe rmissions needed:\n1.Create a new domain:\nThis requires the ability to manage domains, which is specifica lly granted by the domain admin role.\n2.Create two subdomains (subdomain1 and subdomain2):\nSubdomains fall under domain management, so the domain admin role is required.\n3.Assign workspaces (Workspace1 and Workspace2) to subdomains:\nAssigning workspaces to subdomains is also a domain management task, which the domain admin\nrole permits.", "reference": "Explanation:\nTo achieve the tasks described, User1 needs permissions to mana ge domains and assign workspaces to\nsubdomains. Here’s a breakdown of the required tasks and the pe rmissions needed:\n1.Create a new domain:\nThis requires the ability to manage domains, which is specifica lly granted by the domain admin role.\n2.Create two subdomains (subdomain1 and subdomain2):\nSubdomains fall under domain management, so the domain admin role is required.\n3.Assign workspaces (Workspace1 and Workspace2) to subdomains:\nAssigning workspaces to subdomains is also a domain management task, which the domain admin\nrole permits.", "boxes": [], "image": "images/p030.jpg"}, {"id": 23, "shown": "12", "type": "hotspot", "stem": "You have a Fabric tenant that contains three users named User1,  User2, and User3. The tenant contains a\nsecurity group named Group1. User1 and User3 are members of Gro up1.\nThe tenant contains the workspaces shown in the following table .\nThe tenant contains the domains shown in the following table.\nUser1 creates a new workspace named Workspace3.\nYou assign Domain1 as the default domain of Group1.For each of the following statements, select Yes if the stateme nt is true. Otherwise, select No.\nNOTE : Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "User2 is assigned the Contributor role for Workspace3 - No\nUser2 is not a member of Group1 , and Workspace3 is created by User1. Since Workspace3 is assig ned\nto Domain1 (default domain of Group1 ), only members of Group1 will have permissions based on their\nrole in the domain. User2 is not part of Group1 , so they have no role in Workspace3.\n\nUser3 is assigned the Viewer role for Workspace3 - No\nUser3 is a member of Group1 , and the default domain ( Domain1 ) is assigned to Group1 . However, there\nis no indication that User3 has been explicitly granted the Vie wer role in Workspace3. If permissions were\ninherited, User3 would have the default role for Domain1 , but the problem does not specify this explicitly,\nso we assume no Viewer role is assigned.\nUser3 is assigned the Contributor role for Workspace1 - No\nWorkspace1 is explicitly assigned to User1 as the admin. There is no indication that User3 has any\npermissions for Workspace1. Being a member of Group1 does not grant automatic Contributor access to\na workspace unless explicitly configured.", "reference": "Explanation:\nUser2 is assigned the Contributor role for Workspace3 - No\nUser2 is not a member of Group1 , and Workspace3 is created by User1. Since Workspace3 is assig ned\nto Domain1 (default domain of Group1 ), only members of Group1 will have permissions based on their\nrole in the domain. User2 is not part of Group1 , so they have no role in Workspace3.\n\nUser3 is assigned the Viewer role for Workspace3 - No\nUser3 is a member of Group1 , and the default domain ( Domain1 ) is assigned to Group1 . However, there\nis no indication that User3 has been explicitly granted the Vie wer role in Workspace3. If permissions were\ninherited, User3 would have the default role for Domain1 , but the problem does not specify this explicitly,\nso we assume no Viewer role is assigned.\nUser3 is assigned the Contributor role for Workspace1 - No\nWorkspace1 is explicitly assigned to User1 as the admin. There is no indication that User3 has any\npermissions for Workspace1. Being a member of Group1 does not grant automatic Contributor access to\na workspace unless explicitly configured.", "boxes": [], "images": ["images/p030.jpg", "images/p031.jpg"]}, {"id": 24, "shown": "13", "type": "mc", "stem": "You have a Fabric warehouse named Warehouse1 that contains a ta ble named Table1. Table1 contains\ncustomer data.\nYou need to implement row-level security (RLS) for Table1. The solution must ensure that users can see\nonly their respective data.\nWhich two objects should you create? Each correct answer presen ts part of the solution.\nNOTE : Each correct selection is worth one point.", "options": [{"key": "A", "text": "DATABASE ROLE"}, {"key": "B", "text": "STORED PROCEDURE"}, {"key": "C", "text": "CONSTRAINT"}, {"key": "D", "text": "FUNCTION"}, {"key": "E", "text": "SECURITY POLICY"}], "correct": "AE", "multi": true, "explanation": "A database role is used to assign permissions to users or groups. In the conte xt of RLS, you create roles\nthat map to specific user groups or individuals, determining wh ich rows they can access.\nA security policy is used to enforce row-level security. This is done by creatin g a filter predicate that limits\nthe rows returned based on a condition, such as the user's iden tity or a specific column value.", "reference": "Explanation:\nA database role is used to assign permissions to users or groups. In the conte xt of RLS, you create roles\nthat map to specific user groups or individuals, determining wh ich rows they can access.\nA security policy is used to enforce row-level security. This is done by creatin g a filter predicate that limits\nthe rows returned based on a condition, such as the user's iden tity or a specific column value.", "boxes": [], "image": "images/p032.jpg"}, {"id": 25, "shown": "14", "type": "mc", "stem": "You are the administrator of a Fabric workspace that contains a  lakehouse named Lakehouse1.\nLakehouse1 contains the following tables:\nTable1: A Delta table created by using a shortcut\nTable2: An external table created by using Spark\nTable3: A managed table\nYou plan to connect to Lakehouse1 by using its SQL endpoint.What will you be able to do after connecting to Lakehouse1?", "options": [{"key": "A", "text": "Read Table3."}, {"key": "B", "text": "Update the data Table3."}, {"key": "C", "text": "Read Table2."}, {"key": "D", "text": "Update the data in Table1."}], "correct": "A", "multi": false, "explanation": "", "reference": "", "boxes": [], "image": "images/p032.jpg"}, {"id": 26, "shown": "15", "type": "hotspot", "stem": "You have a Fabric tenant that contains a workspace named Worksp ace1. Workspace1 contains a\nwarehouse named DW1. DW1 contains two tables named Employees an d Sales. All users have read\naccess to Dw1.\nYou need to implement access controls to meet the following req uirements:\nFor the Sales table, ensure that the users can see only the sal es data from their respective region.\nFor the Employees table, restrict access to all Personally Iden tifiable Information (PII).\nMaintain access to unrestricted data for all the users.\nWhat should you use for each table? To answer, select the appro priate options in the answer area.\nNOTE:  Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: Column-level security\nFor the Employees table, restrict access to all Personally Iden tifiable Information (PII).\nSynapse Analytics, Column-level security\nColumn-Level security allows customers to control access to tab le columns based on the user's execution\ncontext or group membership.\nColumn-level security simplifies the design and coding of secur ity in your application, allowing you to\nrestrict column access to protect sensitive data. For example, ensuring that specific users can access only\ncertain columns of a table pertinent to their department.\nUse cases\nSome examples of how column-level security is being used today:\nA financial services firm allows only account managers to have access to customer social security\nnumbers (SSN), phone numbers, and other personal data.A health care provider allows only doctors and nurses to have a ccess to sensitive medical records while\npreventing members of the billing department from viewing this data.\nBox 2: Row-level security (RLS)\nFor the Sales table, ensure that the users can see only the sal es data from their respective region.\nSQL Server, Row-level security\nRow-level security (RLS) enables you to use group membership or execution context to control access to\nrows in a database table.\nRow-level security simplifies the design and coding of security in your application. RLS helps you\nimplement restrictions on data row access. For example, you can ensure that workers access only those\ndata rows that are pertinent to their department. Another examp le is to restrict customers' data access to\nonly the data relevant to their company.\n\nUse cases\nHere are design examples of how row-level security (RLS) can be used:\nA hospital can create a security policy that allows nurses to v iew data rows for their patients only.\nA bank can create a policy to restrict access to financial data rows based on an employee's business\ndivision or role in the company.\nA multitenant application can create a policy to enforce a logi cal separation of each tenant's data rows\nfrom every other tenant's rows. Efficiencies are achieved by th e storage of data for many tenants in a\nsingle table. Each tenant can see only its data rows.", "reference": "Explanation:\nBox 1: Column-level security\nFor the Employees table, restrict access to all Personally Iden tifiable Information (PII).\nSynapse Analytics, Column-level security\nColumn-Level security allows customers to control access to tab le columns based on the user's execution\ncontext or group membership.\nColumn-level security simplifies the design and coding of secur ity in your application, allowing you to\nrestrict column access to protect sensitive data. For example, ensuring that specific users can access only\ncertain columns of a table pertinent to their department.\nUse cases\nSome examples of how column-level security is being used today:\nA financial services firm allows only account managers to have access to customer social security\nnumbers (SSN), phone numbers, and other personal data.A health care provider allows only doctors and nurses to have a ccess to sensitive medical records while\npreventing members of the billing department from viewing this data.\nBox 2: Row-level security (RLS)\nFor the Sales table, ensure that the users can see only the sal es data from their respective region.\nSQL Server, Row-level security\nRow-level security (RLS) enables you to use group membership or execution context to control access to\nrows in a database table.\nRow-level security simplifies the design and coding of security in your application. RLS helps you\nimplement restrictions on data row access. For example, you can ensure that workers access only those\ndata rows that are pertinent to their department. Another examp le is to restrict customers' data access to\nonly the data relevant to their company.\n\nUse cases\nHere are design examples of how row-level security (RLS) can be used:\nA hospital can create a security policy that allows nurses to v iew data rows for their patients only.\nA bank can create a policy to restrict access to financial data rows based on an employee's business\ndivision or role in the company.\nA multitenant application can create a policy to enforce a logi cal separation of each tenant's data rows\nfrom every other tenant's rows. Efficiencies are achieved by th e storage of data for many tenants in a\nsingle table. Each tenant can see only its data rows.\nReference:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-d ata-warehouse/column-level-security\nhttps://learn.microsoft.com/en-us/sql/relational-databases/secu rity/row-level-security", "boxes": [{"box": 1, "value": "Column-level security"}, {"box": 2, "value": "Row-level security (RLS)"}], "images": ["images/p033.jpg", "images/p034.jpg"]}, {"id": 27, "shown": "16", "type": "mc", "stem": "You have a Fabric tenant that contains a workspace named Worksp ace1 and a user named User1. User1\nis assigned the Contributor role for Workspace1.\nYou plan to configure Workspace1 to use an Azure DevOps reposit ory for version control.\nYou need to ensure that User1 can commit items to the repositor y.\nWhich two settings should you enable for User1? Each correct an swer presents part of the solution.\nNOTE: Each correct selection is worth one point.", "options": [{"key": "A", "text": "Users can sync workspace items with GitHub repositories"}, {"key": "B", "text": "Users can create and use Data workflows"}, {"key": "C", "text": "Users can create Fabric items"}, {"key": "D", "text": "Users can synchronize workspace items with their Git reposito ries"}], "correct": "CD", "multi": true, "explanation": "To integrate Git with your Microsoft Fabric workspace, you need to set up the following prerequisites for\nboth Fabric and Git.\nFabric prerequisites\nTo access the Git integration feature, you need a Fabric capaci ty. A Fabric capacity is required to use all\nsupported Fabric items\nIn addition, the following tenant switches must be enabled from the Admin portal:\n* (C) Users can create Fabric items\n* (D) Users can synchronize workspace items with their Git repo sitories\n* For GitHub users only: Users can synchronize workspace items with GitHub repositories\nThese switches can be enabled by the tenant admin, capacity adm in, or workspace admin, depending on\nyour organization's settings.", "reference": "Explanation:\nTo integrate Git with your Microsoft Fabric workspace, you need to set up the following prerequisites for\nboth Fabric and Git.\nFabric prerequisites\nTo access the Git integration feature, you need a Fabric capaci ty. A Fabric capacity is required to use all\nsupported Fabric items\nIn addition, the following tenant switches must be enabled from the Admin portal:\n* (C) Users can create Fabric items\n* (D) Users can synchronize workspace items with their Git repo sitories\n* For GitHub users only: Users can synchronize workspace items with GitHub repositories\nThese switches can be enabled by the tenant admin, capacity adm in, or workspace admin, depending on\nyour organization's settings.\nReference:\n https://learn.microsoft.com/en-us/fabric/cicd/git-integration/g it-get-started", "boxes": []}, {"id": 28, "shown": "17", "type": "mc", "stem": "You have a Fabric tenant that contains a workspace named Worksp ace1. Workspace1 contains a data\npipeline named Pipeline1 and a lakehouse named Lakehouse1.\nYou perform the following actions:\nCreate a workspace named Workspace2.\nCreate a deployment pipeline named DeployPipeline1 that will de ploy items from Workspace1 to\nWorkspace2.\nAdd a folder named Folder1 to Workspace1.\nMove Lakehouse1 to Folder1.\nRun DeployPipeline1.\nWhich structure will Workspace2 have when DeployPipeline1 is co mplete?\n\\Folder1\\Lakehouse1\n\\Lakehouse1\n\\Folder1\\Lakehouse1", "options": [{"key": "A", "text": "\\Folder1\\Pipeline1"}, {"key": "B", "text": "\\Pipeline1"}, {"key": "C", "text": "\\Pipeline1"}, {"key": "D", "text": "\\Folder1\\Lakehouse1"}], "correct": "D", "multi": false, "explanation": "The folder structure is copied.\nNote 1:\nFolders in deployment pipelinesFolders enable users to efficiently organize and manage workspa ce items in a familiar way. When you\ndeploy content that contains folders to a different stage, the folder hierarchy of the applied items is\nautomatically applied.\nIn Deployment pipelines, folders are considered part of an item ’s name (an item name includes its full\npath).\nNote 2: Microsoft Fabric, The deployment pipelines process\nThe deployment process lets you clone content from one stage in the deployment pipeline to another,\ntypically from development to test, and from test to production .\nDuring deployment, Microsoft Fabric copies the content from the source stage to the target stage. The\nconnections between the copied items are kept during the copy p rocess.\nDeploying content from a working production pipeline to a stage that has an existing workspace, includes\nthe following steps:\nDeploying new content as an addition to the content already the re.\nDeploying updated content to replace some of the content alread y there.", "reference": "Explanation:\nThe folder structure is copied.\nNote 1:\nFolders in deployment pipelinesFolders enable users to efficiently organize and manage workspa ce items in a familiar way. When you\ndeploy content that contains folders to a different stage, the folder hierarchy of the applied items is\nautomatically applied.\nIn Deployment pipelines, folders are considered part of an item ’s name (an item name includes its full\npath).\nNote 2: Microsoft Fabric, The deployment pipelines process\nThe deployment process lets you clone content from one stage in the deployment pipeline to another,\ntypically from development to test, and from test to production .\nDuring deployment, Microsoft Fabric copies the content from the source stage to the target stage. The\nconnections between the copied items are kept during the copy p rocess.\nDeploying content from a working production pipeline to a stage that has an existing workspace, includes\nthe following steps:\nDeploying new content as an addition to the content already the re.\nDeploying updated content to replace some of the content alread y there.\nReference:\nhttps://learn.microsoft.com/en-us/fabric/cicd/deployment-pipeli nes/understand-the-deployment-process\nhttps://learn.microsoft.com/en-us/fabric/cicd/deployment-pipeli nes/understand-the-deployment-process", "boxes": []}, {"id": 29, "shown": "18", "type": "mc", "stem": "Your company has a finance department.\nYou have a Fabric tenant, an Azure Storage account named storag e1, and a Microsoft Entra group named\nGroup1. Group1 contains the users in the finance department.\nYou need to create a new workspace named Workspace1 in the tena nt. The solution must meet the\nfollowing requirements:\nEnsure that the finance department users can create and edit it ems in Workspace1.\nEnsure that Workspace1 can securely access storage1 to read and  write data.\nEnsure that you are the only admin of Workspace1.\nMinimize administrative effort.\nYou create Workspace1.Which two actions should you perform next? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.", "options": [{"key": "A", "text": "Assign the Contributor role to Group1."}, {"key": "B", "text": "Create a workspace identity."}, {"key": "C", "text": "Assign the Admin role to yourself."}, {"key": "D", "text": "Assign the Contributor role to each finance department user."}], "correct": "AB", "multi": true, "explanation": "A: Ensure that the finance department users can create and edit items in Workspace1.\nGroup1 contains the users in the finance department.\nB: Ensure that Workspace1 can securely access storage1 to read and write data.\nA Fabric workspace identity is an automatically managed service principal that can be associated with a\nFabric workspace. Fabric workspaces with a workspace identity c an securely read or write to firewall-\nenabled Azure Data Lake Storage Gen2 accounts through trusted w orkspace access for OneLake\nshortcuts. Fabric items can use the identity when connecting to resources that support Microsoft Entra\nauthentication. Fabric uses workspace identities to obtain Micr osoft Entra tokens without the customer\nhaving to manage any credentials.\nWorkspace identities can be created in the workspace settings o f any workspace except My workspaces.\nA workspace identity is automatically assigned the workspace co ntributor role and has access to\nworkspace items.\nIncorrect:\nNot D: More administrative effort.", "reference": "Explanation:\nA: Ensure that the finance department users can create and edit items in Workspace1.\nGroup1 contains the users in the finance department.\nB: Ensure that Workspace1 can securely access storage1 to read and write data.\nA Fabric workspace identity is an automatically managed service principal that can be associated with a\nFabric workspace. Fabric workspaces with a workspace identity c an securely read or write to firewall-\nenabled Azure Data Lake Storage Gen2 accounts through trusted w orkspace access for OneLake\nshortcuts. Fabric items can use the identity when connecting to resources that support Microsoft Entra\nauthentication. Fabric uses workspace identities to obtain Micr osoft Entra tokens without the customer\nhaving to manage any credentials.\nWorkspace identities can be created in the workspace settings o f any workspace except My workspaces.\nA workspace identity is automatically assigned the workspace co ntributor role and has access to\nworkspace items.\nIncorrect:\nNot D: More administrative effort.\nReference:\nhttps://learn.microsoft.com/en-us/fabric/security/workspace-ide ntity", "boxes": [], "image": "images/p036.jpg"}, {"id": 30, "shown": "19", "type": "mc", "stem": "You have a Fabric tenant that contains the workspaces shown in the following table.\nYou have a deployment pipeline named Pipeline1 that deploys ite ms from Workspace_DEV to\nWorkspace_TEST. In Pipeline1, all items that have matching name s are paired.\nYou deploy the contents of Workspace_DEV to Workspace_TEST by u sing Pipeline1.\nWhat will the contents of Workspace_TEST be once the deployment  is complete?\nLakehouse2Notebook1Notebook2Pipeline1SemanticModel1\nNotebook1Pipeline1SemanticModel1\nNotebook2SemanticModel1\nNotebook2Pipeline1SemanticModel1", "options": [{"key": "A", "text": "Lakehouse1"}, {"key": "B", "text": "Lakehouse1"}, {"key": "C", "text": "Lakehouse2"}, {"key": "D", "text": "Lakehouse2"}], "correct": "A", "multi": false, "explanation": "The items in Workspace_DEV is added to Workspace_TEST. The item s already in Workspace_TEST are\nkept.\nNote: Microsoft Fabric, The deployment pipelines process\nThe deployment process lets you clone content from one stage in the deployment pipeline to another,\ntypically from development to test, and from test to production .\nDuring deployment, Microsoft Fabric copies the content from the source stage to the target stage. The\nconnections between the copied items are kept during the copy p rocess.\nDeploying content from a working production pipeline to a stage that has an existing workspace, includes\nthe following steps:\nDeploying new content as an addition to the content already the re.\nDeploying updated content to replace some of the content alread y there.", "reference": "Explanation:\nThe items in Workspace_DEV is added to Workspace_TEST. The item s already in Workspace_TEST are\nkept.\nNote: Microsoft Fabric, The deployment pipelines process\nThe deployment process lets you clone content from one stage in the deployment pipeline to another,\ntypically from development to test, and from test to production .\nDuring deployment, Microsoft Fabric copies the content from the source stage to the target stage. The\nconnections between the copied items are kept during the copy p rocess.\nDeploying content from a working production pipeline to a stage that has an existing workspace, includes\nthe following steps:\nDeploying new content as an addition to the content already the re.\nDeploying updated content to replace some of the content alread y there.\nReference:\nhttps://learn.microsoft.com/en-us/fabric/cicd/deployment-pipeli nes/understand-the-deployment-process", "boxes": [], "image": "images/p037.jpg"}, {"id": 31, "shown": "20", "type": "hotspot", "stem": "You have a Fabric tenant that contains a workspace named Worksp ace_DEV. Workspace_DEV contains\nthe semantic models shown in the following table.\nWorkspace_DEV contains the dataflows shown in the following tab le.\nYou create a new workspace named Workspace_TEST.\nYou create a deployment pipeline named Pipeline1 to move items from Workspace_DEV to\nWorkspace_TEST.\nYou run Pipeline1.For each of the following statements, select Yes if the stateme nt is true. Otherwise, select No.\nNOTE:  Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: No\nNo - DF1 will be deployed to Workspace_TEST.\nDF1 is a Dataflow Gen1 and is configured with a scheduled refre sh policy.\nGen1 dataflows cannot be integrated into data pipelines.\nNote: Microsoft Fabric Data Factory, Getting from Dataflow Gene ration 1 to Dataflow Generation 2\n\nDataflow Gen2 is the new generation of dataflows. The new gener ation of dataflows resides alongside the\nPower BI Dataflow (Gen1) and brings new features and improved e xperiences. The following section\nprovides a comparison between Dataflow Gen1 and Dataflow Gen2.\nBox 2: YesYes - Data from Model1 will be deployed to Workspace_TEST.\nAssign a workspace to an empty stage\nWhen you assign content to an empty stage, a new workspace is c reated on a capacity for the stage you\ndeploy to. All the metadata in the reports, dashboards, and sem antic models of the original workspace is\ncopied to the new workspace in the stage you're deploying to.\nAfter the deployment is complete, refresh the semantic models s o that you can use the newly copied\ncontent. The semantic model refresh is required because data is n't copied from one stage to another.\nBox 3: Yes\nYes - The scheduled refresh policy for Model1 will be deployed to Workspace_TEST:", "reference": "Explanation:\nBox 1: No\nNo - DF1 will be deployed to Workspace_TEST.\nDF1 is a Dataflow Gen1 and is configured with a scheduled refre sh policy.\nGen1 dataflows cannot be integrated into data pipelines.\nNote: Microsoft Fabric Data Factory, Getting from Dataflow Gene ration 1 to Dataflow Generation 2\n\nDataflow Gen2 is the new generation of dataflows. The new gener ation of dataflows resides alongside the\nPower BI Dataflow (Gen1) and brings new features and improved e xperiences. The following section\nprovides a comparison between Dataflow Gen1 and Dataflow Gen2.\nBox 2: YesYes - Data from Model1 will be deployed to Workspace_TEST.\nAssign a workspace to an empty stage\nWhen you assign content to an empty stage, a new workspace is c reated on a capacity for the stage you\ndeploy to. All the metadata in the reports, dashboards, and sem antic models of the original workspace is\ncopied to the new workspace in the stage you're deploying to.\nAfter the deployment is complete, refresh the semantic models s o that you can use the newly copied\ncontent. The semantic model refresh is required because data is n't copied from one stage to another.\nBox 3: Yes\nYes - The scheduled refresh policy for Model1 will be deployed to Workspace_TEST:\nReference:\nhttps://learn.microsoft.com/en-us/fabric/data-factory/dataflows -gen2-overview", "boxes": [{"box": 1, "value": "No"}, {"box": 2, "value": "YesYes - Data from Model1 will be deployed to Workspace_TEST."}, {"box": 3, "value": "Yes"}], "images": ["images/p038.jpg", "images/p039.jpg"]}, {"id": 32, "shown": "21", "type": "mc", "stem": "You have a Fabric tenant.\nYou are creating a Fabric Data Factory pipeline.You have a stored procedure that returns the number of active c ustomers and their average sales for the\ncurrent month.\nYou need to add an activity that will execute the stored proced ure in a warehouse. The returned values\nmust be available to the downstream activities of the pipeline.\nWhich type of activity should you add?", "options": [{"key": "A", "text": "Get metadata"}, {"key": "B", "text": "Switch"}, {"key": "C", "text": "Lookup"}, {"key": "D", "text": "Append variable"}], "correct": "C", "multi": false, "explanation": "The Fabric Lookup activity can retrieve a dataset from any of t he data sources supported by Microsoft\nFabric. You can use it to dynamically determine which objects t o operate on in a subsequent activity,\ninstead of hard coding the object name. Some object examples ar e files and tables.\nLookup activity reads and returns the content of a configuratio n file or table. It also returns the result of\nexecuting a query or stored procedure. The output can be a sing leton value or an array of attributes, which\ncan be consumed in a subsequent copy, transformation, or contro l flow activities like ForEach activity.", "reference": "Explanation:\nThe Fabric Lookup activity can retrieve a dataset from any of t he data sources supported by Microsoft\nFabric. You can use it to dynamically determine which objects t o operate on in a subsequent activity,\ninstead of hard coding the object name. Some object examples ar e files and tables.\nLookup activity reads and returns the content of a configuratio n file or table. It also returns the result of\nexecuting a query or stored procedure. The output can be a sing leton value or an array of attributes, which\ncan be consumed in a subsequent copy, transformation, or contro l flow activities like ForEach activity.\nReference:\nhttps://learn.microsoft.com/en-us/fabric/data-factory/lookup-ac tivity", "boxes": [], "image": "images/p040.jpg"}, {"id": 33, "shown": "22", "type": "mc", "stem": "You have a Fabric tenant that contains a semantic model.\nYou need to modify object-level security (OLS) for the model.What should you use?", "options": [{"key": "A", "text": "the Fabric service"}, {"key": "B", "text": "Microsoft Power BI Desktop"}, {"key": "C", "text": "ALM Toolkit"}, {"key": "D", "text": "Tabular Editor"}], "correct": "D", "multi": false, "explanation": "Microsoft Fabric Security, Object-level security (OLS)\nTo create roles on Power BI Desktop semantic models, use extern al tools such as Tabular Editor.\nConfigure object-level security using tabular editor\n1. In Power BI Desktop, create the model and roles that will de fine your OLS rules.\n2. On the External Tools ribbon, select Tabular Editor. If you don’t see the Tabular Editor button, install the\nprogram. When open, Tabular Editor will automatically connect t o your model.\n\n3. In the Model view, select the drop-down menu under Roles. Th e roles you created in step one will\nappear.\n4. Select the role you want to enable an OLS definition for, an d expand the Table Permissions.\n\n5. Set the permissions for the table or column to None or Read.\n6. After you define object-level security for the roles, save y our changes. Screenshot of saving role\ndefinitions.\n7. In Power BI Desktop, publish your semantic model to the Powe r BI Service.\n8. In the Power BI Service, navigate to the Security page by se lecting the more options menu on the\nsemantic model, and assign members or groups to their appropria te roles.", "reference": "Explanation:\n Microsoft Fabric Security, Object-level security (OLS)\nTo create roles on Power BI Desktop semantic models, use extern al tools such as Tabular Editor.\nConfigure object-level security using tabular editor\n1. In Power BI Desktop, create the model and roles that will de fine your OLS rules.\n2. On the External Tools ribbon, select Tabular Editor. If you don’t see the Tabular Editor button, install the\nprogram. When open, Tabular Editor will automatically connect t o your model.\n\n3. In the Model view, select the drop-down menu under Roles. Th e roles you created in step one will\nappear.\n4. Select the role you want to enable an OLS definition for, an d expand the Table Permissions.\n\n5. Set the permissions for the table or column to None or Read.\n6. After you define object-level security for the roles, save y our changes. Screenshot of saving role\ndefinitions.\n7. In Power BI Desktop, publish your semantic model to the Powe r BI Service.\n8. In the Power BI Service, navigate to the Security page by se lecting the more options menu on the\nsemantic model, and assign members or groups to their appropria te roles.\nReference:\nhttps://learn.microsoft.com/en-us/fabric/security/service-admin -object-level-security", "boxes": [], "image": "images/p042.jpg"}, {"id": 34, "shown": "23", "type": "hotspot", "stem": "You have a Fabric tenant that contains a workspace named Enterp rise. Enterprise contains a semantic\nmodel named Model1. Model1 contains a date parameter named Date 1 that was created in Power Query.\nYou build a deployment pipeline named Enterprise Data that incl udes two stages named Development and\nTest. You assign the Enterprise workspace to the Development st age.\nYou need to perform the following actions:\nCreate a workspace named Enterprise [Test] and assign the works pace to the Test stage.\nConfigure a rule that will modify the value of Date1 when chang es are deployed to the Test stage.\nWhich two settings should you use? To answer, select the approp riate settings in the answer area.\nNOTE:  Each correct answer is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: Add workspace button (with a +sign)\nCreate a workspace named Enterprise [Test] and assign the works pace to the Test stage.\nBox 2: Deployment rule [Upper right corner]\nConfigure a rule that will modify the value of Date1 when chang es are deployed to the Test stage.\nIn the pipeline stage you want to create a deployment rule for, select Deployment rules", "reference": "Explanation:\nBox 1: Add workspace button (with a +sign)\nCreate a workspace named Enterprise [Test] and assign the works pace to the Test stage.\nBox 2: Deployment rule [Upper right corner]\nConfigure a rule that will modify the value of Date1 when chang es are deployed to the Test stage.\nIn the pipeline stage you want to create a deployment rule for, select Deployment rules\nReference:\nhttps://learn.microsoft.com/en-us/fabric/cicd/deployment-pipeli nes/create-rules", "boxes": [{"box": 1, "value": "Add workspace button (with a +sign)"}, {"box": 2, "value": "Deployment rule [Upper right corner]"}], "images": ["images/p043.jpg", "images/p044.jpg"]}, {"id": 35, "shown": "24", "type": "mc", "stem": "You have a Fabric tenant that contains a workspace named Worksp ace1 and a user named User1.\nWorkspace1 contains a warehouse named DW1.\nYou share DW1 with User1 and assign User1 the default permissio ns for DW1.\nWhat can User1 do?", "options": [{"key": "A", "text": "Build reports by using the default dataset."}, {"key": "B", "text": "Read data from the tables in DW1."}, {"key": "C", "text": "Connect to DW1 via the Azure SQL Analytics endpoint."}, {"key": "D", "text": "Read the underlying Parquet files from OneLake."}], "correct": "A", "multi": false, "explanation": "By default, when a user is granted access to a Microsoft Fabric warehouse (DW1) , they receive the\nViewer role. The Viewer role allows users to:\nBuild reports using the default dataset associated with the warehouse.\nRead data from the dataset but not directly from tables unless explicitly granted additional\npermissions.", "reference": "Explanation:\nBy default, when a user is granted access to a Microsoft Fabric warehouse (DW1) , they receive the\nViewer role. The Viewer role allows users to:\nBuild reports using the default dataset associated with the warehouse.\nRead data from the dataset but not directly from tables unless explicitly granted additional\npermissions.", "boxes": [], "image": "images/p045.jpg"}, {"id": 36, "shown": "25", "type": "hotspot", "stem": "You have a Fabric tenant that contains a workspace named Worksp ace1 and a user named DBUser.\nWorkspace1 contains a lakehouse named Lakehouse1. DBUser does N OT have access to the tenant.\nYou grant DBUser access to Lakehouse1 as shown in the following  exhibit.\nUse the drop-down menus to select the answer choice that comple tes each statement based on the\ninformation presented in the graphic.\nNOTE : Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "DBUser can read the data in Lakehouse1 by using the OneLake endpoint.\nDBUser can query the data in Lakehouse1 by using the OneLake file explorer.\nDBUser has been granted permission to \"Read all Apache Spark\" d ata but not permission to read\nSQL endpoint data or build reports using the default semantic model .\nSince SQL endpoint access is not granted , DBUser cannot use TDS (Tabular Data Stream) or SSMS\n(SQL Server Management Studio) for querying .\nDBUser can still access OneLake data directly , which is why OneLake endpoint and OneLake file\nexplorer are the correct choices.", "reference": "Explanation:\nDBUser can read the data in Lakehouse1 by using the OneLake endpoint.\nDBUser can query the data in Lakehouse1 by using the OneLake file explorer.\nDBUser has been granted permission to \"Read all Apache Spark\" d ata but not permission to read\nSQL endpoint data or build reports using the default semantic model .\nSince SQL endpoint access is not granted , DBUser cannot use TDS (Tabular Data Stream) or SSMS\n(SQL Server Management Studio) for querying .\nDBUser can still access OneLake data directly , which is why OneLake endpoint and OneLake file\nexplorer are the correct choices.", "boxes": [], "images": ["images/p045.jpg", "images/p046.jpg"]}, {"id": 37, "shown": "26", "type": "dragdrop", "stem": "You have a Fabric workspace named Workspace1.You have three groups named Group1, Group2, and Group3.You need to assign a workspace role to each group. The solution  must follow the principle of least privilege\nand meet the following requirements:\nGroup1 must be able to write data to Workspace1, but be unable to add members to Workspace1.\nGroup2 must be able to configure and maintain the settings of W orkspace1.\nGroup3 must be able to write data and add members to Workspace1 , but be unable to delete\nWorkspace1.\nWhich workspace role should you assign to each group? To answer , drag the appropriate roles to the\ncorrect groups. Each role may be used once, more than once, or not at all. You may need to drag the split\nbar between panes or scroll to view content.\nNOTE : Each correct selection is worth one point.\nSelect and Place:", "options": [], "correct": "", "multi": false, "explanation": "Group1 (Can write data but ca nnot add members) → Contributor\nContributors can write, edit, and manage data, but cannot manage workspace settings or add/\nremove users.\nGroup2 (Can configure and maintain workspace settings) → Admin\nAdmins have full control over the workspace, including configuring se ttings, managing permissions, and\nmaintaining security policies.\nGroup3 (Can write data and add me mbers but cannot delete the wo rkspace) → Member\nMembers can add/remove members and write data, but they cannot delete the workspace or configure\nsettings at the admin level.", "reference": "Explanation:\nGroup1 (Can write data but ca nnot add members) → Contributor\nContributors can write, edit, and manage data, but cannot manage workspace settings or add/\nremove users.\nGroup2 (Can configure and maintain workspace settings) → Admin\nAdmins have full control over the workspace, including configuring se ttings, managing permissions, and\nmaintaining security policies.\nGroup3 (Can write data and add me mbers but cannot delete the wo rkspace) → Member\nMembers can add/remove members and write data, but they cannot delete the workspace or configure\nsettings at the admin level.", "boxes": [], "image": "images/p047.jpg"}, {"id": 38, "shown": "27", "type": "hotspot", "stem": "You have a Fabric workspace named Workspace1 that uses the Prem ium Per User (PPU) license mode\nand contains a semantic model named Model1.\nLarge semantic model storage format  is selected for Model1.\nYou need to ensure that tables imported into Model1 are written  automatically to Delta tables in OneLake.\nWhat should you do for Model1 and Workspace1? To answer, select  the appropriate options in the answer\narea.\nNOTE : Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: Enable OneLake integration\nModel1\nWith Microsoft OneLake integration for semantic models, data im ported into model tables can also be\nautomatically written to Delta tables in OneLake. The Delta for mat is the unified table format across all\ncompute engines in Microsoft Fabric. OneLake integration export s the data with all key performance\nfeatures enabled to provide more seamless data access with high er performance.\nBox 2: Change the license mode to Fabric capacity.\nWorkspace1\nOneLake integration for semantic models is supported on Power B I Premium P and Microsoft Fabric F\nSKUs only. It's not supported on Power BI Pro, Premium Per User , or Power BI Embedded A/EM SKUs.\nBefore enabling OneLake integration, you must have one or more import semantic models in a workspace\non a Power BI Premium or Fabric capacity. Import semantic model is a type of data model where data is\nfully imported into Power BI's in-memory storage, allowing fast and efficient querying.", "reference": "Explanation:\nBox 1: Enable OneLake integration\nModel1\nWith Microsoft OneLake integration for semantic models, data im ported into model tables can also be\nautomatically written to Delta tables in OneLake. The Delta for mat is the unified table format across all\ncompute engines in Microsoft Fabric. OneLake integration export s the data with all key performance\nfeatures enabled to provide more seamless data access with high er performance.\nBox 2: Change the license mode to Fabric capacity.\nWorkspace1\nOneLake integration for semantic models is supported on Power B I Premium P and Microsoft Fabric F\nSKUs only. It's not supported on Power BI Pro, Premium Per User , or Power BI Embedded A/EM SKUs.\nBefore enabling OneLake integration, you must have one or more import semantic models in a workspace\non a Power BI Premium or Fabric capacity. Import semantic model is a type of data model where data is\nfully imported into Power BI's in-memory storage, allowing fast and efficient querying.\nReference:\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/onelake-i ntegration-overview", "boxes": [{"box": 1, "value": "Enable OneLake integration"}, {"box": 2, "value": "Change the license mode to Fabric capacity."}], "image": "images/p048.jpg"}, {"id": 39, "shown": "28", "type": "mc", "stem": "You have a Fabric tenant that contains the workspaces shown in the following table.\nYou have a deployment pipeline named Pipeline1 that deploys ite ms from Workspace_DEV to\nWorkspace_TEST. In Pipeline1, all items that have matching name s are paired.\nYou deploy the contents of Workspace_DEV to Workspace_TEST by u sing Pipeline1.\nWhat will the contents of Workspace_TEST be once the deployment  is complete?\nNotebook2SemanticModel1\nNotebook1Pipeline1SemanticModel1\nLakehouse2Notebook1Notebook2Pipeline1SemanticModel1\nNotebook2Pipeline1SemanticModel1", "options": [{"key": "A", "text": "Lakehouse2"}, {"key": "B", "text": "Lakehouse1"}, {"key": "C", "text": "Lakehouse1"}, {"key": "D", "text": "Lakehouse2"}], "correct": "C", "multi": false, "explanation": "Lakehouse1, Notebook1 and Pipeline1 are copied to Workspace_TES T.\nSemanticModel1 is copied and replaces the old SemanticModel1 in Workspace_TEST.\nNote:\nIn a Fabric deployment pipeline, pipelines, lakehouses, noteboo ks, and semantic models are copied\nbetween workspaces using deployment pipelines. These pipelines leverage a \"paired\" approach, meaning\nthey establish a link between items of the same name and type i n different stages of the deployment\nprocess.\nThis process involves assigning a workspace to a specific stage in the pipeline and then either deploying\nnew, unpaired content or pairing existing content. During deplo yment, the metadata of the reports,\ndashboards, and semantic models (but not the data itself) is co pied to the new workspace. For\nlakehouses, the entire structure and definitions are copied, an d data can be moved using pipelines or\ndataflows.\nIncorrect:\nNot A: Pipeline1 should be in Workspace_TEST\nIn a Fabric deployment pipeline, the process typically involves copying pipelines (and other content) from\nthe source workspace to the destination workspace. This \"copyin g\" is done during the deployment phase,\nwith Fabric making a copy of the items from the source stage to the target stage, maintaining connections\nbetween the copied items. This ensures that the destination wor kspace has a replica of the source, ready\nfor testing or production.", "reference": "Explanation:\nLakehouse1, Notebook1 and Pipeline1 are copied to Workspace_TES T.\nSemanticModel1 is copied and replaces the old SemanticModel1 in Workspace_TEST.\nNote:\nIn a Fabric deployment pipeline, pipelines, lakehouses, noteboo ks, and semantic models are copied\nbetween workspaces using deployment pipelines. These pipelines leverage a \"paired\" approach, meaning\nthey establish a link between items of the same name and type i n different stages of the deployment\nprocess.\nThis process involves assigning a workspace to a specific stage in the pipeline and then either deploying\nnew, unpaired content or pairing existing content. During deplo yment, the metadata of the reports,\ndashboards, and semantic models (but not the data itself) is co pied to the new workspace. For\nlakehouses, the entire structure and definitions are copied, an d data can be moved using pipelines or\ndataflows.\nIncorrect:\nNot A: Pipeline1 should be in Workspace_TEST\nIn a Fabric deployment pipeline, the process typically involves copying pipelines (and other content) from\nthe source workspace to the destination workspace. This \"copyin g\" is done during the deployment phase,\nwith Fabric making a copy of the items from the source stage to the target stage, maintaining connections\nbetween the copied items. This ensures that the destination wor kspace has a replica of the source, ready\nfor testing or production.\nReference:\nhttps://learn.microsoft.com/en-us/fabric/cicd/deployment-pipeli nes/understand-the-deployment-process", "boxes": [], "image": "images/p049.jpg"}, {"id": 40, "shown": "29", "type": "mc", "stem": "You have a Fabric tenant that contains a workspace named Worksp ace1 and a user named User1.\nWorkspace1 contains a warehouse named DW1.\nYou share DW1 with User1 and assign User1 the default permissio ns for DW1.\nWhat can User1 do?", "options": [{"key": "A", "text": "Read data from the tables in DW1."}, {"key": "B", "text": "Build reports by using the default dataset."}, {"key": "C", "text": "Read the underlying Parquet files from OneLake."}, {"key": "D", "text": "Connect to DW1 via the TDS (Tabular Data Stream) endpoint."}], "correct": "D", "multi": false, "explanation": "Assigning a user default permission (\"Read\") to a warehouse in a Fabric workspace enables them to\nconnect to the SQL analytics endpoint, which is the equivalent of CONNECT permissions in SQL Server.\nHowever, this permission only allows them to connect; it doesn' t grant them the ability to query tables,\nviews, functions, or stored procedures within the warehouse unl ess they're also given access to those\nspecific objects through T-SQL GRANT statements.\nIn essence, the default \"Read\" permission provides the necessar y connectivity to the warehouse's SQL\nanalytics endpoint, but it does not automatically grant access to the data and objects within the warehouse\nitself. Further permissions need to be granted through T-SQL or Fabric's workspace roles and item\npermissions system.\nNote:\nDefault Read Permission:When sharing a warehouse, the default permission assigned to a user is \"Read\".\nConnectivity:\nThis \"Read\" permission allows the user to connect to the SQL an alytics endpoint of the warehouse.\nLimited Object Access:\nThe \"Read\" permission itself does not grant access to specific objects within the warehouse (tables, views,\nfunctions, etc.). To query these objects, the user needs to be granted the appropriate permissions using T-\nSQL GRANT statements.", "reference": "Explanation:\nAssigning a user default permission (\"Read\") to a warehouse in a Fabric workspace enables them to\nconnect to the SQL analytics endpoint, which is the equivalent of CONNECT permissions in SQL Server.\nHowever, this permission only allows them to connect; it doesn' t grant them the ability to query tables,\nviews, functions, or stored procedures within the warehouse unl ess they're also given access to those\nspecific objects through T-SQL GRANT statements.\nIn essence, the default \"Read\" permission provides the necessar y connectivity to the warehouse's SQL\nanalytics endpoint, but it does not automatically grant access to the data and objects within the warehouse\nitself. Further permissions need to be granted through T-SQL or Fabric's workspace roles and item\npermissions system.\nNote:\nDefault Read Permission:When sharing a warehouse, the default permission assigned to a user is \"Read\".\nConnectivity:\nThis \"Read\" permission allows the user to connect to the SQL an alytics endpoint of the warehouse.\nLimited Object Access:\nThe \"Read\" permission itself does not grant access to specific objects within the warehouse (tables, views,\nfunctions, etc.). To query these objects, the user needs to be granted the appropriate permissions using T-\nSQL GRANT statements.\nReference:\nhttps://learn.microsoft.com/en-us/fabric/data-warehouse/share-w arehouse-manage-permissions\n\nPrepare data\nTestlet 1Case studyThis is a case study. Case studies are not timed separately. You can use as much exam time as you\nwould like to complete each case. However, there may be additional case studies and sections on this\nexam. You must manage your time to ensure that you are able to complete all questions included on this\nexam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in\nthe case study. Case studies might contain exhibits and other r esources that provide more information\nabout the scenario that is described in the case study. Each qu estion is independent of the other questions\nin this case study.\nAt the end of this case study, a review screen will appear. Thi s screen allows you to review your answers\nand to make changes before you move to the next section of the exam. After you begin a new section, you\ncannot return to this section.\nTo start the case study\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to\nexplore the content of the case study before you answer the que stions. Clicking these buttons displays\ninformation such as business requirements, existing environment , and problem statements. If the case\nstudy has an All Information tab, note that the information displayed is identical to the i nformation\ndisplayed on the subsequent tabs. When you are ready to answer a question, click the Question button to\nreturn to the question.\nOverview\nContoso, Ltd. is a US-based health supplements company. Contoso has two divisions named Sales and\nResearch. The Sales division contains two departments named Onl ine Sales and Retail Sales. The\nResearch division assigns internally developed product lines to individual teams of researchers and\nanalysts.\nExisting Environment\nIdentity Environment\nContoso has a Microsoft Entra tenant named contoso.com. The ten ant contains two groups named\nResearchReviewersGroup1 and ResearchReviewersGroup2.\nData Environment\nContoso has the following data environment:\nThe Sales division uses a Microsoft Power BI Premium capacity.\nThe semantic model of the Online Sales department includes a fa ct table named Orders that uses\nImport mode. In the system of origin, the OrderID value represe nts the sequence in which orders are\ncreated.\nThe Research department uses an on-premises, third-party data w arehousing product.\nFabric is enabled for contoso.com.\nAn Azure Data Lake Storage Gen2 storage account named storage1 contains Research division data\nfor a product line named Productline1. The data is in the delta format.\nA Data Lake Storage Gen2 storage account named storage2 contain s Research division data for a\nproduct line named Productline2. The data is in the CSV format.\nRequirements\nPlanned Changes\nContoso plans to make the following changes:\n\nEnable support for Fabric in the Power BI Premium capacity used by the Sales division.\nMake all the data for the Sales division and the Research divis ion available in Fabric.\nFor the Research division, create two Fabric workspaces named P roductline1ws and Productline2ws.\nIn Productline1ws, create a lakehouse named Lakehouse1.\nIn Lakehouse1, create a shortcut to storage1 named ResearchProd uct.\nData Analytics Requirements\nContoso identifies the following data analytics requirements:\nAll the workspaces for the Sales division and the Research divi sion must support all Fabric\nexperiences.\nThe Research division workspaces must use a dedicated, on-deman d capacity that has per-minute\nbilling.\nThe Research division workspaces must be grouped together logic ally to support OneLake data hub\nfiltering based on the department name.\nFor the Research division workspaces, the members of ResearchRe viewersGroup1 must be able to\nread lakehouse and warehouse data and shortcuts by using SQL en dpoints.\nFor the Research division workspaces, the members of ResearchRe viewersGroup2 must be able to\nread lakehouse data by using Lakehouse explorer.\nAll the semantic models and reports for the Research division m ust use version control that supports\nbranching.\nData Preparation Requirements\nContoso identifies the following data preparation requirements:\nThe Research division data for Productline2 must be retrieved f rom Lakehouse1 by using Fabric\nnotebooks.\nAll the Research division data in the lakehouses must be presen ted as managed tables in Lakehouse\nexplorer.\nSemantic Model Requirements\nContoso identifies the following requirements for implementing and managing semantic models:\nThe number of rows added to the Orders table during refreshes m ust be minimized.\nThe semantic models in the Research division workspaces must us e Direct Lake mode.\nGeneral Requirements\nContoso identifies the following high-level requirements that m ust be considered for all solutions:\nFollow the principle of least privilege when applicable.\nMinimize implementation and maintenance effort when possible.", "boxes": [], "image": "images/p052.jpg"}, {"id": 41, "shown": "1", "type": "mc", "stem": "Which syntax should you use in a notebook to access the Researc h division data for Productline1?", "options": [{"key": "A", "text": "spark.read.format(“delta”).load(“Tables/ResearchProduct”)"}, {"key": "B", "text": "spark.read.format(“delta”).load(“Files/ResearchProduct”)"}, {"key": "C", "text": "spark.sql(“SELECT * FROM Lakehouse1.productline1.ResearchProduct”)"}, {"key": "D", "text": "spark.read.format(“delta”).load(“Tables/productline1/ResearchProduct”)"}], "correct": "A", "multi": false, "explanation": "Correct: * spark.read.format(\"delta\").load(\"Tables/ResearchProduct\")\n* spark.sql(“SELECT * FROM Lakehouse1.ResearchProduct”)\n\nIncorrect:\n* external_table(ResearchProduct)\n* external_table(Tables/ResearchProduct)\n* spark.read.format(“delta”).load(“Files/ResearchProduct”)* spark.read.format(\"delta\").load(\"Tables/productline1/ResearchProduct\")* spark.sql(“SELECT * FROM Lakehouse1.productline1.ResearchProduct”)\n* spark.sql(\"SELECT * FROM Lakehouse1.Tables.ResearchProduct\")\nNote: Apache Spark\nApache Spark notebooks and Apache Spark jobs can use shortcuts that you create in OneLake. Relative\nfile paths can be used to directly read data from shortcuts. Ad ditionally, if you create a shortcut in the\nTables section of the lakehouse and it is in the Delta format, you can read it as a managed table using\nApache Spark SQL syntax.\nCan use either:\ndf = spark.read.format(\"delta\").load(\"Tables/MyShortcut\")display(df)\nORdf = spark.sql(\"SELECT * FROM MyLakehouse.MyShortcut LIMIT 1000 \")\ndisplay(df)\n--\nThe spark.read.format(\"delta\").load(...) method is specifically designed for reading data\nstored in Delta format, which is what the Research division dat a for Productline1 is based on.\nThe path \"Tables/ResearchProduct\" correctly refers to the shortcut created in Lakehouse1, allowi ng\nyou to access the data efficiently.\nScenario:\nAn Azure Data Lake Storage Gen2 storage account named storage1 contains Research division data for a\nproduct line named Productline1. The data is in the delta forma t.\nThe Research division data for Productline1 must be retrieved f rom Lakehouse1 by using Fabric\nnotebooks.\nPlanned changes include:\nIn Lakehouse1, create a shortcut to storage1 named ResearchProd uct.", "reference": "Explanation:\nCorrect: * spark.read.format(\"delta\").load(\"Tables/ResearchProduct\")\n* spark.sql(“SELECT * FROM Lakehouse1.ResearchProduct”)\n\nIncorrect:\n* external_table(ResearchProduct)\n* external_table(Tables/ResearchProduct)\n* spark.read.format(“delta”).load(“Files/ResearchProduct”)* spark.read.format(\"delta\").load(\"Tables/productline1/ResearchProduct\")* spark.sql(“SELECT * FROM Lakehouse1.productline1.ResearchProduct”)\n* spark.sql(\"SELECT * FROM Lakehouse1.Tables.ResearchProduct\")\nNote: Apache Spark\nApache Spark notebooks and Apache Spark jobs can use shortcuts that you create in OneLake. Relative\nfile paths can be used to directly read data from shortcuts. Ad ditionally, if you create a shortcut in the\nTables section of the lakehouse and it is in the Delta format, you can read it as a managed table using\nApache Spark SQL syntax.\nCan use either:\ndf = spark.read.format(\"delta\").load(\"Tables/MyShortcut\")display(df)\nORdf = spark.sql(\"SELECT * FROM MyLakehouse.MyShortcut LIMIT 1000 \")\ndisplay(df)\n--\nThe spark.read.format(\"delta\").load(...) method is specifically designed for reading data\nstored in Delta format, which is what the Research division dat a for Productline1 is based on.\nThe path \"Tables/ResearchProduct\" correctly refers to the shortcut created in Lakehouse1, allowi ng\nyou to access the data efficiently.\nScenario:\nAn Azure Data Lake Storage Gen2 storage account named storage1 contains Research division data for a\nproduct line named Productline1. The data is in the delta forma t.\nThe Research division data for Productline1 must be retrieved f rom Lakehouse1 by using Fabric\nnotebooks.\nPlanned changes include:\nIn Lakehouse1, create a shortcut to storage1 named ResearchProd uct.\nReference:\nhttps://learn.microsoft.com/en-us/fabric/onelake/onelake-shortc uts\nhttps://learn.microsoft.com/en-us/fabric/onelake/onelake-shortc uts", "boxes": [], "image": "images/p053.jpg"}, {"id": 42, "shown": "2", "type": "mc", "stem": "Which syntax should you use in a notebook to access the Researc h division data for Productline1?", "options": [{"key": "A", "text": "spark.sql(\"SELECT * FROM Lakehouse1.Tables.ResearchProduct\")"}, {"key": "B", "text": "spark.read.format(\"delta\").load(\"Tables/productline1/ResearchProduct\")"}, {"key": "C", "text": "external_table(ResearchProduct)"}, {"key": "D", "text": "spark.read.format(\"delta\").load(\"Tables/ResearchProduct\")"}], "correct": "D", "multi": false, "explanation": "Correct: * spark.read.format(\"delta\").load(\"Tables/ResearchProduct\")\n* spark.sql(“SELECT * FROM Lakehouse1.ResearchProduct”)\n\nIncorrect:\n* external_table(ResearchProduct)\n* external_table(Tables/ResearchProduct)\n* spark.read.format(“delta”).load(“Files/ResearchProduct”)* spark.read.format(\"delta\").load(\"Tables/productline1/ResearchProduct\")* spark.sql(“SELECT * FROM Lakehouse1.productline1.ResearchProduct”)\n* spark.sql(\"SELECT * FROM Lakehouse1.Tables.ResearchProduct\")\nNote: Apache Spark\nApache Spark notebooks and Apache Spark jobs can use shortcuts that you create in OneLake. Relative\nfile paths can be used to directly read data from shortcuts. Ad ditionally, if you create a shortcut in the\nTables section of the lakehouse and it is in the Delta format, you can read it as a managed table using\nApache Spark SQL syntax.\nCan use either:\ndf = spark.read.format(\"delta\").load(\"Tables/MyShortcut\")display(df)\nORdf = spark.sql(\"SELECT * FROM MyLakehouse.MyShortcut LIMIT 1000 \")\ndisplay(df)\n--\nThe spark.read.format(\"delta\").load(...) method is specifically designed for reading data\nstored in Delta format, which is what the Research division dat a for Productline1 is based on.\nThe path \"Tables/ResearchProduct\" correctly refers to the shortcut created in Lakehouse1, allowi ng\nyou to access the data efficiently.\nScenario:\nAn Azure Data Lake Storage Gen2 storage account named storage1 contains Research division data for a\nproduct line named Productline1. The data is in the delta forma t.\nThe Research division data for Productline2 must be retrieved f rom Lakehouse1 by using Fabric\nnotebooks.\nPlanned changes include:\nIn Lakehouse1, create a shortcut to storage1 named ResearchProd uct.", "reference": "Explanation:\nCorrect: * spark.read.format(\"delta\").load(\"Tables/ResearchProduct\")\n* spark.sql(“SELECT * FROM Lakehouse1.ResearchProduct”)\n\nIncorrect:\n* external_table(ResearchProduct)\n* external_table(Tables/ResearchProduct)\n* spark.read.format(“delta”).load(“Files/ResearchProduct”)* spark.read.format(\"delta\").load(\"Tables/productline1/ResearchProduct\")* spark.sql(“SELECT * FROM Lakehouse1.productline1.ResearchProduct”)\n* spark.sql(\"SELECT * FROM Lakehouse1.Tables.ResearchProduct\")\nNote: Apache Spark\nApache Spark notebooks and Apache Spark jobs can use shortcuts that you create in OneLake. Relative\nfile paths can be used to directly read data from shortcuts. Ad ditionally, if you create a shortcut in the\nTables section of the lakehouse and it is in the Delta format, you can read it as a managed table using\nApache Spark SQL syntax.\nCan use either:\ndf = spark.read.format(\"delta\").load(\"Tables/MyShortcut\")display(df)\nORdf = spark.sql(\"SELECT * FROM MyLakehouse.MyShortcut LIMIT 1000 \")\ndisplay(df)\n--\nThe spark.read.format(\"delta\").load(...) method is specifically designed for reading data\nstored in Delta format, which is what the Research division dat a for Productline1 is based on.\nThe path \"Tables/ResearchProduct\" correctly refers to the shortcut created in Lakehouse1, allowi ng\nyou to access the data efficiently.\nScenario:\nAn Azure Data Lake Storage Gen2 storage account named storage1 contains Research division data for a\nproduct line named Productline1. The data is in the delta forma t.\nThe Research division data for Productline2 must be retrieved f rom Lakehouse1 by using Fabric\nnotebooks.\nPlanned changes include:\nIn Lakehouse1, create a shortcut to storage1 named ResearchProd uct.\nReference:\nhttps://learn.microsoft.com/en-us/fabric/onelake/onelake-shortc uts\nhttps://learn.microsoft.com/en-us/fabric/onelake/onelake-shortc uts", "boxes": []}, {"id": 43, "shown": "3", "type": "mc", "stem": "Which syntax should you use in a notebook to access the Researc h division data for Productline1?", "options": [{"key": "A", "text": "spark.read.format(“delta”).load(“Tables/ResearchProduct”)"}, {"key": "B", "text": "spark.read.format(“delta”).load(“Files/ResearchProduct”)"}, {"key": "C", "text": "external_table(‘Tables/ResearchProduct’)"}, {"key": "D", "text": "external_table(ResearchProduct)"}], "correct": "A", "multi": false, "explanation": "Correct: * spark.read.format(\"delta\").load(\"Tables/ResearchProduct\")\n* spark.sql(“SELECT * FROM Lakehouse1.ResearchProduct”)\nIncorrect:\n\n* external_table(ResearchProduct)\n* external_table(‘Tables/ResearchProduct’)\n* spark.read.format(“delta”).load(“Files/ResearchProduct”)* spark.read.format(\"delta\").load(\"Tables/productline1/ResearchProduct\")* spark.sql(“SELECT * FROM Lakehouse1.productline1.ResearchProduct”)\n* spark.sql(\"SELECT * FROM Lakehouse1.Tables.ResearchProduct\")\nNote: Apache Spark\nApache Spark notebooks and Apache Spark jobs can use shortcuts that you create in OneLake. Relative\nfile paths can be used to directly read data from shortcuts. Ad ditionally, if you create a shortcut in the\nTables section of the lakehouse and it is in the Delta format, you can read it as a managed table using\nApache Spark SQL syntax.\nCan use either:\ndf = spark.read.format(\"delta\").load(\"Tables/MyShortcut\")display(df)\nORdf = spark.sql(\"SELECT * FROM MyLakehouse.MyShortcut LIMIT 1000 \")\ndisplay(df)\n--\nThe spark.read.format(\"delta\").load(...) method is specifically designed for reading data\nstored in Delta format, which is what the Research division dat a for Productline1 is based on.\nThe path \"Tables/ResearchProduct\" correctly refers to the shortcut created in Lakehouse1, allowi ng\nyou to access the data efficiently.\nScenario:\nAn Azure Data Lake Storage Gen2 storage account named storage1 contains Research division data for a\nproduct line named Productline1. The data is in the delta forma t.\nThe Research division data for Productline2 must be retrieved f rom Lakehouse1 by using Fabric\nnotebooks.\nPlanned changes include:\nIn Lakehouse1, create a shortcut to storage1 named ResearchProd uct.", "reference": "Explanation:\nCorrect: * spark.read.format(\"delta\").load(\"Tables/ResearchProduct\")\n* spark.sql(“SELECT * FROM Lakehouse1.ResearchProduct”)\nIncorrect:\n\n* external_table(ResearchProduct)\n* external_table(‘Tables/ResearchProduct’)\n* spark.read.format(“delta”).load(“Files/ResearchProduct”)* spark.read.format(\"delta\").load(\"Tables/productline1/ResearchProduct\")* spark.sql(“SELECT * FROM Lakehouse1.productline1.ResearchProduct”)\n* spark.sql(\"SELECT * FROM Lakehouse1.Tables.ResearchProduct\")\nNote: Apache Spark\nApache Spark notebooks and Apache Spark jobs can use shortcuts that you create in OneLake. Relative\nfile paths can be used to directly read data from shortcuts. Ad ditionally, if you create a shortcut in the\nTables section of the lakehouse and it is in the Delta format, you can read it as a managed table using\nApache Spark SQL syntax.\nCan use either:\ndf = spark.read.format(\"delta\").load(\"Tables/MyShortcut\")display(df)\nORdf = spark.sql(\"SELECT * FROM MyLakehouse.MyShortcut LIMIT 1000 \")\ndisplay(df)\n--\nThe spark.read.format(\"delta\").load(...) method is specifically designed for reading data\nstored in Delta format, which is what the Research division dat a for Productline1 is based on.\nThe path \"Tables/ResearchProduct\" correctly refers to the shortcut created in Lakehouse1, allowi ng\nyou to access the data efficiently.\nScenario:\nAn Azure Data Lake Storage Gen2 storage account named storage1 contains Research division data for a\nproduct line named Productline1. The data is in the delta forma t.\nThe Research division data for Productline2 must be retrieved f rom Lakehouse1 by using Fabric\nnotebooks.\nPlanned changes include:\nIn Lakehouse1, create a shortcut to storage1 named ResearchProd uct.\nReference:\nhttps://learn.microsoft.com/en-us/fabric/onelake/onelake-shortc uts\nhttps://learn.microsoft.com/en-us/fabric/onelake/onelake-shortc uts", "boxes": []}, {"id": 44, "shown": "4", "type": "hotspot", "stem": "You need to migrate the Research division data for Productline2 . The solution must meet the data\npreparation requirements.\nHow should you complete the code? To answer, select the appropr iate options in the answer area.\nNOTE : Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: delta\nDelta Lake uses versioned Parquet files to store your data in y our cloud storage.\nBox 2: Tables/productline2Note: Apache Spark\nApache Spark notebooks and Apache Spark jobs can use shortcuts that you create in OneLake. Relative\nfile paths can be used to directly read data from shortcuts. Ad ditionally, if you create a shortcut in the\nTables section of the lakehouse and it is in the Delta format, you can read it as a managed table using\nApache Spark SQL syntax.\ndf = spark.read.format(\"delta\").load(\"Tables/MyShortcut\")\ndisplay(df)\ndf = spark.sql(\"SELECT * FROM MyLakehouse.MyShortcut LIMIT 1000 \")\ndisplay(df)\nScenario:\nA Data Lake Storage Gen2 storage account named storage2 contain s Research division data for a product\nline named Productline2. The data is in the CSV format.\nRequirements, Planned Changes\nFor the Research division, create two Fabric workspaces named P roductline1ws and Productline2ws.\nData Preparation Requirements\nContoso identifies the following data preparation requirements:\n* The Research division data for Productline2 must be retrieved from Lakehouse1 by using Fabric\nnotebooks.\n*-> All the Research division data in the lakehouses must be pr esented as managed tables in Lakehouse\nexplorer.\nNote:\nSpark provides two types of tables that Azure Synapse exposes i n SQL automatically:\n* Managed tables\nSpark provides many options for how to store data in managed ta bles, such as TEXT, CSV, JSON, JDBC,\nPARQUET, ORC, HIVE, DELTA, and LIBSVM. These files are normally stored in the warehouse directory\nwhere managed table data is stored.\n* External tables", "reference": "Explanation:\nBox 1: delta\nDelta Lake uses versioned Parquet files to store your data in y our cloud storage.\nBox 2: Tables/productline2Note: Apache Spark\nApache Spark notebooks and Apache Spark jobs can use shortcuts that you create in OneLake. Relative\nfile paths can be used to directly read data from shortcuts. Ad ditionally, if you create a shortcut in the\nTables section of the lakehouse and it is in the Delta format, you can read it as a managed table using\nApache Spark SQL syntax.\ndf = spark.read.format(\"delta\").load(\"Tables/MyShortcut\")\ndisplay(df)\ndf = spark.sql(\"SELECT * FROM MyLakehouse.MyShortcut LIMIT 1000 \")\ndisplay(df)\nScenario:\nA Data Lake Storage Gen2 storage account named storage2 contain s Research division data for a product\nline named Productline2. The data is in the CSV format.\nRequirements, Planned Changes\nFor the Research division, create two Fabric workspaces named P roductline1ws and Productline2ws.\nData Preparation Requirements\nContoso identifies the following data preparation requirements:\n* The Research division data for Productline2 must be retrieved from Lakehouse1 by using Fabric\nnotebooks.\n*-> All the Research division data in the lakehouses must be pr esented as managed tables in Lakehouse\nexplorer.\nNote:\nSpark provides two types of tables that Azure Synapse exposes i n SQL automatically:\n* Managed tables\nSpark provides many options for how to store data in managed ta bles, such as TEXT, CSV, JSON, JDBC,\nPARQUET, ORC, HIVE, DELTA, and LIBSVM. These files are normally stored in the warehouse directory\nwhere managed table data is stored.\n* External tables\nReference:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/metad ata/table\nhttps://learn.microsoft.com/en-us/fabric/onelake/onelake-shortc uts", "boxes": [{"box": 1, "value": "delta"}, {"box": 2, "value": "Tables/productline2Note: Apache Spark"}], "images": ["images/p056.jpg", "images/p057.jpg"]}, {"id": 45, "shown": "5", "type": "mc", "stem": "Which syntax should you use in a notebook to access the Researc h division data for Productline1?", "options": [{"key": "A", "text": "spark.read.format(“delta”).load(“Files/ResearchProduct”)"}, {"key": "B", "text": "spark.sql(“SELECT * FROM Lakehouse1.ResearchProduct ”)"}, {"key": "C", "text": "spark.sql(“SELECT * FROM Lakehouse1.Tables.ResearchProduct ”)"}, {"key": "D", "text": "external_table(ResearchProduct)"}], "correct": "B", "multi": false, "explanation": "Correct: * spark.read.format(\"delta\").load(\"Tables/ResearchProduct\")\n* spark.sql(“SELECT * FROM Lakehouse1.ResearchProduct”)\nIncorrect:\n* external_table(ResearchProduct)\n* external_table(Tables/ResearchProduct)\n* spark.read.format(“delta”).load(“Files/ResearchProduct”)* spark.read.format(\"delta\").load(\"Tables/productline1/ResearchProduct\")* spark.sql(“SELECT * FROM Lakehouse1.productline1.ResearchProduct”)\n* spark.sql(\"SELECT * FROM Lakehouse1.Tables.ResearchProduct\")\nNote: Apache Spark\nApache Spark notebooks and Apache Spark jobs can use shortcuts that you create in OneLake. Relative\nfile paths can be used to directly read data from shortcuts. Ad ditionally, if you create a shortcut in the\nTables section of the lakehouse and it is in the Delta format, you can read it as a managed table using\nApache Spark SQL syntax.\nCan use either:\ndf = spark.read.format(\"delta\").load(\"Tables/MyShortcut\")display(df)\nORdf = spark.sql(\"SELECT * FROM MyLakehouse.MyShortcut LIMIT 1000 \")\ndisplay(df)\n--\nThe spark.read.format(\"delta\").load(...) method is specifically designed for reading data\nstored in Delta format, which is what the Research division dat a for Productline1 is based on.\nThe path \"Tables/ResearchProduct\" correctly refers to the shortcut created in Lakehouse1, allowi ng\nyou to access the data efficiently.\nScenario:\nAn Azure Data Lake Storage Gen2 storage account named storage1 contains Research division data for a\nproduct line named Productline1. The data is in the delta forma t.\nThe Research division data for Productline2 must be retrieved f rom Lakehouse1 by using Fabric\nnotebooks.\nPlanned changes include:\nIn Lakehouse1, create a shortcut to storage1 named ResearchProd uct.", "reference": "Explanation:\nCorrect: * spark.read.format(\"delta\").load(\"Tables/ResearchProduct\")\n* spark.sql(“SELECT * FROM Lakehouse1.ResearchProduct”)\nIncorrect:\n* external_table(ResearchProduct)\n* external_table(Tables/ResearchProduct)\n* spark.read.format(“delta”).load(“Files/ResearchProduct”)* spark.read.format(\"delta\").load(\"Tables/productline1/ResearchProduct\")* spark.sql(“SELECT * FROM Lakehouse1.productline1.ResearchProduct”)\n* spark.sql(\"SELECT * FROM Lakehouse1.Tables.ResearchProduct\")\nNote: Apache Spark\nApache Spark notebooks and Apache Spark jobs can use shortcuts that you create in OneLake. Relative\nfile paths can be used to directly read data from shortcuts. Ad ditionally, if you create a shortcut in the\nTables section of the lakehouse and it is in the Delta format, you can read it as a managed table using\nApache Spark SQL syntax.\nCan use either:\ndf = spark.read.format(\"delta\").load(\"Tables/MyShortcut\")display(df)\nORdf = spark.sql(\"SELECT * FROM MyLakehouse.MyShortcut LIMIT 1000 \")\ndisplay(df)\n--\nThe spark.read.format(\"delta\").load(...) method is specifically designed for reading data\nstored in Delta format, which is what the Research division dat a for Productline1 is based on.\nThe path \"Tables/ResearchProduct\" correctly refers to the shortcut created in Lakehouse1, allowi ng\nyou to access the data efficiently.\nScenario:\nAn Azure Data Lake Storage Gen2 storage account named storage1 contains Research division data for a\nproduct line named Productline1. The data is in the delta forma t.\nThe Research division data for Productline2 must be retrieved f rom Lakehouse1 by using Fabric\nnotebooks.\nPlanned changes include:\nIn Lakehouse1, create a shortcut to storage1 named ResearchProd uct.\nReference:\nhttps://learn.microsoft.com/en-us/fabric/onelake/onelake-shortc uts\nhttps://learn.microsoft.com/en-us/fabric/onelake/onelake-shortc uts", "boxes": []}, {"id": 46, "shown": "6", "type": "hotspot", "stem": "You need to migrate the Research division data for Productline1 . The solution must meet the data\npreparation requirements.\nHow should you complete the code? To answer, select the appropr iate options in the answer area.\nNOTE : Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: Delta\nWith Microsoft OneLake integration for semantic models, data im ported into model tables can also be\nautomatically written to Delta tables in OneLake. The Delta for mat is the unified table format across all\ncompute engines in Microsoft Fabric. OneLake integration export s the data with all key performance\nfeatures enabled to provide more seamless data access with high er performance.\nData scientists, database analysts, app developers, data engine ers, and other data consumers can then\naccess the same data that drives your business intelligence and financial reports in Power BI. T-SQL,\nPython, Scala, PySpark, Spark SQL, R, and no-code/low-code solu tions can all be used to query data from\nDelta tables.\nBox 2: Tables/productline1\nHow to Save a Pyspark Dataframe as a Table in a Fabric Warehous e\nExample, save the dataframe result as a table in my Lakehouse.:\ntf_df.write.format(\"delta\").mode(\"append\").save(\"Tables/actual_ weather\")\nScenario: Data Preparation Requirements\nContoso identifies the following data preparation requirements:\n* The Research division data for Productline2 must be retrieved from Lakehouse1 by using Fabric\nnotebooks.* All the Research division data in the lakehouses must be pres ented as managed tables in Lakehouse\nexplorer.", "reference": "Explanation:\nBox 1: Delta\nWith Microsoft OneLake integration for semantic models, data im ported into model tables can also be\nautomatically written to Delta tables in OneLake. The Delta for mat is the unified table format across all\ncompute engines in Microsoft Fabric. OneLake integration export s the data with all key performance\nfeatures enabled to provide more seamless data access with high er performance.\nData scientists, database analysts, app developers, data engine ers, and other data consumers can then\naccess the same data that drives your business intelligence and financial reports in Power BI. T-SQL,\nPython, Scala, PySpark, Spark SQL, R, and no-code/low-code solu tions can all be used to query data from\nDelta tables.\nBox 2: Tables/productline1\nHow to Save a Pyspark Dataframe as a Table in a Fabric Warehous e\nExample, save the dataframe result as a table in my Lakehouse.:\ntf_df.write.format(\"delta\").mode(\"append\").save(\"Tables/actual_ weather\")\nScenario: Data Preparation Requirements\nContoso identifies the following data preparation requirements:\n* The Research division data for Productline2 must be retrieved from Lakehouse1 by using Fabric\nnotebooks.* All the Research division data in the lakehouses must be pres ented as managed tables in Lakehouse\nexplorer.\nReference:\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/onelake-i ntegration-overview\nhttps://medium.com/the-data-therapy/how-to-save-a-pyspark-dataf rame-as-a-table-in-a-fabric-warehouse-\ne3b04915f066", "boxes": [{"box": 1, "value": "Delta"}, {"box": 2, "value": "Tables/productline1"}], "image": "images/p059.jpg"}, {"id": 47, "shown": "7", "type": "mc", "stem": "You need to refresh the Orders table of the Online Sales depart ment. The solution must meet the semantic\nmodel requirements.\nWhat should you include in the solution?\nvalue of the OrderID column in the destination lakehouse\nof the OrderID column in the destination lakehouse\ncolumn in the destination lakehouse\ncolumn in the destination lakehouse", "options": [{"key": "A", "text": "an Azure Data Factory pipeline that executes a Stored procedu re activity to retrieve the maximum"}, {"key": "B", "text": "an Azure Data Factory pipeline that executes a Stored procedu re activity to retrieve the minimum value"}, {"key": "C", "text": "an Azure Data Factory pipeline that executes a dataflow to re trieve the minimum value of the OrderID"}, {"key": "D", "text": "an Azure Data Factory pipeline that executes a dataflow to re trieve the maximum value of the OrderID"}], "correct": "D", "multi": false, "explanation": "Dataflow instead of Store procedure to minimize implementation and maintenance effort.\nMaximum OrderID top retrieve the Order that was created most re cently.\nScenario:\nThe semantic model of the Online Sales department includes a fa ct table named Orders that uses Import\nmode. In the system of origin, the OrderID value represents the sequence in which orders are created.\nSemantic Model Requirements\nContoso identifies the following requirements for implementing and managing semantic models:\n*-> The number of rows added to the Orders table during refresh es must be minimized.\nThe semantic models in the Research division workspaces must us e Direct Lake mode.\nGeneral Requirements\nContoso identifies the following high-level requirements that m ust be considered for all solutions:\nFollow the principle of least privilege when applicable.\n*-> Minimize implementation and maintenance effort when possibl e.", "reference": "Explanation:\nDataflow instead of Store procedure to minimize implementation and maintenance effort.\nMaximum OrderID top retrieve the Order that was created most re cently.\nScenario:\nThe semantic model of the Online Sales department includes a fa ct table named Orders that uses Import\nmode. In the system of origin, the OrderID value represents the sequence in which orders are created.\nSemantic Model Requirements\nContoso identifies the following requirements for implementing and managing semantic models:\n*-> The number of rows added to the Orders table during refresh es must be minimized.\nThe semantic models in the Research division workspaces must us e Direct Lake mode.\nGeneral Requirements\nContoso identifies the following high-level requirements that m ust be considered for all solutions:\nFollow the principle of least privilege when applicable.\n*-> Minimize implementation and maintenance effort when possibl e.", "boxes": []}, {"id": 48, "shown": "8", "type": "mc", "stem": "Which syntax should you use in a notebook to access the Researc h division data for Productline1?", "options": [{"key": "A", "text": "spark.read.format(“delta”).load(“Tables/productline1/ResearchProduct”)"}, {"key": "B", "text": "spark.sql(“SELECT * FROM Lakehouse1.ResearchProduct ”)"}, {"key": "C", "text": "external_table(‘Tables/ResearchProduct)"}, {"key": "D", "text": "external_table(ResearchProduct)"}], "correct": "B", "multi": false, "explanation": "Correct: * spark.read.format(\"delta\").load(\"Tables/ResearchProduct\")\n* spark.sql(“SELECT * FROM Lakehouse1.ResearchProduct”)\nIncorrect:\n* external_table(ResearchProduct)\n\n* external_table(Tables/ResearchProduct)\n* spark.read.format(“delta”).load(“Files/ResearchProduct”)* spark.read.format(\"delta\").load(\"Tables/productline1/ResearchProduct\")* spark.sql(“SELECT * FROM Lakehouse1.productline1.ResearchProduct”)\n* spark.sql(\"SELECT * FROM Lakehouse1.Tables.ResearchProduct\")\nNote: Apache Spark\nApache Spark notebooks and Apache Spark jobs can use shortcuts that you create in OneLake. Relative\nfile paths can be used to directly read data from shortcuts. Ad ditionally, if you create a shortcut in the\nTables section of the lakehouse and it is in the Delta format, you can read it as a managed table using\nApache Spark SQL syntax.\nCan use either:\ndf = spark.read.format(\"delta\").load(\"Tables/MyShortcut\")display(df)\nORdf = spark.sql(\"SELECT * FROM MyLakehouse.MyShortcut LIMIT 1000 \")\ndisplay(df)\n--\nThe spark.read.format(\"delta\").load(...) method is specifically designed for reading data\nstored in Delta format, which is what the Research division dat a for Productline1 is based on.\nThe path \"Tables/ResearchProduct\" correctly refers to the shortcut created in Lakehouse1, allowi ng\nyou to access the data efficiently.\nScenario:\nAn Azure Data Lake Storage Gen2 storage account named storage1 contains Research division data for a\nproduct line named Productline1. The data is in the delta forma t.\nThe Research division data for Productline2 must be retrieved f rom Lakehouse1 by using Fabric\nnotebooks.\nPlanned changes include:\nIn Lakehouse1, create a shortcut to storage1 named ResearchProd uct.", "reference": "Explanation:\nCorrect: * spark.read.format(\"delta\").load(\"Tables/ResearchProduct\")\n* spark.sql(“SELECT * FROM Lakehouse1.ResearchProduct”)\nIncorrect:\n* external_table(ResearchProduct)\n\n* external_table(Tables/ResearchProduct)\n* spark.read.format(“delta”).load(“Files/ResearchProduct”)* spark.read.format(\"delta\").load(\"Tables/productline1/ResearchProduct\")* spark.sql(“SELECT * FROM Lakehouse1.productline1.ResearchProduct”)\n* spark.sql(\"SELECT * FROM Lakehouse1.Tables.ResearchProduct\")\nNote: Apache Spark\nApache Spark notebooks and Apache Spark jobs can use shortcuts that you create in OneLake. Relative\nfile paths can be used to directly read data from shortcuts. Ad ditionally, if you create a shortcut in the\nTables section of the lakehouse and it is in the Delta format, you can read it as a managed table using\nApache Spark SQL syntax.\nCan use either:\ndf = spark.read.format(\"delta\").load(\"Tables/MyShortcut\")display(df)\nORdf = spark.sql(\"SELECT * FROM MyLakehouse.MyShortcut LIMIT 1000 \")\ndisplay(df)\n--\nThe spark.read.format(\"delta\").load(...) method is specifically designed for reading data\nstored in Delta format, which is what the Research division dat a for Productline1 is based on.\nThe path \"Tables/ResearchProduct\" correctly refers to the shortcut created in Lakehouse1, allowi ng\nyou to access the data efficiently.\nScenario:\nAn Azure Data Lake Storage Gen2 storage account named storage1 contains Research division data for a\nproduct line named Productline1. The data is in the delta forma t.\nThe Research division data for Productline2 must be retrieved f rom Lakehouse1 by using Fabric\nnotebooks.\nPlanned changes include:\nIn Lakehouse1, create a shortcut to storage1 named ResearchProd uct.\nReference:\nhttps://learn.microsoft.com/en-us/fabric/onelake/onelake-shortc uts\nhttps://learn.microsoft.com/en-us/fabric/onelake/onelake-shortc uts\n\nPrepare data\nTestlet 2Case study\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you\nwould like to complete each case. However, there may be additional case studies and sections on this\nexam. You must manage your time to ensure that you are able to complete all questions included on this\nexam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in\nthe case study. Case studies might contain exhibits and other r esources that provide more information\nabout the scenario that is described in the case study. Each qu estion is independent of the other questions\nin this case study.\nAt the end of this case study, a review screen will appear. Thi s screen allows you to review your answers\nand to make changes before you move to the next section of the exam. After you begin a new section, you\ncannot return to this section.\nTo start the case study\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to\nexplore the content of the case study before you answer the que stions. Clicking these buttons displays\ninformation such as business requirements, existing environment , and problem statements. If the case\nstudy has an All Information tab, note that the information displayed is identical to the i nformation\ndisplayed on the subsequent tabs. When you are ready to answer a question, click the Question button to\nreturn to the question.\nOverview\nLitware, Inc. is a manufacturing company that has offices throu ghout North America. The analytics team at\nLitware contains data engineers, analytics engineers, data anal ysts, and data scientists.\nExisting Environment\nFabric Environment\nLitware has been using a Microsoft Power BI tenant for three ye ars. Litware has NOT enabled any Fabric\ncapacities and features.\nAvailable Data\nLitware has data that must be analyzed as shown in the followin g table.\nThe Product data contains a single table and the following colu mns.\n\nThe customer satisfaction data contains the following tables:\nSurvey\nQuestion\nResponse\nFor each survey submitted, the following occurs:\nOne row is added to the Survey table.\nOne row is added to the Response table for each question in the survey.\nThe Question table contains the text of each survey question. T he third question in each survey response\nis an overall satisfaction score. Customers can submit a survey after each purchase.\nUser Problems\nThe analytics team has large volumes of data, some of which is semi-structured. The team wants to use\nFabric to create a new data store.\nProduct data is often classified into three pricing groups: hig h, medium, and low. This logic is implemented\nin several databases and semantic models, but the logic does NOT always match across implementations.\nRequirements\nPlanned Changes\nLitware plans to enable Fabric features in the existing tenant. The analytics team will create a new data\nstore as a proof of concept (PoC). The remaining Litware users will only get access to the Fabric features\nonce the PoC is complete. The PoC will be completed by using a Fabric trial capacity.\nThe following three workspaces will be created:\nAnalyticsPOC: Will contain the data store, semantic models, rep orts pipelines, dataflow, and notebooks\nused to populate the data store\nDataEngPOC: Will contain all the pipelines, dataflows, and note books used to populate OneLake\nDataSciPOC: Will contain all the notebooks and reports created by the data scientists\nThe following will be created in the AnalyticsPOC workspace:\nA data store (type to be decided)\nA custom semantic model\nA default semantic model\nInteractive reports\nThe data engineers will create data pipelines to load data to O neLake either hourly or daily depending on\nthe data source. The analytics engineers will create processes to ingest, transform, and load the data to\nthe data store in the AnalyticsPOC workspace daily. Whenever po ssible, the data engineers will use low-\ncode tools for data ingestion. The choice of which data cleansi ng and transformation tools to use will be at\nthe data engineers’ discretion.\nAll the semantic models and reports in the Analytics POC worksp ace will use the data store as the sole\ndata source.\nTechnical Requirements\nThe data store must support the following:\nRead access by using T-SQL or Python\nSemi-structured and unstructured data\nRow-level security (RLS) for users executing T-SQL queries\nFiles loaded by the data engineers to OneLake will be stored in the Parquet format and will meet Delta\nLake specifications.\n\nData will be loaded without transformation in one area of the A nalyticsPOC data store. The data will then\nbe cleansed, merged, and transformed into a dimensional model.\nThe data load process must ensure that the raw and cleansed dat a is updated completely before\npopulating the dimensional model.\nThe dimensional model must contain a date dimension. There is n o existing data source for the date\ndimension. The Litware fiscal year matches the calendar year. T he date dimension must always contain\ndates from 2010 through the end of the current year.\nThe product pricing group logic must be maintained by the analy tics engineers in a single location. The\npricing group data must be made available in the data store for T-SQL queries and in the default semantic\nmodel. The following logic must be used:\nList prices that are less than or equal to 50 are in the low pr icing group.\nList prices that are greater than 50 and less than or equal to 1,000 are in the medium pricing group.\nList prices that are greater than 1,000 are in the high pricing group.\nSecurity Requirements\nOnly Fabric administrators and the analytics team must be able to see the Fabric items created as part of\nthe PoC.\nLitware identifies the following security requirements for the Fabric items in the AnalyticsPOC workspace:\nFabric administrators will be the workspace administrators.\nThe data engineers must be able to read from and write to the d ata store. No access must be granted\nto datasets or reports.\nThe analytics engineers must be able to read from, write to, an d create schemas in the data store. They\nalso must be able to create and share semantic models with the data analysts and view and modify all\nreports in the workspace.\nThe data scientists must be able to read from the data store, b ut not write to it. They will access the\ndata by using a Spark notebook\nThe data analysts must have read access to only the dimensional model objects in the data store. They\nalso must have access to create Power BI reports by using the s emantic models created by the\nanalytics engineers.\nThe date dimension must be available to all users of the data s tore.\nThe principle of least privilege must be followed.\nBoth the default and custom semantic models must include only t ables or views from the dimensional\nmodel in the data store. Litware already has the following Micr osoft Entra security groups:\nFabricAdmins: Fabric administrators\nAnalyticsTeam: All the members of the analytics team\nDataAnalysts: The data analysts on the analytics team\nDataScientists: The data scientists on the analytics team\nDataEngineers: The data engineers on the analytics team\nAnalyticsEngineers: The analytics engineers on the analytics te am\nReport Requirements\nThe data analysts must create a customer satisfaction report th at meets the following requirements:\nEnables a user to select a product to filter customer survey re sponses to only those who have\npurchased that product.\nDisplays the average overall satisfaction score of all the surv eys submitted during the last 12 months\nup to a selected date.\nShows data as soon as the data is updated in the data store.\nEnsures that the report and the semantic model only contain dat a from the current and previous year.\nEnsures that the report respects any table-level security speci fied in the source data store.\nMinimizes the execution time of report queries.", "boxes": [], "images": ["images/p062.jpg", "images/p063.jpg", "images/p064.jpg"]}, {"id": 49, "shown": "1", "type": "mc", "stem": "What should you recommend using to ingest the customer data int o the data store in the AnalyticsPOC\nworkspace?", "options": [{"key": "A", "text": "a stored procedure"}, {"key": "B", "text": "a pipeline that contains a KQL activity"}, {"key": "C", "text": "a Spark notebook"}, {"key": "D", "text": "a dataflow"}], "correct": "D", "multi": false, "explanation": "Lakehouse end-to-end scenario: overview and architecture\nArchitecture\nThe following image shows the lakehouse end-to-end architecture . The components involved are\ndescribed in the following list.\nScenario: The data engineers will create data pipelines to load data to O neLake either hourly or daily depending on\nthe data source. The analytics engineers will create processes to ingest, transform, and load the data to\nthe data store in the AnalyticsPOC workspace daily. Whenever po ssible, the data engineers will use low-\ncode tools for data ingestion. The choice of which data cleansi ng and transformation tools to use will be at\nthe data engineers’ discretion.\n* Ingestion: You can quickly build insights for your organizati on using more than 200 native connectors.\nThese connectors are integrated into the Fabric pipeline and ut ilize the user-friendly drag-and-drop data\ntransformation with *dataflow*.\n* Transform and store: Fabric standardizes on Delta Lake format . Which means all the Fabric engines can\naccess and manipulate the same dataset stored in OneLake withou t duplicating data. This storage system\nprovides the flexibility to build lakehouses using a medallion architecture or a data mesh, depending on\nyour organizational requirement. You can choose between a low-c ode or no-code experience for data\ntransformation, utilizing either *pipelines/dataflows* or noteb ook/Spark for a code-first experience.\n\nIncorrect:\n* stored procedure, a Spark notebookWould require coding.", "reference": "Explanation:\nLakehouse end-to-end scenario: overview and architecture\nArchitecture\nThe following image shows the lakehouse end-to-end architecture . The components involved are\ndescribed in the following list.\nScenario: The data engineers will create data pipelines to load data to O neLake either hourly or daily depending on\nthe data source. The analytics engineers will create processes to ingest, transform, and load the data to\nthe data store in the AnalyticsPOC workspace daily. Whenever po ssible, the data engineers will use low-\ncode tools for data ingestion. The choice of which data cleansi ng and transformation tools to use will be at\nthe data engineers’ discretion.\n* Ingestion: You can quickly build insights for your organizati on using more than 200 native connectors.\nThese connectors are integrated into the Fabric pipeline and ut ilize the user-friendly drag-and-drop data\ntransformation with *dataflow*.\n* Transform and store: Fabric standardizes on Delta Lake format . Which means all the Fabric engines can\naccess and manipulate the same dataset stored in OneLake withou t duplicating data. This storage system\nprovides the flexibility to build lakehouses using a medallion architecture or a data mesh, depending on\nyour organizational requirement. You can choose between a low-c ode or no-code experience for data\ntransformation, utilizing either *pipelines/dataflows* or noteb ook/Spark for a code-first experience.\n\nIncorrect:\n* stored procedure, a Spark notebookWould require coding.\nReference:\nhttps://learn.microsoft.com/en-us/fabric/data-engineering/tutor ial-lakehouse-introduction", "boxes": [], "image": "images/p065.jpg"}, {"id": 50, "shown": "2", "type": "mc", "stem": "You need to recommend a solution to prepare the tenant for the PoC.\nWhich two actions should you recommend performing from the Fabr ic Admin portal? Each correct answer\npresents part of the solution.\nNOTE : Each correct answer is worth one point.\ngroups.", "options": [{"key": "A", "text": "Enable the Users can try Microsoft Fabric paid features optio n for the entire organization."}, {"key": "B", "text": "Enable the Users can try Microsoft Fabric paid features optio n for specific security groups."}, {"key": "C", "text": "Enable the Allow Microsoft Entra ID guest users to access Mic rosoft Fabric option for specific security"}, {"key": "D", "text": "Enable the Users can create Fabric items option and exclude s pecific security groups."}, {"key": "E", "text": "Enable the Users can create Fabric items option for specific security groups."}], "correct": "BE", "multi": true, "explanation": "B: Fabric trial capacity for the analytics team.\nScenario: Planned Changes\nLitware plans to enable Fabric features in the existing tenant. The analytics team will create a new data\nstore as a proof of concept (PoC). The remaining Litware users will only get access to the Fabric features\nonce the PoC is complete. The PoC will be completed by using a Fabric trial capacity.\nE: Enable the Users can create Fabric items option for the dat a engineers.\nScenario: The data engineers will create data pipelines to load data to OneLake either hourly or daily\ndepending on the data source. The analytics engineers will crea te processes to ingest, transform, and load\nthe data to the data store in the AnalyticsPOC workspace daily. Whenever possible, the data engineers will\nuse low-code tools for data ingestion. The choice of which data cleansing and transformation tools to use\nwill be at the data engineers’ discretion.", "reference": "Explanation:\nB: Fabric trial capacity for the analytics team.\nScenario: Planned Changes\nLitware plans to enable Fabric features in the existing tenant. The analytics team will create a new data\nstore as a proof of concept (PoC). The remaining Litware users will only get access to the Fabric features\nonce the PoC is complete. The PoC will be completed by using a Fabric trial capacity.\nE: Enable the Users can create Fabric items option for the dat a engineers.\nScenario: The data engineers will create data pipelines to load data to OneLake either hourly or daily\ndepending on the data source. The analytics engineers will crea te processes to ingest, transform, and load\nthe data to the data store in the AnalyticsPOC workspace daily. Whenever possible, the data engineers will\nuse low-code tools for data ingestion. The choice of which data cleansing and transformation tools to use\nwill be at the data engineers’ discretion.", "boxes": []}, {"id": 51, "shown": "3", "type": "mc", "stem": "You need to implement the date dimension in the data store. The  solution must meet the technical\nrequirements.\nWhat are two ways to achieve the goal? Each correct answer pres ents a complete solution.\nNOTE : Each correct selection is worth one point.", "options": [{"key": "A", "text": "Populate the date dimension table by using a dataflow."}, {"key": "B", "text": "Populate the date dimension table by using a Copy activity in  a pipeline."}, {"key": "C", "text": "Populate the date dimension view by using T-SQL."}, {"key": "D", "text": "Populate the date dimension table by using a Stored procedure  activity in a pipeline."}], "correct": "AD", "multi": true, "explanation": "", "reference": "", "boxes": []}, {"id": 52, "shown": "4", "type": "mc", "stem": "You need to ensure the data loading activities in the Analytics POC workspace are executed in the\nappropriate sequence. The solution must meet the technical requ irements.\nWhat should you do?", "options": [{"key": "A", "text": "Create a dataflow that has multiple steps and schedule the da taflow."}, {"key": "B", "text": "Create and schedule a Spark notebook."}, {"key": "C", "text": "Create and schedule a Spark job definition."}, {"key": "D", "text": "Create a pipeline that has dependencies between activities an d schedule the pipeline."}], "correct": "D", "multi": false, "explanation": "Microsoft Fabric, Ingest data into OneLake and analyze with Azu re Databricks\nYou can do this with a pipeline in a workspace and ingest data into your OneLake in Delta format.\nRead and modify a Delta table in OneLake with Azure Databricks.\nNote: Power BI dataflows enable you to connect to, transform, c ombine, and distribute data for\ndownstream analytics. A key element in dataflows is the refresh process, which applies the transformation\nsteps you authored in the dataflows and updates the data in the items themselves.\nScenario:\nTechnical Requirements\nFiles loaded by the data engineers to OneLake will be stored in the Parquet format and will meet Delta\nLake specifications.\nData will be loaded without transformation in one area of the A nalyticsPOC data store. The data will then\nbe cleansed, merged, and transformed into a dimensional model\nThe data load process must ensure that the raw and cleansed dat a is updated completely before\npopulating the dimensional model\nPlanned change:\nThe data engineers will create data pipelines to load data to O neLake either hourly or daily depending on\nthe data source. The analytics engineers will create processes to ingest, transform, and load the data to\nthe data store in the AnalyticsPOC workspace daily. Whenever po ssible, the data engineers will use low-\ncode tools for data ingestion. The choice of which data cleansi ng and transformation tools to use will be at\nthe data engineers’ discretion.", "reference": "Explanation:\nMicrosoft Fabric, Ingest data into OneLake and analyze with Azu re Databricks\nYou can do this with a pipeline in a workspace and ingest data into your OneLake in Delta format.\nRead and modify a Delta table in OneLake with Azure Databricks.\nNote: Power BI dataflows enable you to connect to, transform, c ombine, and distribute data for\ndownstream analytics. A key element in dataflows is the refresh process, which applies the transformation\nsteps you authored in the dataflows and updates the data in the items themselves.\nScenario:\nTechnical Requirements\nFiles loaded by the data engineers to OneLake will be stored in the Parquet format and will meet Delta\nLake specifications.\nData will be loaded without transformation in one area of the A nalyticsPOC data store. The data will then\nbe cleansed, merged, and transformed into a dimensional model\nThe data load process must ensure that the raw and cleansed dat a is updated completely before\npopulating the dimensional model\nPlanned change:\nThe data engineers will create data pipelines to load data to O neLake either hourly or daily depending on\nthe data source. The analytics engineers will create processes to ingest, transform, and load the data to\nthe data store in the AnalyticsPOC workspace daily. Whenever po ssible, the data engineers will use low-\ncode tools for data ingestion. The choice of which data cleansi ng and transformation tools to use will be at\nthe data engineers’ discretion.\nReference:\nhttps://learn.microsoft.com/en-us/fabric/onelake/onelake-open-a ccess-quickstart\nhttps://learn.microsoft.com/en-us/power-bi/transform-model/data flows/dataflows-understand-optimize-\nrefresh", "boxes": []}, {"id": 53, "shown": "5", "type": "hotspot", "stem": "You need to resolve the issue with the pricing group classifica tion.\nHow should you complete the T-SQL statement? To answer, select the appropriate options in the answer\narea.\nNOTE:  Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Scenario:\nProduct data is often classified into three pricing groups: hig h, medium, and low. This logic is implemented\nin several databases and semantic models, but the logic does NO T always match across implementations.\nBox 1: TABLE\nCREATE TABLE AS SELECTThe CREATE TABLE AS SELECT (CTAS) statement is one of the most important T-SQL features\navailable. CTAS is a parallel operation that creates a new tabl e based on the output of a SELECT\nstatement. CTAS is the simplest and fastest way to create and i nsert data into a table with a single\ncommand.\nBox 2: CASE Syntax\nSyntax for SQL Server, Azure SQL Database and Azure Synapse Ana lytics.\n-- Simple CASE expression:\nCASE input_expression WHEN when_expression THEN result_expression [ ...n ]\n\n [ ELSE else_result_expression ]\nEND\nBox 3: WHEN (ListPrice BETWEEN 50 AND 1000) THEN 'medium'\nMust use an expression that includes 1000 in medium.\nNote: BETWEEN returns TRUE if the value of test_expression is g reater than or equal to the value of\nbegin_expression and less than or equal to the value of end_exp ression.", "reference": "Explanation:\nScenario:\nProduct data is often classified into three pricing groups: hig h, medium, and low. This logic is implemented\nin several databases and semantic models, but the logic does NO T always match across implementations.\nBox 1: TABLE\nCREATE TABLE AS SELECTThe CREATE TABLE AS SELECT (CTAS) statement is one of the most important T-SQL features\navailable. CTAS is a parallel operation that creates a new tabl e based on the output of a SELECT\nstatement. CTAS is the simplest and fastest way to create and i nsert data into a table with a single\ncommand.\nBox 2: CASE Syntax\nSyntax for SQL Server, Azure SQL Database and Azure Synapse Ana lytics.\n-- Simple CASE expression:\nCASE input_expression WHEN when_expression THEN result_expression [ ...n ]\n\n [ ELSE else_result_expression ]\nEND\nBox 3: WHEN (ListPrice BETWEEN 50 AND 1000) THEN 'medium'\nMust use an expression that includes 1000 in medium.\nNote: BETWEEN returns TRUE if the value of test_expression is g reater than or equal to the value of\nbegin_expression and less than or equal to the value of end_exp ression.\nReference:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-d ata-warehouse/sql-data-warehouse-\ndevelop-ctas\n\nPrepare data\nQuestion Set 3", "boxes": [{"box": 1, "value": "TABLE"}, {"box": 2, "value": "CASE Syntax"}, {"box": 3, "value": "WHEN (ListPrice BETWEEN 50 AND 1000) THEN 'medium'"}], "images": ["images/p068.jpg", "images/p069.jpg"]}, {"id": 54, "shown": "1", "type": "mc", "stem": "You have a Fabric tenant that contains a machine learning model  registered in a Fabric workspace.\nYou need to use the model to generate predictions by using the PREDICT function in a Fabric notebook.\nWhich two languages can you use to perform model scoring? Each correct answer presents a complete\nsolution.\nNOTE:  Each correct answer is worth one point.", "options": [{"key": "A", "text": "T-SQL"}, {"key": "B", "text": "DAX"}, {"key": "C", "text": "Spark SQL"}, {"key": "D", "text": "PySpark"}], "correct": "CD", "multi": true, "explanation": "Machine learning model scoring with PREDICT in Microsoft Fabric\nTo invoke the PREDICT function, you can use the Transformer API , the Spark SQL API, or a PySpark\nuser-defined function (UDF).\nNote: Microsoft Fabric allows users to operationalize machine l earning models with a scalable function\ncalled PREDICT, which supports batch scoring in any compute eng ine. Users can generate batch\npredictions directly from a Microsoft Fabric notebook or from a given ML model's item page.", "reference": "Explanation:\nMachine learning model scoring with PREDICT in Microsoft Fabric\nTo invoke the PREDICT function, you can use the Transformer API , the Spark SQL API, or a PySpark\nuser-defined function (UDF).\nNote: Microsoft Fabric allows users to operationalize machine l earning models with a scalable function\ncalled PREDICT, which supports batch scoring in any compute eng ine. Users can generate batch\npredictions directly from a Microsoft Fabric notebook or from a given ML model's item page.\nReference:\nhttps://learn.microsoft.com/en-us/fabric/data-science/model-sco ring-predict", "boxes": []}, {"id": 55, "shown": "2", "type": "mc", "stem": "You have a Fabric workspace that contains a DirectQuery semanti c model. The model queries a data\nsource that has 500 million rows.\nYou have a Microsoft Power Bi report named Report1 that uses th e model. Report1 contains visuals on\nmultiple pages.\nYou need to reduce the query execution time for the visuals on all the pages.\nWhat are two features that you can use? Each correct answer pre sents a complete solution,\nNOTE:  Each correct answer is worth one point.", "options": [{"key": "A", "text": "user-defined aggregations"}, {"key": "B", "text": "automatic aggregation"}, {"key": "C", "text": "query caching"}, {"key": "D", "text": "OneLake integration"}], "correct": "AB", "multi": true, "explanation": "", "reference": "", "boxes": []}, {"id": 56, "shown": "3", "type": "hotspot", "stem": "You have a Fabric tenant that contains two lakehouses.You are building a dataflow that will combine data from the lak ehouses. The applied steps from one of the\nqueries in the dataflow is shown in the following exhibit.\nUse the drop-down menus to select the answer choice that comple tes each statement based on the\ninformation presented in the graphic.\nNOTE:  Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: Some\n____ of the transformation steps in the query will fold.\nWe see from the exhibit that the View Native Query option isn't enabled (greyed out).\nIf the View Native Query option isn't enabled (greyed out), thi s is evidence that not all query steps can be\nfolded. However, it could mean that a subset of steps can still be folded. Working backwards from the last\nstep, you can check each step to see if the View Native Query o ption is enabled. If so, then you've learned\nwhere, in the sequence of steps, that query folding could no lo nger be achieved.\n\nBox 2: the Microsoft Power Query engine\nThe Added custom step will be performed in ______.\nDepending on how the query is structured, there could be three possible outcomes to the query folding\nmechanism:\n-> No query folding: When the query contains transformations th at can't be translated to the native query\nlanguage of your data source, either because the transformation s aren't supported or the connector\ndoesn't support query folding. For this case, Power Query gets the raw data from your data source and\nuses the Power Query engine to achieve the output you want by p rocessing the required transforms at the\nPower Query engine level.\nFull query folding: When all of your query transformations get pushed back to the data source and minimal\nprocessing occurs at the Power Query engine.\nPartial query folding: When only a few transformations in your query, and not all, can be pushed back to\nthe data source. In this case, only a subset of your transforma tions is done at your data source and the\nrest of your query transformations occur in the Power Query eng ine.\nNote: Query folding is the ability for a Power Query query to g enerate a single query statement to retrieve\nand transform source data. The Power Query mashup engine strive s to achieve query folding whenever\npossible for reasons of efficiency.\n\nThe goal of query folding is to offload or push as much of the evaluation of a query to a data source that\ncan compute the transformations of your query.\nThe query folding mechanism accomplishes this goal by translati ng your M script to a language that can be\ninterpreted and executed by your data source. It then pushes th e evaluation to your data source and sends\nthe result of that evaluation to Power Query.", "reference": "Explanation:\nBox 1: Some\n____ of the transformation steps in the query will fold.\nWe see from the exhibit that the View Native Query option isn't enabled (greyed out).\nIf the View Native Query option isn't enabled (greyed out), thi s is evidence that not all query steps can be\nfolded. However, it could mean that a subset of steps can still be folded. Working backwards from the last\nstep, you can check each step to see if the View Native Query o ption is enabled. If so, then you've learned\nwhere, in the sequence of steps, that query folding could no lo nger be achieved.\n\nBox 2: the Microsoft Power Query engine\nThe Added custom step will be performed in ______.\nDepending on how the query is structured, there could be three possible outcomes to the query folding\nmechanism:\n-> No query folding: When the query contains transformations th at can't be translated to the native query\nlanguage of your data source, either because the transformation s aren't supported or the connector\ndoesn't support query folding. For this case, Power Query gets the raw data from your data source and\nuses the Power Query engine to achieve the output you want by p rocessing the required transforms at the\nPower Query engine level.\nFull query folding: When all of your query transformations get pushed back to the data source and minimal\nprocessing occurs at the Power Query engine.\nPartial query folding: When only a few transformations in your query, and not all, can be pushed back to\nthe data source. In this case, only a subset of your transforma tions is done at your data source and the\nrest of your query transformations occur in the Power Query eng ine.\nNote: Query folding is the ability for a Power Query query to g enerate a single query statement to retrieve\nand transform source data. The Power Query mashup engine strive s to achieve query folding whenever\npossible for reasons of efficiency.\n\nThe goal of query folding is to offload or push as much of the evaluation of a query to a data source that\ncan compute the transformations of your query.\nThe query folding mechanism accomplishes this goal by translati ng your M script to a language that can be\ninterpreted and executed by your data source. It then pushes th e evaluation to your data source and sends\nthe result of that evaluation to Power Query.\nReference:\nhttps://learn.microsoft.com/en-us/power-query/power-query-foldi ng\nhttps://learn.microsoft.com/en-us/power-query/query-folding-bas ics", "boxes": [{"box": 1, "value": "Some"}, {"box": 2, "value": "the Microsoft Power Query engine"}], "images": ["images/p072.jpg", "images/p073.jpg", "images/p074.jpg"]}, {"id": 57, "shown": "4", "type": "mc", "stem": "You have a Fabric tenant that contains a lakehouse named Lakeho use1. Lakehouse1 contains a table\nnamed Table1.\nYou are creating a new data pipeline.You plan to copy external data to Table1. The schema of the ext ernal data changes regularly.\nYou need the copy operation to meet the following requirements:\nReplace Table1 with the schema of the external data.\nReplace all the data in Table1 with the rows in the external da ta.\nYou add a Copy data activity to the pipeline.What should you do for the Copy data activity?", "options": [{"key": "A", "text": "From the Source tab, add additional columns."}, {"key": "B", "text": "From the Destination tab, set Table action to Overwrite ."}, {"key": "C", "text": "From the Settings tab, select Enable staging ."}, {"key": "D", "text": "From the Source tab, select Enable partition discovery ."}, {"key": "E", "text": "From the Source tab, select Recursively ."}], "correct": "B", "multi": false, "explanation": "Ingest data into the lakehouse\nB: Destination\nThe following properties are supported for Lakehouse under the Destination tab of a copy activity.\n* Overwrite: Overwrite the existing data and schema in the tabl e using the new values.\n* Etc.\nIncorrect:\nNot D: The following tables contain more information about a co py activity in Lakehouse.\nSource information\n* Enable partition discoveryWhether to parse the partitions from the file path and add them as extra source columns.\nNot E:\n* RecursivelyProcess all files in the input folder and its subfolders recurs ively or just the ones in the selected folder. This\nsetting is disabled when a single file is selected.\n* Etc.", "reference": "Explanation:\nIngest data into the lakehouse\nB: Destination\nThe following properties are supported for Lakehouse under the Destination tab of a copy activity.\n* Overwrite: Overwrite the existing data and schema in the tabl e using the new values.\n* Etc.\nIncorrect:\nNot D: The following tables contain more information about a co py activity in Lakehouse.\nSource information\n* Enable partition discoveryWhether to parse the partitions from the file path and add them as extra source columns.\nNot E:\n* RecursivelyProcess all files in the input folder and its subfolders recurs ively or just the ones in the selected folder. This\nsetting is disabled when a single file is selected.\n* Etc. Reference:\n\nhttps://learn.microsoft.com/en-us/fabric/data-factory/connector -lakehouse-copy-activity", "boxes": [], "image": "images/p075.jpg"}, {"id": 58, "shown": "5", "type": "mc", "stem": "You have a Fabric tenant that contains a lakehouse.\nYou plan to query sales data files by using the SQL endpoint. T he files will be in an Amazon Simple\nStorage Service (Amazon S3) storage bucket.\nYou need to recommend which file format to use and where to cre ate a shortcut.\nWhich two actions should you include in the recommendation? Eac h correct answer presents part of the\nsolution.\nNOTE:  Each correct answer is worth one point.", "options": [{"key": "A", "text": "Create a shortcut in the Files section."}, {"key": "B", "text": "Use the Parquet format"}, {"key": "C", "text": "Use the CSV format."}, {"key": "D", "text": "Create a shortcut in the Tables section."}, {"key": "E", "text": "Use the delta format."}], "correct": "BD", "multi": true, "explanation": "", "reference": "", "boxes": []}, {"id": 59, "shown": "6", "type": "mc", "stem": "You have a Fabric tenant that contains a lakehouse named Lakeho use1. Lakehouse1 contains a subfolder\nnamed Subfolder1 that contains CSV files.\nYou need to convert the CSV files into the delta format that ha s V-Order optimization enabled.\nWhat should you do from Lakehouse explorer?", "options": [{"key": "A", "text": "Use the Load to Tables feature."}, {"key": "B", "text": "Create a new shortcut in the Files section."}, {"key": "C", "text": "Create a new shortcut in the Tables section."}, {"key": "D", "text": "Use the Optimize feature."}], "correct": "A", "multi": false, "explanation": "Load to Delta Lake tableThe Lakehouse in Microsoft Fabric provides a feature to efficie ntly load common file types to an optimized\nDelta table ready for analytics. The Load to Table feature allo ws users to load a single file or a folder of\nfiles to a table. This feature increases productivity for data engineers by allowing them to quickly use a\nright-click action to enable table loading on files and folders . Loading to the table is also a no-code\nexperience, which lowers the entry bar for all personas.", "reference": "Explanation:\nLoad to Delta Lake tableThe Lakehouse in Microsoft Fabric provides a feature to efficie ntly load common file types to an optimized\nDelta table ready for analytics. The Load to Table feature allo ws users to load a single file or a folder of\nfiles to a table. This feature increases productivity for data engineers by allowing them to quickly use a\nright-click action to enable table loading on files and folders . Loading to the table is also a no-code\nexperience, which lowers the entry bar for all personas.\nReference:\nhttps://learn.microsoft.com/en-us/fabric/data-engineering/load- to-tables", "boxes": []}, {"id": 60, "shown": "7", "type": "mc", "stem": "You have a Fabric tenant that contains a lakehouse named Lakeho use1. Lakehouse1 contains an\nunpartitioned table named Table1.\nYou plan to copy data to Table1 and partition the table based o n a date column in the source data.\nYou create a Copy activity to copy the data to Table1.\nYou need to specify the partition column in the Destination set tings of the Copy activity.\nWhat should you do first?", "options": [{"key": "A", "text": "From the Destination tab, set Mode to Append ."}, {"key": "B", "text": "From the Destination tab, select the partition column."}, {"key": "C", "text": "From the Source tab, select Enable partition discovery ."}, {"key": "D", "text": "From the Destination tabs, set Mode to Overwrite ."}], "correct": "D", "multi": false, "explanation": "The following properties are supported for Lakehouse under the Destination tab of a copy activity.\n* Under Advanced, you can specify the following fields:\n- Table actions: Specify the operation against the selected tab le.\n-- Overwrite: Overwrite the existing data and schema in the tab le using the new values. If this operation is\nselected, you can enable partition on your target table:--- Enable Partition: This selection allows you to create parti tions in a folder structure based on one or\nmultiple columns. Each distinct column value (pair) is a new pa rtition. For example, \"year=2000/month=01/\nfile\". This selection supports insert-only mode and requires an empty directory in the destination.\n----Partition column name: Select from the destination columns in schemas mapping. Supported data types\nare string, integer, boolean, and datetime. Format respects typ e conversion settings under the Mapping\ntab.\nIncorrect: Not A:\n* Append: Append new values to existing table.\n* Etc. Not C: The following tables contain more information about a co py activity in Lakehouse.\nSource information\n* Enable partition discoveryWhether to parse the partitions from the file path and add them as extra source columns.\n* Etc.", "reference": "Explanation:\nThe following properties are supported for Lakehouse under the Destination tab of a copy activity.\n* Under Advanced, you can specify the following fields:\n- Table actions: Specify the operation against the selected tab le.\n-- Overwrite: Overwrite the existing data and schema in the tab le using the new values. If this operation is\nselected, you can enable partition on your target table:--- Enable Partition: This selection allows you to create parti tions in a folder structure based on one or\nmultiple columns. Each distinct column value (pair) is a new pa rtition. For example, \"year=2000/month=01/\nfile\". This selection supports insert-only mode and requires an empty directory in the destination.\n----Partition column name: Select from the destination columns in schemas mapping. Supported data types\nare string, integer, boolean, and datetime. Format respects typ e conversion settings under the Mapping\ntab.\nIncorrect: Not A:\n* Append: Append new values to existing table.\n* Etc. Not C: The following tables contain more information about a co py activity in Lakehouse.\nSource information\n* Enable partition discoveryWhether to parse the partitions from the file path and add them as extra source columns.\n* Etc. Reference:\nhttps://learn.microsoft.com/en-us/fabric/data-factory/connector -lakehouse-copy-activity", "boxes": []}, {"id": 61, "shown": "8", "type": "mc", "stem": "You have source data in a folder on a local computer.\nYou need to create a solution that will use Fabric to populate a data store. The solution must meet the\nfollowing requirements:\nSupport the use of dataflows to load and append data to the dat a store.\nEnsure that Delta tables are V-Order optimized and compacted au tomatically.\nWhich two types of data stores should you use? Each correct ans wer presents a complete solution.\nNOTE : Each correct selection is worth one point.", "options": [{"key": "A", "text": "a lakehouse"}, {"key": "B", "text": "an Azure SQL database"}, {"key": "C", "text": "a warehouse"}, {"key": "D", "text": "a KQL database"}], "correct": "AC", "multi": true, "explanation": "Delta Lake table format interoperabilityIn Microsoft Fabric, the Delta Lake table format is the standar d for analytics. Delta Lake is an open-source\nstorage layer that brings ACID (Atomicity, Consistency, Isolati on, Durability) transactions to big data and\nanalytics workloads.\nAll Fabric experiences generate and consume Delta Lake tables, driving interoperability and a unified\nproduct experience. Delta Lake tables produced by one compute e ngine, such as *Synapse Data\nwarehouse* or Synapse Spark, can be consumed by any other engin e, such as Power BI. When you\ningest data into Fabric, Fabric stores it as Delta tables by de fault. You can easily integrate external data\ncontaining Delta Lake tables by using OneLake shortcuts.\nThe following matrix shows key Delta Lake features and their su pport on each Fabric capability.\nEtc.", "reference": "Explanation:\nDelta Lake table format interoperabilityIn Microsoft Fabric, the Delta Lake table format is the standar d for analytics. Delta Lake is an open-source\nstorage layer that brings ACID (Atomicity, Consistency, Isolati on, Durability) transactions to big data and\nanalytics workloads.\nAll Fabric experiences generate and consume Delta Lake tables, driving interoperability and a unified\nproduct experience. Delta Lake tables produced by one compute e ngine, such as *Synapse Data\nwarehouse* or Synapse Spark, can be consumed by any other engin e, such as Power BI. When you\ningest data into Fabric, Fabric stores it as Delta tables by de fault. You can easily integrate external data\ncontaining Delta Lake tables by using OneLake shortcuts.\nThe following matrix shows key Delta Lake features and their su pport on each Fabric capability.\nEtc.\nReference:\nhttps://learn.microsoft.com/en-us/fabric/get-started/delta-lake -interoperability", "boxes": [], "image": "images/p077.jpg"}, {"id": 62, "shown": "9", "type": "hotspot", "stem": "You have a Fabric tenant that contains a lakehouse.You are using a Fabric notebook to save a large DataFrame by us ing the following code.\ndf.write.partitionBy(“year”, “month”, “day”).mode(“overwrite”).parquet(“Files/\nSalesOrder”)\nFor each of the following statements, select Yes if the stateme nt is true. Otherwise, select No.\nNOTE:  Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: Yes\nPartitionBy segregates data into folders.\nNote: PySpark partitionBy() is a function of pyspark.sql.DataFr ameWriter class which is used to partition\nthe large dataset (DataFrame) into smaller files based on one o r multiple columns while writing to disk-\nBox 2: YesBox 3: No", "reference": "Explanation:\nBox 1: Yes\nPartitionBy segregates data into folders.\nNote: PySpark partitionBy() is a function of pyspark.sql.DataFr ameWriter class which is used to partition\nthe large dataset (DataFrame) into smaller files based on one o r multiple columns while writing to disk-\nBox 2: YesBox 3: NoReference:\nhttps://sparkbyexamples.com/pyspark/pyspark-partitionby-example /", "boxes": [{"box": 1, "value": "Yes"}, {"box": 2, "value": "YesBox 3: NoReference:"}], "image": "images/p078.jpg"}, {"id": 63, "shown": "10", "type": "mc", "stem": "You have a Fabric tenant that contains a data pipeline.\nYou need to ensure that the pipeline runs every four hours on M ondays and Fridays.\nTo what should you set Repeat for the schedule?", "options": [{"key": "A", "text": "Daily"}, {"key": "B", "text": "By the minute"}, {"key": "C", "text": "Weekly"}, {"key": "D", "text": "Hourly"}], "correct": "C", "multi": false, "explanation": "", "reference": "", "boxes": [], "image": "images/p079.jpg"}, {"id": 64, "shown": "11", "type": "dragdrop", "stem": "You are creating a data flow in Fabric to ingest data from an A zure SQL database by using a T-SQL\nstatement.\nYou need to ensure that any foldable Power Query transformation  steps are processed by the Microsoft\nSQL Server engine.\nHow should you complete the code? To answer, drag the appropria te values to the correct targets. Each\nvalue may be used once, more than once, or not at all. You may need to drag the split bar between panes\nor scroll to view content.\nNOTE:  Each correct selection is worth one point.\nSelect and Place:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: Value\nQuery folding on native queriesUse Value.NativeQuery functionThe goal of this process is to execute the following SQL code, and to apply more transformations with\n\nPower Query that can be folded back to the source.\nSELECT DepartmentID, Name FROM HumanResources.Department WHERE GroupName = 'Research\nand Development'\nThe first step was to define the correct target, which in this case is the database where the SQL code will\nbe run. Once a step has the correct target, you can select that step—in this case, Source in Applied Steps\n—and then select the fx button in the formula bar to add a cust om step. In this example, replace the\nSource formula with the following formula:\nValue.NativeQuery(Source, \"SELECT DepartmentID, Name FROM Human Resources.Department\nWHERE GroupName = 'Research and Development'\nBox 2: NativeQueryBox 3: EnableFolding\nThe most important component of this formula is the use of the optional record for the forth parameter of\nthe function that has the EnableFolding record field set to tru e.", "reference": "Explanation:\nBox 1: Value\nQuery folding on native queriesUse Value.NativeQuery functionThe goal of this process is to execute the following SQL code, and to apply more transformations with\n\nPower Query that can be folded back to the source.\nSELECT DepartmentID, Name FROM HumanResources.Department WHERE GroupName = 'Research\nand Development'\nThe first step was to define the correct target, which in this case is the database where the SQL code will\nbe run. Once a step has the correct target, you can select that step—in this case, Source in Applied Steps\n—and then select the fx button in the formula bar to add a cust om step. In this example, replace the\nSource formula with the following formula:\nValue.NativeQuery(Source, \"SELECT DepartmentID, Name FROM Human Resources.Department\nWHERE GroupName = 'Research and Development'\nBox 2: NativeQueryBox 3: EnableFolding\nThe most important component of this formula is the use of the optional record for the forth parameter of\nthe function that has the EnableFolding record field set to tru e.\nReference:\nhttps://learn.microsoft.com/en-us/power-query/native-query-fold ing", "boxes": [{"box": 1, "value": "Value"}, {"box": 2, "value": "NativeQueryBox 3: EnableFolding"}], "image": "images/p080.jpg"}, {"id": 65, "shown": "12", "type": "hotspot", "stem": "You have a Fabric tenant that contains a lakehouse named Lakeho use1. Lakehouse1 contains a table\nnamed Nyctaxi_raw. Nyctaxi_raw contains the following table:\nYou create a Fabric notebook and attach it to Lakehouse1.\nYou need to use PySpark code to transform the data. The solutio n must meet the following requirements:\nAdd a column named pickupDate that will contain only the date p ortion of pickupDateTime.\nFilter the DataFrame to include only rows where fareAmount is a  positive number that is less than 100.\nHow should you complete the code? To answer, select the appropr iate options in the answer area.\nNOTE:  Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: df.withColumnRenamed\nAdd a column named pickupDate that will contain only the date p ortion of pickupDateTime.\nwithColumnRenamed(existing, new)[source]\nReturns a new DataFrame by renaming an existing column. This is a no-op if schema doesn’t contain the\ngiven column name.\nParameters:\nexisting – string, name of the existing column to rename.col – string, new name of the column.>>> df.withColumnRenamed('age', 'age2').collect()[Row(age2=2, name='Alice'), Row(age2=5, name='Bob')]\nIncorrect:\n* df.withColumnwithColumn(colName, col)[source]Returns a new DataFrame by adding a column or replacing the exi sting column that has the same name.\nThe column expression must be an expression over this DataFrame ; attempting to add a column from\nsome other dataframe will raise an error.\nParameters:\ncolName – string, name of the new column.col – a Column expression for the new column.>>> df.withColumn('age2', df.age + 2).collect()[Row(age=2, name='Alice', age2=4), Row(age=5, name='Bob', age2= 7)]\nBox 2: cast('date')\ncast(dataType)[source]Convert the column into type dataType.\n\n>>> df.select(df.age.cast(\"string\").alias('ages')).collect()\n[Row(ages='2'), Row(ages='5')]>>> df.select(df.age.cast(StringType()).alias('ages')).collect( )\n[Row(ages='2'), Row(ages='5')]\nBox 3: .filter(\"fareAmount > 0 AND fareAmount < 100\"\nFilter the DataFrame to include only rows where fareAmount is a positive number that is less than 100.\nfilter(condition)[source]\nFilters rows using the given condition.\nwhere() is an alias for filter().>>> df.filter(df.age > 3).collect()\n[Row(age=5, name='Bob')]>>> df.where(df.age == 2).collect()[Row(age=2, name='Alice')]>>> df.filter(\"age > 3\").collect()[Row(age=5, name='Bob')]>>> df.where(\"age = 2\").collect()[Row(age=2, name='Alice')]\nIncorrect:\n*.whereIsin will not give the desired result.\nNote: In Apache Spark, the where() function can be used to filt er rows in a DataFrame based on a given\ncondition. The condition is specified as a string that is evalu ated for each row in the DataFrame. Rows for\nwhich the condition evaluates to True are retained, while those for which it evaluates to False are removed.\nisin(*cols)[source]\nA boolean expression that is evaluated to true if the value of this expression is contained by the evaluated\nvalues of the arguments.\n>>> df[df.name.isin(\"Bob\", \"Mike\")].collect()\n[Row(age=5, name='Bob')]>>> df[df.age.isin([1, 2, 3])].collect()[Row(age=2, name='Alice')]", "reference": "Explanation:\nBox 1: df.withColumnRenamed\nAdd a column named pickupDate that will contain only the date p ortion of pickupDateTime.\nwithColumnRenamed(existing, new)[source]\nReturns a new DataFrame by renaming an existing column. This is a no-op if schema doesn’t contain the\ngiven column name.\nParameters:\nexisting – string, name of the existing column to rename.col – string, new name of the column.>>> df.withColumnRenamed('age', 'age2').collect()[Row(age2=2, name='Alice'), Row(age2=5, name='Bob')]\nIncorrect:\n* df.withColumnwithColumn(colName, col)[source]Returns a new DataFrame by adding a column or replacing the exi sting column that has the same name.\nThe column expression must be an expression over this DataFrame ; attempting to add a column from\nsome other dataframe will raise an error.\nParameters:\ncolName – string, name of the new column.col – a Column expression for the new column.>>> df.withColumn('age2', df.age + 2).collect()[Row(age=2, name='Alice', age2=4), Row(age=5, name='Bob', age2= 7)]\nBox 2: cast('date')\ncast(dataType)[source]Convert the column into type dataType.\n\n>>> df.select(df.age.cast(\"string\").alias('ages')).collect()\n[Row(ages='2'), Row(ages='5')]>>> df.select(df.age.cast(StringType()).alias('ages')).collect( )\n[Row(ages='2'), Row(ages='5')]\nBox 3: .filter(\"fareAmount > 0 AND fareAmount < 100\"\nFilter the DataFrame to include only rows where fareAmount is a positive number that is less than 100.\nfilter(condition)[source]\nFilters rows using the given condition.\nwhere() is an alias for filter().>>> df.filter(df.age > 3).collect()\n[Row(age=5, name='Bob')]>>> df.where(df.age == 2).collect()[Row(age=2, name='Alice')]>>> df.filter(\"age > 3\").collect()[Row(age=5, name='Bob')]>>> df.where(\"age = 2\").collect()[Row(age=2, name='Alice')]\nIncorrect:\n*.whereIsin will not give the desired result.\nNote: In Apache Spark, the where() function can be used to filt er rows in a DataFrame based on a given\ncondition. The condition is specified as a string that is evalu ated for each row in the DataFrame. Rows for\nwhich the condition evaluates to True are retained, while those for which it evaluates to False are removed.\nisin(*cols)[source]\nA boolean expression that is evaluated to true if the value of this expression is contained by the evaluated\nvalues of the arguments.\n>>> df[df.name.isin(\"Bob\", \"Mike\")].collect()\n[Row(age=5, name='Bob')]>>> df[df.age.isin([1, 2, 3])].collect()[Row(age=2, name='Alice')]\nReference:\nhttps://spark.apache.org/docs/2.3.0/api/python/pyspark.sql.html", "boxes": [{"box": 1, "value": "df.withColumnRenamed"}, {"box": 2, "value": "cast('date')"}, {"box": 3, "value": ".filter(\"fareAmount > 0 AND fareAmount < 100\""}], "images": ["images/p081.jpg", "images/p082.jpg"]}, {"id": 66, "shown": "13", "type": "hotspot", "stem": "You have a Fabric tenant.You need to configure OneLake security for users shown in the f ollowing table.\nThe solution must follow the principle of least privilege.\nWhich permission should you assign to each user? To answer, sel ect the appropriate options in the\nanswer area.\nNOTE : Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: ReadAll\nUser1, Read all the Spark data\nIf the “Read all Apache Spark” box is checked, users will be gi ven ReadAll. This permission allows users to\naccess data in OneLake. This could be through direct OneLake ac cess, Apache Spark queries, or the\nlakehouse UX.\nBox 2: ReadData\nUser2, Read all the SQL endpoint data\nIf the “Read all SQL endpoint data” is checked, users will be g iven the ReadData permission. ReadData\ngives access to all Tables in the item when accessing through t he SQL Endpoint. Users will not be able to\naccess OneLake directly.", "reference": "Explanation:\nBox 1: ReadAll\nUser1, Read all the Spark data\nIf the “Read all Apache Spark” box is checked, users will be gi ven ReadAll. This permission allows users to\naccess data in OneLake. This could be through direct OneLake ac cess, Apache Spark queries, or the\nlakehouse UX.\nBox 2: ReadData\nUser2, Read all the SQL endpoint data\nIf the “Read all SQL endpoint data” is checked, users will be g iven the ReadData permission. ReadData\ngives access to all Tables in the item when accessing through t he SQL Endpoint. Users will not be able to\naccess OneLake directly.\nReference:\nhttps://support.fabric.microsoft.com/en-us/blog/building-common -data-architectures-with-onelake-in-\nmicrosoft-fabric", "boxes": [{"box": 1, "value": "ReadAll"}, {"box": 2, "value": "ReadData"}], "image": "images/p083.jpg"}, {"id": 67, "shown": "14", "type": "dragdrop", "stem": "You are implementing a medallion architecture in a single Fabri c workspace.\nYou have a lakehouse that contains the Bronze and Silver layers  and a warehouse that contains the Gold\nlayer.\nYou create the items required to populate the layers as shown i n the following table.\nYou need to ensure that the layers are populated daily in seque ntial order such that Silver is populated\nonly after Bronze is complete, and Gold is populated only after  Silver is complete. The solution must\nminimize development effort and complexity.\nWhat should you use to execute each set of items? To answer, dr ag the appropriate options to the correct\nitems. Each option may be used once, more than once, or not at all. You may need to drag the split bar\nbetween panes or scroll to view content.\nNOTE : Each correct selection is worth one point.\nSelect and Place:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: A schedule\nOrchestration pipeline\n\nRun and schedule the data pipeline\nBox 2: A pipeline Copy activity\nBronze layer, pipelines with Copy activities (Lakehouse)\nConfigure Lakehouse in a copy activity\nUse the copy activity in a data pipeline to copy data from and to the Fabric Lakehouse.\nBox 3: A pipeline Dataflow activity\nSilver layer, dataflows (Lakehouse)\nMicrosoft Fabric, Data Factory, Use a dataflow in a pipeline\nA dataflow is a reusable data transformation that can be used i n a pipeline.\nBox 4: A pipeline Stored procedure actvitity\nGold layer, stored procedures (warehouse)\nAzure Data Factory, Transform data by using the SQL Server Stor ed Procedure activity in Azure Data\nFactory or Synapse Analytics", "reference": "Explanation:\nBox 1: A schedule\nOrchestration pipeline\n\nRun and schedule the data pipeline\nBox 2: A pipeline Copy activity\nBronze layer, pipelines with Copy activities (Lakehouse)\nConfigure Lakehouse in a copy activity\nUse the copy activity in a data pipeline to copy data from and to the Fabric Lakehouse.\nBox 3: A pipeline Dataflow activity\nSilver layer, dataflows (Lakehouse)\nMicrosoft Fabric, Data Factory, Use a dataflow in a pipeline\nA dataflow is a reusable data transformation that can be used i n a pipeline.\nBox 4: A pipeline Stored procedure actvitity\nGold layer, stored procedures (warehouse)\nAzure Data Factory, Transform data by using the SQL Server Stor ed Procedure activity in Azure Data\nFactory or Synapse Analytics\nReference:\nhttps://learn.microsoft.com/en-us/fabric/data-factory/connector -lakehouse-copy-activity\nhttps://learn.microsoft.com/en-us/fabric/data-factory/tutorial- dataflows-gen2-pipeline-activity\nhttps://learn.microsoft.com/en-us/azure/data-factory/transform- data-using-stored-procedure", "boxes": [{"box": 1, "value": "A schedule"}, {"box": 2, "value": "A pipeline Copy activity"}, {"box": 3, "value": "A pipeline Dataflow activity"}, {"box": 4, "value": "A pipeline Stored procedure actvitity"}], "images": ["images/p084.jpg", "images/p085.jpg"]}, {"id": 68, "shown": "15", "type": "dragdrop", "stem": "You are building a solution by using a Fabric notebook.You have a Spark DataFrame assigned to a variable named df. The  DataFrame returns four columns.\nYou need to change the data type of a string column named Age t o integer. The solution must return a\nDataFrame that includes all the columns.\nHow should you complete the code? To answer, drag the appropria te values to the correct targets. Each\nvalue may be used once, more than once, or not at all. You may need to drag the split bar between panes\nor scroll to view content.\nNOTE : Each correct selection is worth one point.\nSelect and Place:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: withColumn\nIn PySpark, we can use the cast method to change the data type.\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql import functions as F\n# first method\ndf = df.withColumn(\"Age\", df.age.cast(\"int\"))\n# second method\ndf = df.withColumn(\"Age\", df.age.cast(IntegerType()))\n# third method <-- This one\ndf = df.withColumn(\"Age\", F.col(\"Age\").cast(IntegerType()))\nBox 2: colBox 3: cast", "reference": "Explanation:\nBox 1: withColumn\nIn PySpark, we can use the cast method to change the data type.\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql import functions as F\n# first method\ndf = df.withColumn(\"Age\", df.age.cast(\"int\"))\n# second method\ndf = df.withColumn(\"Age\", df.age.cast(IntegerType()))\n# third method <-- This one\ndf = df.withColumn(\"Age\", F.col(\"Age\").cast(IntegerType()))\nBox 2: colBox 3: castReference:\nhttps://www.aporia.com/resources/how-to/change-column-data-type s-in-dataframe/", "boxes": [{"box": 1, "value": "withColumn"}, {"box": 2, "value": "colBox 3: castReference:"}], "image": "images/p086.jpg"}, {"id": 69, "shown": "16", "type": "hotspot", "stem": "You have an Azure Data Lake Storage Gen2 account named storage1  that contains a Parquet file named\nsales.parquet.\nYou have a Fabric tenant that contains a workspace named Worksp ace1.\nUsing a notebook in Workspace1, you need to load the content of  the file to the default lakehouse. The\nsolution must ensure that the content will display automaticall y as a table named Sales in Lakehouse\nexplorer.\nHow should you complete the code? To answer, select the appropr iate options in the answer area.\nNOTE : Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: delta\nUse a notebook to load data into your Lakehouse\nSaving data in the Lakehouse using capabilities such as Load to Tables or methods described in Options\nto get data into the Fabric Lakehouse, all data is saved in Del ta format.\n# Keep it if you want to save dataframe as a delta lake, parque t table to Tables section of the default\nLakehouse\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(delta_ta ble_name)\n# Keep it if you want to save the dataframe as a delta lake, ap pending the data to an existing table\ndf.write.mode(\"append\").format(\"delta\").saveAsTable(delta_table _name)\nQuestion: The solution must ensure that the content will displa y automatically as a table named Sales in\nLakehouse explorer.\nBox 2: files/sales", "reference": "Explanation:\nBox 1: delta\nUse a notebook to load data into your Lakehouse\nSaving data in the Lakehouse using capabilities such as Load to Tables or methods described in Options\nto get data into the Fabric Lakehouse, all data is saved in Del ta format.\n# Keep it if you want to save dataframe as a delta lake, parque t table to Tables section of the default\nLakehouse\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(delta_ta ble_name)\n# Keep it if you want to save the dataframe as a delta lake, ap pending the data to an existing table\ndf.write.mode(\"append\").format(\"delta\").saveAsTable(delta_table _name)\nQuestion: The solution must ensure that the content will displa y automatically as a table named Sales in\nLakehouse explorer.\nBox 2: files/salesReference:\nhttps://learn.microsoft.com/en-us/fabric/data-engineering/lakeh ouse-notebook-load-data\nhttps://learn.microsoft.com/en-us/fabric/data-engineering/lakeh ouse-and-delta-tables", "boxes": [{"box": 1, "value": "delta"}, {"box": 2, "value": "files/salesReference:"}], "image": "images/p087.jpg"}, {"id": 70, "shown": "17", "type": "mc", "stem": "You have a Fabric workspace named Workspace1 that contains a la kehouse named Lakehouse1.\nIn Workspace1, you create a data pipeline named Pipeline1.You have CSV files stored in an Azure Storage account.You need to add an activity to Pipeline1 that will copy data fr om the CSV files to Lakehouse1. The activity\nmust support Power Query M formula language expressions.\nWhich type of activity should you add?", "options": [{"key": "A", "text": "Dataflow"}, {"key": "B", "text": "Notebook"}, {"key": "C", "text": "Script"}, {"key": "D", "text": "Copy data"}], "correct": "A", "multi": false, "explanation": "Power Query activity in Azure Data FactoryThe Power Query activity allows you to build and execute Power Query mash-ups to execute data\nwrangling at scale in a Data Factory pipeline. You can create a new Power Query mash-up from the New\nresources menu option or by adding a Power Activity to your pip eline.\nTranslation to data flow script\nTo achieve scale with your Power Query activity, Azure Data Fac tory translates your M script into a data\nflow script so that you can execute your Power Query at scale u sing the Azure Data Factory data flow\nSpark environment.\nExample:", "reference": "Explanation:\nPower Query activity in Azure Data FactoryThe Power Query activity allows you to build and execute Power Query mash-ups to execute data\nwrangling at scale in a Data Factory pipeline. You can create a new Power Query mash-up from the New\nresources menu option or by adding a Power Activity to your pip eline.\nTranslation to data flow script\nTo achieve scale with your Power Query activity, Azure Data Fac tory translates your M script into a data\nflow script so that you can execute your Power Query at scale u sing the Azure Data Factory data flow\nSpark environment.\nExample:\nReference:\nhttps://learn.microsoft.com/en-us/azure/data-factory/control-fl ow-power-query-activity", "boxes": [], "image": "images/p088.jpg"}, {"id": 71, "shown": "18", "type": "hotspot", "stem": "You have a Fabric tenant that contains lakehouse named Lakehous e1. Lakehouse1 contains a Delta table\nwith eight columns.\nYou receive new data that contains the same eight columns and t wo additional columns.\nYou create a Spark DataFrame and assign the DataFrame to a vari able named df. The DataFrame\ncontains the new data.\nYou need to add the new data to the Delta table to meet the fol lowing requirements:\nKeep all the existing rows.\nEnsure that all the new data is added to the table.\nHow should you complete the code? To answer, select the appropr iate options in the answer area.\nNOTE : Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: append\nMode \"append\" atomically adds new data to an existing Delta tab le and \"overwrite\" atomically replaces all\nof the data in a table.\nBox 2: overwriteSchema falseExplicitly update schema to change column type or name\nYou can change a column’s type or name or drop a column by rewr iting the table. To do this, use the\noverwriteSchema option.\nThe following example shows changing a column type: (spark.read.table(...)\n .withColumn(\"birthDate\", col(\"birthDate\").cast(\"date\")) .write .mode(\"overwrite\") .option(\"overwriteSchema\", \"true\") .saveAsTable(...)), when performing an Overwrite, the data will be deleted befor e writing out the new data.", "reference": "Explanation:\nBox 1: append\nMode \"append\" atomically adds new data to an existing Delta tab le and \"overwrite\" atomically replaces all\nof the data in a table.\nBox 2: overwriteSchema falseExplicitly update schema to change column type or name\nYou can change a column’s type or name or drop a column by rewr iting the table. To do this, use the\noverwriteSchema option.\nThe following example shows changing a column type: (spark.read.table(...)\n .withColumn(\"birthDate\", col(\"birthDate\").cast(\"date\")) .write .mode(\"overwrite\") .option(\"overwriteSchema\", \"true\") .saveAsTable(...)), when performing an Overwrite, the data will be deleted befor e writing out the new data.\nReference:\nhttps://learn.microsoft.com/en-us/azure/databricks/delta/update -schema", "boxes": [{"box": 1, "value": "append"}, {"box": 2, "value": "overwriteSchema falseExplicitly update schema to change column type or name"}], "images": ["images/p089.jpg", "images/p090.jpg"]}, {"id": 72, "shown": "19", "type": "mc", "stem": "You have a Fabric tenant that contains a lakehouse.\nYou plan to use a visual query to merge two tables.You need to ensure that the query returns all the rows in both tables.\nWhich type of join should you use?", "options": [{"key": "A", "text": "inner"}, {"key": "B", "text": "full outer"}, {"key": "C", "text": "left outer"}, {"key": "D", "text": "right anti"}, {"key": "E", "text": "right outer"}, {"key": "F", "text": "left anti"}], "correct": "B", "multi": false, "explanation": "The FULL OUTER JOIN keyword returns all records when there is a match in left (table1) or right (table2)\ntable records.", "reference": "Explanation:\nThe FULL OUTER JOIN keyword returns all records when there is a match in left (table1) or right (table2)\ntable records.\n\nReference:\nhttps://www.w3schools.com/sql/sql_join_full.asp", "boxes": []}, {"id": 73, "shown": "20", "type": "dragdrop", "stem": "You are implementing two dimension tables named Customers and P roducts in a Fabric warehouse.\nYou need to use slowly changing dimension (SCD) to manage the v ersioning of data. The solution must\nmeet the requirements shown in the following table.\nWhich type of SCD should you use for each table? To answer, dra g the appropriate SCD types to the\ncorrect tables. Each SCD type may be used once, more than once,  or not at all. You may need to drag the\nsplit bar between panes or scroll to view content.\nNOTE : Each correct selection is worth one point.\nSelect and Place:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: Type 2\nThere are 6 types of Slowly Changing Dimension that are commonl y used, they are as follows:\nType 0 – Fixed Dimension\nNo changes allowed, dimension never changes\nType 1 – No History\nUpdate record directly, there is no record of historical values , only current state\nType 2 – Row Versioning\nTrack changes as version records with current flag & active dat es and other metadata\nType 3 – Previous Value column\nTrack change to a specific attribute, add a column to show the previous value, which is updated as further\nchanges occur\nEtc. Box 2: Type 1", "reference": "Explanation:\nBox 1: Type 2\nThere are 6 types of Slowly Changing Dimension that are commonl y used, they are as follows:\nType 0 – Fixed Dimension\nNo changes allowed, dimension never changes\nType 1 – No History\nUpdate record directly, there is no record of historical values , only current state\nType 2 – Row Versioning\nTrack changes as version records with current flag & active dat es and other metadata\nType 3 – Previous Value column\nTrack change to a specific attribute, add a column to show the previous value, which is updated as further\nchanges occur\nEtc. Box 2: Type 1Reference:\nhttps://adatis.co.uk/introduction-to-slowly-changing-dimensions -scd-types/", "boxes": [{"box": 1, "value": "Type 2"}, {"box": 2, "value": "Type 1Reference:"}], "image": "images/p092.jpg"}, {"id": 74, "shown": "21", "type": "mc", "stem": "You have a Fabric workspace named Workspace1 and an Azure SQL d atabase.\nYou plan to create a dataflow that will read data from the data base, and then transform the data by\nperforming an inner join.\nYou need to ignore spaces in the values when performing the inn er join. The solution must minimize\ndevelopment effort.\nWhat should you do?", "options": [{"key": "A", "text": "Append the queries by using fuzzy matching."}, {"key": "B", "text": "Merge the queries by using fuzzy matching."}, {"key": "C", "text": "Append the queries by using a lookup table."}, {"key": "D", "text": "Merge the queries by using a lookup table."}], "correct": "B", "multi": false, "explanation": "Joins are merge operations.\nJoin transformation in mapping data flow\nUse the join transformation to combine data from two sources or streams in a mapping data flow. The\noutput stream will include all columns from both sources matche d based on a join condition.\nInner join only outputs rows that have matching values in both tables.\nFuzzy join\nYou can choose to join based on fuzzy join logic instead of exa ct column value matching by turning on the\n\"Use fuzzy matching\" checkbox option.*-> Combine text parts: Use this option to find matches by remo ve space between words. For example,\nData Factory is matched with DataFactory if this option is enab led.\nSimilarity score column: You can optionally choose to store the matching score for each row in a column\nby entering a new column name here to store that value.Similarity threshold: Choose a value between 60 and 100 as a pe rcentage match between values in the\ncolumns you've selected.", "reference": "Explanation:\nJoins are merge operations.\nJoin transformation in mapping data flow\nUse the join transformation to combine data from two sources or streams in a mapping data flow. The\noutput stream will include all columns from both sources matche d based on a join condition.\nInner join only outputs rows that have matching values in both tables.\nFuzzy join\nYou can choose to join based on fuzzy join logic instead of exa ct column value matching by turning on the\n\"Use fuzzy matching\" checkbox option.*-> Combine text parts: Use this option to find matches by remo ve space between words. For example,\nData Factory is matched with DataFactory if this option is enab led.\nSimilarity score column: You can optionally choose to store the matching score for each row in a column\nby entering a new column name here to store that value.Similarity threshold: Choose a value between 60 and 100 as a pe rcentage match between values in the\ncolumns you've selected.\nReference:https://learn.microsoft.com/en-us/azure/data-factory/data-flow- join", "boxes": [], "image": "images/p093.jpg"}, {"id": 75, "shown": "22", "type": "mc", "stem": "You have a Fabric tenant that contains a warehouse named Wareho use1. Warehouse1 contains two\nschemas name schema1 and schema2 and a table named schema1.city .\nYou need to make a copy of schema1.city in schema2. The solutio n must minimize the copying of data.\nWhich T-SQL statement should you run?", "options": [{"key": "A", "text": "INSERT INTO schema2.city SELECT * FROM schema1.city;"}, {"key": "B", "text": "SELECT * INTO schema2.city FROM schema1.city;"}, {"key": "C", "text": "CREATE TABLE schema2.city AS CLONE OF schema1.city;"}, {"key": "D", "text": "CREATE TABLE schema2.city AS SELECT * FROM schema1.city;"}], "correct": "C", "multi": false, "explanation": "CREATE TABLE AS CLONE OFApplies to: Warehouse in Microsoft Fabric\nCreates a new table as a zero-copy clone of another table in Wa rehouse in Microsoft Fabric. Only the\nmetadata of the table is copied. The underlying data of the tab le, stored as parquet files, is not copied.", "reference": "Explanation:\nCREATE TABLE AS CLONE OFApplies to: Warehouse in Microsoft Fabric\nCreates a new table as a zero-copy clone of another table in Wa rehouse in Microsoft Fabric. Only the\nmetadata of the table is copied. The underlying data of the tab le, stored as parquet files, is not copied.\nReference:\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-t able-as-clone-of-transact-sql", "boxes": [], "image": "images/p094.jpg"}, {"id": 76, "shown": "23", "type": "hotspot", "stem": "You have a Fabric tenant that contains a workspace named Worksp ace1. Workspace1 contains a\nlakehouse named Lakehouse1 and a warehouse named Warehouse1.\nYou need to create a new table in Warehouse1 named POSCustomers  by querying the customer table in\nLakehouse1.\nHow should you complete the T-SQL statement? To answer, select the appropriate options in the answer\narea.\nNOTE : Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: CREATE TABLE dbo.POSCustomers as SELECT Box 2: FROM lakehous1.dbo.customer\nSpecify the lakehouse and the table within the lakehouse.", "reference": "Explanation:\nBox 1: CREATE TABLE dbo.POSCustomers as SELECT Box 2: FROM lakehous1.dbo.customer\nSpecify the lakehouse and the table within the lakehouse.", "boxes": [{"box": 1, "value": "CREATE TABLE dbo.POSCustomers as SELECT Box 2: FROM lakehous1.dbo.customer"}], "image": "images/p095.jpg"}, {"id": 77, "shown": "24", "type": "mc", "stem": "You have a Fabric tenant.\nYou are creating a Fabric Data Factory pipeline.You have a stored procedure that returns the number of active c ustomers and their average sales for the\ncurrent month.\nYou need to add an activity that will execute the stored proced ure in a warehouse. The returned values\nmust be available to the downstream activities of the pipeline.\nWhich type of activity should you add?", "options": [{"key": "A", "text": "Switch"}, {"key": "B", "text": "Copy data"}, {"key": "C", "text": "Append variable"}, {"key": "D", "text": "Lookup"}], "correct": "D", "multi": false, "explanation": "Lookup ActivityLookup Activity can be used to read or look up a record/ table name/ value from any external source. This\noutput can further be referenced by succeeding activities.\n\nNote: Lookup activity can retrieve a dataset from any of the da ta sources supported by data factory and\nSynapse pipelines. You can use it to dynamically determine whic h objects to operate on in a subsequent\nactivity, instead of hard coding the object name. Some object e xamples are files and tables.\nLookup activity reads and returns the content of a configuratio n file or table. It also returns the result of\nexecuting a query or stored procedure. The output can be a sing leton value or an array of attributes, which\ncan be consumed in a subsequent copy, transformation, or contro l flow activities like ForEach activity.\nIncorrect:\n* Append variableAppend Variable activity in Azure Data Factory and Synapse Anal ytics\nUse the Append Variable activity to add a value to an existing array variable defined in a Data Factory or\nSynapse Analytics pipeline", "reference": "Explanation:\nLookup ActivityLookup Activity can be used to read or look up a record/ table name/ value from any external source. This\noutput can further be referenced by succeeding activities.\n\nNote: Lookup activity can retrieve a dataset from any of the da ta sources supported by data factory and\nSynapse pipelines. You can use it to dynamically determine whic h objects to operate on in a subsequent\nactivity, instead of hard coding the object name. Some object e xamples are files and tables.\nLookup activity reads and returns the content of a configuratio n file or table. It also returns the result of\nexecuting a query or stored procedure. The output can be a sing leton value or an array of attributes, which\ncan be consumed in a subsequent copy, transformation, or contro l flow activities like ForEach activity.\nIncorrect:\n* Append variableAppend Variable activity in Azure Data Factory and Synapse Anal ytics\nUse the Append Variable activity to add a value to an existing array variable defined in a Data Factory or\nSynapse Analytics pipeline\nReference:\nhttps://learn.microsoft.com/en-us/azure/data-factory/control-fl ow-lookup-activity\nhttps://learn.microsoft.com/en-us/azure/data-factory/control-fl ow-append-variable-activity", "boxes": [], "image": "images/p096.jpg"}, {"id": 78, "shown": "25", "type": "mc", "stem": "You have a Fabric tenant that contains two workspaces named Wor kspace1 and Workspace2.\nWorkspace1 contains a lakehouse named Lakehouse1. Workspace2 co ntains a lakehouse named\nLakehouse2. Lakehouse1 contains a table named dbo.Sales. Lakeho use2 contains a table named\ndbo.Customers.\nYou need to ensure that you can write queries that reference bo th dbo.Sales and dbo.Customers in the\nsame SQL query without making additional copies of the tables.\nWhat should you use?", "options": [{"key": "A", "text": "a shortcut"}, {"key": "B", "text": "a dataflow"}, {"key": "C", "text": "a view"}, {"key": "D", "text": "a managed table"}], "correct": "A", "multi": false, "explanation": "", "reference": "", "boxes": []}, {"id": 79, "shown": "26", "type": "mc", "stem": "You have a Fabric tenant that contains a warehouse.\nYou are designing a star schema model that will contain a custo mer dimension. The customer dimension\ntable will be a Type 2 slowly changing dimension (SCD).\nYou need to recommend which columns to add to the table. The co lumns must NOT already exist in the\nsource.\nWhich three types of columns should you recommend? Each correct  answer presents part of the solution.\nNOTE : Each correct answer is worth one point.", "options": [{"key": "A", "text": "a foreign key"}, {"key": "B", "text": "a natural key"}, {"key": "C", "text": "an effective end date and time"}, {"key": "D", "text": "a surrogate key"}, {"key": "E", "text": "an effective start date and time"}], "correct": "CDE", "multi": true, "explanation": "Type 2 SCDA Type 2 SCD supports versioning of dimension members. Often th e source system doesn't store\nversions, so the data warehouse load process detects and manage s changes in a dimension table. In this\ncase, the dimension table must use a *surrogate key* to provide a unique reference to a version of the\ndimension member. It also includes columns that define the date range validity of the version (for example,\nStartDate and EndDate) and possibly a flag column (for example, IsCurrent) to easily filter by current\ndimension members.\nFor example, Adventure Works assigns salespeople to a sales reg ion. When a salesperson relocates\nregion, a new version of the salesperson must be created to ens ure that historical facts remain associated\nwith the former region. To support accurate historic analysis o f sales by salesperson, the dimension table\nmust store versions of salespeople and their associated region( s). The table should also include *start and\nend date* values to define the time validity. Current versions may define an empty end date (or\n12/31/9999), which indicates that the row is the current versio n. The table must also define a surrogate key\nbecause the business key (in this instance, employee ID) won't be unique.", "reference": "Explanation:\nType 2 SCDA Type 2 SCD supports versioning of dimension members. Often th e source system doesn't store\nversions, so the data warehouse load process detects and manage s changes in a dimension table. In this\ncase, the dimension table must use a *surrogate key* to provide a unique reference to a version of the\ndimension member. It also includes columns that define the date range validity of the version (for example,\nStartDate and EndDate) and possibly a flag column (for example, IsCurrent) to easily filter by current\ndimension members.\nFor example, Adventure Works assigns salespeople to a sales reg ion. When a salesperson relocates\nregion, a new version of the salesperson must be created to ens ure that historical facts remain associated\nwith the former region. To support accurate historic analysis o f sales by salesperson, the dimension table\nmust store versions of salespeople and their associated region( s). The table should also include *start and\nend date* values to define the time validity. Current versions may define an empty end date (or\n12/31/9999), which indicates that the row is the current versio n. The table must also define a surrogate key\nbecause the business key (in this instance, employee ID) won't be unique.\nReference:\nhttps://learn.microsoft.com/en-us/training/modules/populate-slo wly-changing-dimensions-azure-synapse-\nanalytics-pipelines/3-choose-between-dimension-types", "boxes": []}, {"id": 80, "shown": "27", "type": "mc", "stem": "You have a Fabric tenant.\nYou plan to create a data pipeline named Pipeline1. Pipeline1 w ill include two activities that will execute in\nsequence.\nYou need to ensure that a failure of the first activity will NO T block the second activity.\nWhich conditional path should you configure between the first a ctivity and the second activity?", "options": [{"key": "A", "text": "Upon Failure"}, {"key": "B", "text": "Upon Completion"}, {"key": "C", "text": "Upon Skip"}, {"key": "D", "text": "Upon Success"}], "correct": "B", "multi": false, "explanation": "Conditional pathsAzure Data Factory and Synapse Pipeline orchestration allows co nditional logic and enables the user to\ntake a different path based upon outcomes of a previous activit y. Using different paths allow users to build\nrobust pipelines and incorporates error handling in ETL/ELT log ic. In total, we allow four conditional paths.\n\n* Upon Success\n(Default Pass) Execute this path if the current activity succee ded\n* Upon Failure\nExecute this path if the current activity failed\n*-> Upon Completion\nExecute this path after the current activity completed, regardl ess if it succeeded or not\n* Upon Skip\nExecute this path if the activity itself didn't run\nYou may add multiple branches following an activity, with one e xception: Upon Completion path can't\ncoexist with either Upon Success or Upon Failure path. For each pipeline run, at most one path is\nactivated, based on the execution outcome of the activity.", "reference": "Explanation:\nConditional pathsAzure Data Factory and Synapse Pipeline orchestration allows co nditional logic and enables the user to\ntake a different path based upon outcomes of a previous activit y. Using different paths allow users to build\nrobust pipelines and incorporates error handling in ETL/ELT log ic. In total, we allow four conditional paths.\n\n* Upon Success\n(Default Pass) Execute this path if the current activity succee ded\n* Upon Failure\nExecute this path if the current activity failed\n*-> Upon Completion\nExecute this path after the current activity completed, regardl ess if it succeeded or not\n* Upon Skip\nExecute this path if the activity itself didn't run\nYou may add multiple branches following an activity, with one e xception: Upon Completion path can't\ncoexist with either Upon Success or Upon Failure path. For each pipeline run, at most one path is\nactivated, based on the execution outcome of the activity.\nReference:\nhttps://learn.microsoft.com/en-us/azure/data-factory/tutorial-p ipeline-failure-error-handling", "boxes": [], "image": "images/p098.jpg"}, {"id": 81, "shown": "28", "type": "dragdrop", "stem": "You have a Fabric tenant that contains a Microsoft Power BI rep ort named Report1.\nReport1 is slow to render. You suspect that an inefficient DAX query is being executed.\nYou need to identify the slowest DAX query, and then review how  long the query spends in the formula\nengine as compared to the storage engine.\nWhich five actions should you perform in sequence? To answer, m ove the appropriate actions from the list\nof actions to the answer area and arrange them in the correct o rder.\nSelect and Place:", "options": [], "correct": "", "multi": false, "explanation": "Step 1: From Performance analyzer, capture a recording\nPerformance Analyzer to increase performanceThe performance analyzer is a tool, built in to Power BI, that can help you to find different aspects that\n\nmake your report to run slowly.\nHow do you get the results from Performance Analyzer?\nTo get the result above from the performance analyzer, use the steps below:\n1. Create a new blank page (This is needed to not have any cache when running the tool)\n2. Go to View -> Performance Analyzer and start recording (While inside the Report Tab)\n3. Go to specific page that you want to see the performance of4. Wait until the performance analyzer is done and you will get the information you see on the right picture\nabove.\n5. (Optional), Press Export to export this to an external progra m if you want to use the data from the\nperformance analyzer.\nStep 2: Sort the Duration (ms) column in descending order.\nNote: How do you interpret the results of the Performance Analy zer?\nDifferent parts of the Performance AnalyzerFor each visual, we can see three headers as well as a number b esides them:\n\nDAX query\nVisual displayOtherDuration\nDuration (ms) is how long it takes for the specific aspect abov e to render. Important to notice is that it is in\nmilliseconds, 1384 milliseconds means 1.38 seconds.\nDAX query\nThe DAX query is what is generated automatically when a specifi c object/visual is gathering the data from\nthe original source. All visuals according to the DAX query can be seen as tables, no matter visual they\nare.\nStep 3: Copy the first query to DAX Studio\nHow to limit the time it takes to run the DAX query\nChange the DAX measures to use the Formula Engine and Storage E ngine optimally\nDAX studio is the best way to analyze how you can minimize the DAX query timings. You can see how\nmuch time it takes for the Formula Engine respectively the Stor age Engine to run a specific query in there.\nStep 4: Enable Query Times and Server Timings. Run the query.\nServer timings tells exactly how long the Formula Engine and th e Server Engine takes.\nStep 5: View the Server Timings tabNote: The Storage Engine is much quicker than the Formula Engin e, but can handle much less complex\nformulas.By rewriting the different DAX measures, and testing these out in DAX studio, you can change the\ncomplexity of the measures, move them from the Formula Engine t o the Storage Engine, and make the\nperformance of the model much better.", "reference": "Explanation:\nStep 1: From Performance analyzer, capture a recording\nPerformance Analyzer to increase performanceThe performance analyzer is a tool, built in to Power BI, that can help you to find different aspects that\n\nmake your report to run slowly.\nHow do you get the results from Performance Analyzer?\nTo get the result above from the performance analyzer, use the steps below:\n1. Create a new blank page (This is needed to not have any cache when running the tool)\n2. Go to View -> Performance Analyzer and start recording (While inside the Report Tab)\n3. Go to specific page that you want to see the performance of4. Wait until the performance analyzer is done and you will get the information you see on the right picture\nabove.\n5. (Optional), Press Export to export this to an external progra m if you want to use the data from the\nperformance analyzer.\nStep 2: Sort the Duration (ms) column in descending order.\nNote: How do you interpret the results of the Performance Analy zer?\nDifferent parts of the Performance AnalyzerFor each visual, we can see three headers as well as a number b esides them:\n\nDAX query\nVisual displayOtherDuration\nDuration (ms) is how long it takes for the specific aspect abov e to render. Important to notice is that it is in\nmilliseconds, 1384 milliseconds means 1.38 seconds.\nDAX query\nThe DAX query is what is generated automatically when a specifi c object/visual is gathering the data from\nthe original source. All visuals according to the DAX query can be seen as tables, no matter visual they\nare.\nStep 3: Copy the first query to DAX Studio\nHow to limit the time it takes to run the DAX query\nChange the DAX measures to use the Formula Engine and Storage E ngine optimally\nDAX studio is the best way to analyze how you can minimize the DAX query timings. You can see how\nmuch time it takes for the Formula Engine respectively the Stor age Engine to run a specific query in there.\nStep 4: Enable Query Times and Server Timings. Run the query.\nServer timings tells exactly how long the Formula Engine and th e Server Engine takes.\nStep 5: View the Server Timings tabNote: The Storage Engine is much quicker than the Formula Engin e, but can handle much less complex\nformulas.By rewriting the different DAX measures, and testing these out in DAX studio, you can change the\ncomplexity of the measures, move them from the Formula Engine t o the Storage Engine, and make the\nperformance of the model much better.\nReference:\nhttps://medium.com/nerd-for-tech/tips-to-increase-the-performan ce-of-your-power-bi-reports-\n566004c35e9b\nhttps://learn.microsoft.com/en-us/power-bi/create-reports/deskt op-performance-analyzer", "boxes": [], "images": ["images/p099.jpg", "images/p100.jpg"]}, {"id": 82, "shown": "29", "type": "mc", "stem": "You have a Microsoft Power BI semantic model that contains meas ures. The measures use multiple\nCALCULATE functions and a FILTER function.\nYou are evaluating the performance of the measures.In which use case will replacing the FILTER function with the K EEPFILTERS function reduce execution\ntime?", "options": [{"key": "A", "text": "when the FILTER function uses a nested calculate function"}, {"key": "B", "text": "when the FILTER function references a measure"}, {"key": "C", "text": "when the FILTER function references columns from multiple tab les"}, {"key": "D", "text": "when the FILTER function references a column from a single ta ble that uses Import mode"}], "correct": "D", "multi": false, "explanation": "", "reference": "", "boxes": []}, {"id": 83, "shown": "30", "type": "mc", "stem": "Note: This question is part of a series of questions that prese nt the same scenario. Each question\nin the series contains a unique solution that might meet the st ated goals. Some question\nsets might have more than one correct solution, while others mi ght not have a correct solution.\nAfter you answer a question in this section, you will NOT be ab le to return to it. As a result, these\nquestions will not appear in the review screen.\nYou have a Fabric tenant that contains a semantic model named M odel1.\nYou discover that the following query performs slowly against M odel1.\nYou need to reduce the execution time of the query.\nSolution: You replace line 4 by using the following code:\nISEMPTY ( RELATEDTABLE ( 'Order Item' ) )\nDoes this meet the goal?", "options": [{"key": "A", "text": "Yes"}, {"key": "B", "text": "No"}], "correct": "B", "multi": false, "explanation": "Correct: NOT ISEMPTY ( CALCULATETABLE ( 'Order Item ' ) )\nJust check if it is empty or not.\nNote: ISEMPTY\nChecks if a table is empty.\nSyntax\nISEMPTY(<table_expression>)\nParameters\ntable_expression - A table reference or a DAX expression that r eturns a table.\nReturn value - True if the table is empty (has no rows), if els e, False.\nIncorrect:\n* CALCULATE ( COUNTROWS ( 'Order Item' ) ) >= 0\n* ISEMPTY ( RELATEDTABLE ( 'Order Item' ) )", "reference": "Explanation:\nCorrect: NOT ISEMPTY ( CALCULATETABLE ( 'Order Item ' ) )\nJust check if it is empty or not.\nNote: ISEMPTY\nChecks if a table is empty.\nSyntax\nISEMPTY(<table_expression>)\nParameters\ntable_expression - A table reference or a DAX expression that r eturns a table.\nReturn value - True if the table is empty (has no rows), if els e, False.\nIncorrect:\n* CALCULATE ( COUNTROWS ( 'Order Item' ) ) >= 0\n* ISEMPTY ( RELATEDTABLE ( 'Order Item' ) )\nReference:\nhttps://learn.microsoft.com/en-us/dax/isempty-function-dax", "boxes": []}, {"id": 84, "shown": "31", "type": "mc", "stem": "Note: This question is part of a series of questions that prese nt the same scenario. Each question\nin the series contains a unique solution that might meet the st ated goals. Some question\nsets might have more than one correct solution, while others mi ght not have a correct solution.\nAfter you answer a question in this section, you will NOT be ab le to return to it. As a result, these\nquestions will not appear in the review screen.\nYou have a Fabric tenant that contains a semantic model named M odel1.\nYou discover that the following query performs slowly against M odel1.\nYou need to reduce the execution time of the query.\nSolution: You replace line 4 by using the following code:\nNOT ( ISEMPTY ( CALCULATETABLE ( 'Order Item ' ) ) )\nDoes this meet the goal?", "options": [{"key": "A", "text": "Yes"}, {"key": "B", "text": "No"}], "correct": "A", "multi": false, "explanation": "Correct: NOT ISEMPTY ( CALCULATETABLE ( 'Order Item ' ) )\nJust check if it is empty or not.\nNote: ISEMPTY\nChecks if a table is empty.\nSyntax\nISEMPTY(<table_expression>)\nParameters\ntable_expression - A table reference or a DAX expression that r eturns a table.\nReturn value - True if the table is empty (has no rows), if els e, False.\nIncorrect:\n* CALCULATE ( COUNTROWS ( 'Order Item' ) ) >= 0\n* ISEMPTY ( RELATEDTABLE ( 'Order Item' ) )", "reference": "Explanation:\nCorrect: NOT ISEMPTY ( CALCULATETABLE ( 'Order Item ' ) )\nJust check if it is empty or not.\nNote: ISEMPTY\nChecks if a table is empty.\nSyntax\nISEMPTY(<table_expression>)\nParameters\ntable_expression - A table reference or a DAX expression that r eturns a table.\nReturn value - True if the table is empty (has no rows), if els e, False.\nIncorrect:\n* CALCULATE ( COUNTROWS ( 'Order Item' ) ) >= 0\n* ISEMPTY ( RELATEDTABLE ( 'Order Item' ) )\nReference:\nhttps://learn.microsoft.com/en-us/dax/isempty-function-dax", "boxes": [], "image": "images/p102.jpg"}, {"id": 85, "shown": "32", "type": "mc", "stem": "Note: This question is part of a series of questions that prese nt the same scenario. Each question\nin the series contains a unique solution that might meet the st ated goals. Some question\nsets might have more than one correct solution, while others mi ght not have a correct solution.\nAfter you answer a question in this section, you will NOT be ab le to return to it. As a result, these\nquestions will not appear in the review screen.\nYou have a Fabric tenant that contains a semantic model named M odel1.\nYou discover that the following query performs slowly against M odel1.\nYou need to reduce the execution time of the query.\nSolution: You replace line 4 by using the following code:\nCALCULATE ( COUNTROWS ( 'Order Item' ) ) >= 0\nDoes this meet the goal?", "options": [{"key": "A", "text": "Yes"}, {"key": "B", "text": "No"}], "correct": "B", "multi": false, "explanation": "Correct: NOT ISEMPTY ( CALCULATETABLE ( 'Order Item ' ) )\nJust check if it is empty or not.\nNote: ISEMPTY\nChecks if a table is empty.\nSyntax\nISEMPTY(<table_expression>)\nParameters\ntable_expression - A table reference or a DAX expression that r eturns a table.\nReturn value - True if the table is empty (has no rows), if els e, False.\nIncorrect:\n* CALCULATE ( COUNTROWS ( 'Order Item' ) ) >= 0\n* ISEMPTY ( RELATEDTABLE ( 'Order Item' ) )", "reference": "Explanation:\nCorrect: NOT ISEMPTY ( CALCULATETABLE ( 'Order Item ' ) )\nJust check if it is empty or not.\nNote: ISEMPTY\nChecks if a table is empty.\nSyntax\nISEMPTY(<table_expression>)\nParameters\ntable_expression - A table reference or a DAX expression that r eturns a table.\nReturn value - True if the table is empty (has no rows), if els e, False.\nIncorrect:\n* CALCULATE ( COUNTROWS ( 'Order Item' ) ) >= 0\n* ISEMPTY ( RELATEDTABLE ( 'Order Item' ) )\nReference:\nhttps://learn.microsoft.com/en-us/dax/isempty-function-dax", "boxes": [], "image": "images/p103.jpg"}, {"id": 86, "shown": "33", "type": "hotspot", "stem": "You have a Fabric workspace that uses the default Spark starter  pool and runtime version 1.2.\nYou plan to read a CSV file named Sales_raw.csv in a lakehouse,  select columns, and save the data as a\nDelta table to the managed area of the lakehouse. Sales_raw.csv  contains 12 columns.\nYou have the following code.\nFor each of the following statements, select Yes if the stateme nt is true. Otherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: Yes\nYes - The Spark engine will read only the 'SalesOrderNumber','O rderDate','CustomerName','UnitPrice'\ncolumns from Sales_raw.csv.\nNote:\nDataFrame.select(*cols: ColumnOrName) → DataFrame[source]Projects a set of expressions and returns a new DataFrame\n\nParameters\ncolsstr, Column, or listcolumn names (string) or expressions (Column). If one of the co lumn names is ‘*’, that column is expanded\nto include all columns in the current DataFrame.\nBox 2: No\nNo - The Year column replaces the OrderDate column in the table .\nwithColumn adds one extra columnNote: pyspark.sql.dataframe.DataFrame[source]\nReturns a new DataFrame by adding multiple columns or replacing the existing columns that have the\nsame names.\nThe colsMap is a map of column name and column, the column must only refer to attributes supplied by\nthis Dataset. It is an error to add columns that refer to some other Dataset.\nBox 3: Yes\nYes - Adding inferSchema='true' to the options will increase th e execution time of the query.\nWhen you set inferSchema to True, PySpark will make an addition al pass over the data to determine the\ndata types of each column. This can be useful when you don't ha ve a predefined schema for your data and\nwant Spark to automatically deduce the types based on the actua l data values.", "reference": "Explanation:\nBox 1: Yes\nYes - The Spark engine will read only the 'SalesOrderNumber','O rderDate','CustomerName','UnitPrice'\ncolumns from Sales_raw.csv.\nNote:\nDataFrame.select(*cols: ColumnOrName) → DataFrame[source]Projects a set of expressions and returns a new DataFrame\n\nParameters\ncolsstr, Column, or listcolumn names (string) or expressions (Column). If one of the co lumn names is ‘*’, that column is expanded\nto include all columns in the current DataFrame.\nBox 2: No\nNo - The Year column replaces the OrderDate column in the table .\nwithColumn adds one extra columnNote: pyspark.sql.dataframe.DataFrame[source]\nReturns a new DataFrame by adding multiple columns or replacing the existing columns that have the\nsame names.\nThe colsMap is a map of column name and column, the column must only refer to attributes supplied by\nthis Dataset. It is an error to add columns that refer to some other Dataset.\nBox 3: Yes\nYes - Adding inferSchema='true' to the options will increase th e execution time of the query.\nWhen you set inferSchema to True, PySpark will make an addition al pass over the data to determine the\ndata types of each column. This can be useful when you don't ha ve a predefined schema for your data and\nwant Spark to automatically deduce the types based on the actua l data values.\nReference:\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspa rk.sql/api/\npyspark.sql.DataFrame.select.html\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspa rk.sql/api/\npyspark.sql.DataFrame.withColumns.html\nhttps://medium.com/@sujathamudadla1213/what-are-the-considerati ons-and-implications-of-setting-\ninferschema-to-true-and-false-in-pyspark-9fc77fa2ad9a", "boxes": [{"box": 1, "value": "Yes"}, {"box": 2, "value": "No"}, {"box": 3, "value": "Yes"}], "images": ["images/p104.jpg", "images/p105.jpg"]}, {"id": 87, "shown": "34", "type": "mc", "stem": "You have a Fabric tenant.\nYou are creating a Fabric Data Factory pipeline.You have a stored procedure that returns the number of active c ustomers and their average sales for the\ncurrent month.\nYou need to add an activity that will execute the stored proced ure in a warehouse. The returned values\nmust be available to the downstream activities of the pipeline.\nWhich type of activity should you add?", "options": [{"key": "A", "text": "Switch"}, {"key": "B", "text": "KQL"}, {"key": "C", "text": "Append variable"}, {"key": "D", "text": "Lookup"}], "correct": "D", "multi": false, "explanation": "Lookup ActivityLookup Activity can be used to read or look up a record/ table name/ value from any external source. This\noutput can further be referenced by succeeding activities.\nNote: Lookup activity can retrieve a dataset from any of the da ta sources supported by data factory and\nSynapse pipelines. You can use it to dynamically determine whic h objects to operate on in a subsequent\nactivity, instead of hard coding the object name. Some object e xamples are files and tables.\n\nLookup activity reads and returns the content of a configuratio n file or table. It also returns the result of\nexecuting a query or stored procedure. The output can be a sing leton value or an array of attributes, which\ncan be consumed in a subsequent copy, transformation, or contro l flow activities like ForEach activity.\nIncorrect:\n* Append variableAppend Variable activity in Azure Data Factory and Synapse Anal ytics\nUse the Append Variable activity to add a value to an existing array variable defined in a Data Factory or\nSynapse Analytics pipeline\n* Copy data\nIn Data Pipeline, you can use the Copy activity to copy data am ong data stores located in the cloud.\nAfter you copy the data, you can use other activities to furthe r transform and analyze it. You can also use\nthe Copy activity to publish transformation and analysis result s for business intelligence (BI) and\napplication consumption.\n* KQL\nThe KQL activity in Data Factory for Microsoft Fabric allows yo u to run a query in Kusto Query Language\n(KQL) against an Azure Data Explorer instance.\n* Switch\nThe Switch activity in Microsoft Fabric provides the same funct ionality that a switch statement provides in\nprogramming languages. It evaluates a set of activities corresp onding to a case that matches the condition\nevaluation.", "reference": "Explanation:\nLookup ActivityLookup Activity can be used to read or look up a record/ table name/ value from any external source. This\noutput can further be referenced by succeeding activities.\nNote: Lookup activity can retrieve a dataset from any of the da ta sources supported by data factory and\nSynapse pipelines. You can use it to dynamically determine whic h objects to operate on in a subsequent\nactivity, instead of hard coding the object name. Some object e xamples are files and tables.\n\nLookup activity reads and returns the content of a configuratio n file or table. It also returns the result of\nexecuting a query or stored procedure. The output can be a sing leton value or an array of attributes, which\ncan be consumed in a subsequent copy, transformation, or contro l flow activities like ForEach activity.\nIncorrect:\n* Append variableAppend Variable activity in Azure Data Factory and Synapse Anal ytics\nUse the Append Variable activity to add a value to an existing array variable defined in a Data Factory or\nSynapse Analytics pipeline\n* Copy data\nIn Data Pipeline, you can use the Copy activity to copy data am ong data stores located in the cloud.\nAfter you copy the data, you can use other activities to furthe r transform and analyze it. You can also use\nthe Copy activity to publish transformation and analysis result s for business intelligence (BI) and\napplication consumption.\n* KQL\nThe KQL activity in Data Factory for Microsoft Fabric allows yo u to run a query in Kusto Query Language\n(KQL) against an Azure Data Explorer instance.\n* Switch\nThe Switch activity in Microsoft Fabric provides the same funct ionality that a switch statement provides in\nprogramming languages. It evaluates a set of activities corresp onding to a case that matches the condition\nevaluation.\nReference:\nhttps://learn.microsoft.com/en-us/azure/data-factory/control-fl ow-lookup-activity\nhttps://learn.microsoft.com/en-us/azure/data-factory/control-fl ow-append-variable-activity", "boxes": []}, {"id": 88, "shown": "35", "type": "mc", "stem": "Note: This question is part of a series of questions that prese nt the same scenario. Each question\nin the series contains a unique solution that might meet the st ated goals. Some question\nsets might have more than one correct solution, while others mi ght not have a correct solution.\nAfter you answer a question in this section, you will NOT be ab le to return to it. As a result, these\nquestions will not appear in the review screen.\nYou have a Fabric tenant that contains a semantic model named M odel1.\nYou discover that the following query performs slowly against M odel1.\nYou need to reduce the execution time of the query.\nSolution: You replace line 4 by using the following code:\nNOT ( CALCULATE ( COUNTROWS ( 'Order Item' ) ) < 0)\nDoes this meet the goal?", "options": [{"key": "A", "text": "Yes"}, {"key": "B", "text": "No"}], "correct": "B", "multi": false, "explanation": "Correct: You replace line 4 by using the following code:\nNOT ISEMPTY ( CALCULATETABLE ( 'Order Item ' ) )\nJust check if it is empty or not.\nNote: ISEMPTY\nChecks if a table is empty.\nSyntax\nISEMPTY(<table_expression>)\nParameters\ntable_expression - A table reference or a DAX expression that r eturns a table.\nReturn value - True if the table is empty (has no rows), if els e, False.\nIncorrect:\n* CALCULATE ( COUNTROWS ( 'Order Item' ) ) >= 0\n* ISEMPTY ( RELATEDTABLE ( 'Order Item' ) )\n* NOT ( CALCULATE ( COUNTROWS ( 'Order Item' ) ) < 0)", "reference": "Explanation:\nCorrect: You replace line 4 by using the following code:\nNOT ISEMPTY ( CALCULATETABLE ( 'Order Item ' ) )\nJust check if it is empty or not.\nNote: ISEMPTY\nChecks if a table is empty.\nSyntax\nISEMPTY(<table_expression>)\nParameters\ntable_expression - A table reference or a DAX expression that r eturns a table.\nReturn value - True if the table is empty (has no rows), if els e, False.\nIncorrect:\n* CALCULATE ( COUNTROWS ( 'Order Item' ) ) >= 0\n* ISEMPTY ( RELATEDTABLE ( 'Order Item' ) )\n* NOT ( CALCULATE ( COUNTROWS ( 'Order Item' ) ) < 0)\nReference:\nhttps://learn.microsoft.com/en-us/dax/isempty-function-dax", "boxes": [], "image": "images/p107.jpg"}, {"id": 89, "shown": "36", "type": "mc", "stem": "You have a Fabric tenant.\nYou are creating a Fabric Data Factory pipeline.You have a stored procedure that returns the number of active c ustomers and their average sales for the\ncurrent month.\nYou need to add an activity that will execute the stored proced ure in a warehouse. The returned values\nmust be available to the downstream activities of the pipeline.\nWhich type of activity should you add?", "options": [{"key": "A", "text": "Get metadata"}, {"key": "B", "text": "Copy data"}, {"key": "C", "text": "Lookup"}, {"key": "D", "text": "Append variable"}], "correct": "C", "multi": false, "explanation": "The Lookup activity is specifically designed to execute a query or stored procedure and retrieve data from\na data source, such as a warehouse. It allows you to capture th e output, which can then be used in\nsubsequent activities in the pipeline.\nBy using the Lookup activity, you can access the returned value s (number of active customers and their\n\naverage sales) and pass them on to other activities for further processing.", "reference": "Explanation:\nThe Lookup activity is specifically designed to execute a query or stored procedure and retrieve data from\na data source, such as a warehouse. It allows you to capture th e output, which can then be used in\nsubsequent activities in the pipeline.\nBy using the Lookup activity, you can access the returned value s (number of active customers and their\n\naverage sales) and pass them on to other activities for further processing.", "boxes": []}, {"id": 90, "shown": "37", "type": "mc", "stem": "You have an Amazon Web Services (AWS) subscription that contain s an Amazon Simple Storage Service\n(Amazon S3) bucket named bucket1.\nYou have a Fabric tenant that contains a lakehouse named LH1.In LH1, you plan to create a OneLake shortcut to bucket1.You need to configure authentication for the connection.Which two values should you provide? Each correct answer presen ts part of the solution.\nNOTE : Each correct selection is worth one point.", "options": [{"key": "A", "text": "the shared access signature (SAS) token"}, {"key": "B", "text": "the secret access key"}, {"key": "C", "text": "the access ID"}, {"key": "D", "text": "the access key ID"}, {"key": "E", "text": "the certificate thumbprint"}], "correct": "BD", "multi": true, "explanation": "the secret access key\nThe secret access key is used in conjunction with the access ke y ID to authenticate requests to AWS\nservices, including S3.\nthe access key ID\nThe access key ID is a unique identifier associated with the AW S account or IAM user that you will use to\nauthenticate requests to the S3 bucket.", "reference": "Explanation:\nthe secret access key\nThe secret access key is used in conjunction with the access ke y ID to authenticate requests to AWS\nservices, including S3.\nthe access key ID\nThe access key ID is a unique identifier associated with the AW S account or IAM user that you will use to\nauthenticate requests to the S3 bucket.", "boxes": []}, {"id": 91, "shown": "38", "type": "mc", "stem": "You have a Fabric tenant.\nYou are creating a Fabric Data Factory pipeline.You have a stored procedure that returns the number of active c ustomers and their average sales for the\ncurrent month.\nYou need to add an activity that will execute the stored proced ure in a warehouse. The returned values\nmust be available to the downstream activities of the pipeline.\nWhich type of activity should you add?", "options": [{"key": "A", "text": "Append variable"}, {"key": "B", "text": "Lookup"}, {"key": "C", "text": "Copy data"}, {"key": "D", "text": "KQL"}], "correct": "B", "multi": false, "explanation": "The Lookup activity is specifically designed for executing queries or sto red procedures and retrieving data\nfrom a data source. It allows you to capture the output from th e stored procedure, making it available for\nuse in subsequent activities within the pipeline.\n\nThis is particularly useful for scenarios where you need to pro cess or route data based on the results\nreturned from a stored procedure.", "reference": "Explanation:\nThe Lookup activity is specifically designed for executing queries or sto red procedures and retrieving data\nfrom a data source. It allows you to capture the output from th e stored procedure, making it available for\nuse in subsequent activities within the pipeline.\n\nThis is particularly useful for scenarios where you need to pro cess or route data based on the results\nreturned from a stored procedure.", "boxes": []}, {"id": 92, "shown": "39", "type": "hotspot", "stem": "You have a Fabric tenant that contains a PySpark notebook named  Notebook1.\nYou define sas_token as a variable in the first cell of Noteboo k1 and store a shared access signature\n(SAS) token in the variable.\nIn the second cell, you run the following code.\nFor each of the following statements, select Yes if the stateme nt is true. Otherwise, select No.\nNOTE : Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "customers is a pandas DataFrame. - No.\nThe customers DataFrame is created using spark.read.parquet() , which is part of PySpark.\n\nTherefore, customers is a Spark DataFrame, not a pandas DataFrame.\nIf a delta table named Customers does NOT exist, an error will be generated. - No.\nThe code uses .mode(\"overwrite\") , which will create the table if it does not exist. There will not be an\nerror if the table does not exist initially.\nThe source data is located in the customers folder in a container named contacts . - Yes.\nThe URI wasbs://contacts@contoso.blob.core.windows.net/customers specifies that the\ndata is stored in a container named contacts within the folder customers .", "reference": "Explanation:\ncustomers is a pandas DataFrame. - No.\nThe customers DataFrame is created using spark.read.parquet() , which is part of PySpark.\n\nTherefore, customers is a Spark DataFrame, not a pandas DataFrame.\nIf a delta table named Customers does NOT exist, an error will be generated. - No.\nThe code uses .mode(\"overwrite\") , which will create the table if it does not exist. There will not be an\nerror if the table does not exist initially.\nThe source data is located in the customers folder in a container named contacts . - Yes.\nThe URI wasbs://contacts@contoso.blob.core.windows.net/customers specifies that the\ndata is stored in a container named contacts within the folder customers .", "boxes": [], "image": "images/p110.jpg"}, {"id": 93, "shown": "40", "type": "mc", "stem": "You have a Fabric tenant that contains a warehouse named DW1 an d a lakehouse named LH1. DW1\ncontains a table named Sales.Product. LH1 contains a table name d Sales.Orders.\nYou plan to schedule an automated process that will create a ne w point-in-time (PIT) table named\nSales.ProductOrder in DW1. Sales.ProductOrder will be built by using the results of a query that will join\nSales.Product and Sales.Orders.\nYou need to ensure that the types of columns in Sales.ProductOr der match the column types in the source\ntables. The solution must minimize the number of operations req uired to create the new table.\nWhich operation should you use?", "options": [{"key": "A", "text": "INSERT INTO"}, {"key": "B", "text": "CREATE TABLE AS SELECT (CTAS)"}, {"key": "C", "text": "CREATE TABLE AS CLONE OF"}, {"key": "D", "text": "CREATE MATERIALIZED VIEW AS SELECT"}], "correct": "B", "multi": false, "explanation": "The CREATE TABLE AS SELECT (CTAS) statement allows you to create a new table based on the result\nof a SELECT query. This method automatically defines the new table's colum ns with the same names and\ndata types as those in the result set of the query, ensuring co nsistency with the source tables.", "reference": "Explanation:\nThe CREATE TABLE AS SELECT (CTAS) statement allows you to create a new table based on the result\nof a SELECT query. This method automatically defines the new table's colum ns with the same names and\ndata types as those in the result set of the query, ensuring co nsistency with the source tables.", "boxes": [], "image": "images/p111.jpg"}, {"id": 94, "shown": "41", "type": "mc", "stem": "You have a Fabric workspace named Workspace1 that contains a la kehouse named Lakehouse1.\nLakehouse1 contains a table named Table1. Table1 contains the f ollowing data.\nYou need to perform the following actions:\nLoad the data from Table1 into a star schema.\nCreate a product dimension table named DimProduct and a fact ta ble named FactSales.\nWhich three columns should you include in DimProduct?", "options": [{"key": "A", "text": "ProductColor, ProductID, and ProductName."}, {"key": "B", "text": "ProductName, SalesAmount, and TransactionlD."}, {"key": "C", "text": "Date, ProductID, and TransactionlD."}, {"key": "D", "text": "ProductID, ProductName, and SalesAmount"}], "correct": "A", "multi": false, "explanation": "DimProduct is the product dimension table, so it should include attribute s that describe the product:\nProductID : A unique identifier for the product (acts as the key for the dimension table).\nProductName : Describes the name of the product.\nProductColor : Describes the product’s color.", "reference": "Explanation:\nDimProduct is the product dimension table, so it should include attribute s that describe the product:\nProductID : A unique identifier for the product (acts as the key for the dimension table).\nProductName : Describes the name of the product.\nProductColor : Describes the product’s color.", "boxes": [], "image": "images/p111.jpg"}, {"id": 95, "shown": "42", "type": "dragdrop", "stem": "You have a Fabric workspace that contains a Dataflow Gen2 query . The query returns the following data.\nYou need to filter the results to ensure that only the latest v ersion of each customer’s record is retained.\nThe solution must ensure that no new columns are loaded to the semantic model.\nWhich four actions should you perform in sequence in Power Quer y Editor? To answer, move the\nappropriate actions from the list of actions to the answer area  and arrange them in the correct order.\nSelect and Place:", "options": [], "correct": "", "multi": false, "explanation": "1. Group by CustomerID and calculate the max version date per cust omer ID.\nGroup the data by the CustomerID column and calculate the maximum value of the VersionDate\ncolumn for each customer. This ensures you identify the latest version of each customer’s record.\n2. Filter the query where the version date value equals the max ve rsion date value.\nUse the result from the grouping operation to filter the origin al dataset, keeping only the rows where\nVersionDate matches the calculated maximum version date for each CustomerID .\n3. Remove duplicates based on CustomerID.\nEnsure that only one row per CustomerID remains, in case there are multiple records with the same\nmaximum VersionDate .\n4. Remove the max version date column.\nTo ensure no new columns are added to the semantic model, remov e the calculated column after filtering\nthe data.", "reference": "Explanation:\n1. Group by CustomerID and calculate the max version date per cust omer ID.\nGroup the data by the CustomerID column and calculate the maximum value of the VersionDate\ncolumn for each customer. This ensures you identify the latest version of each customer’s record.\n2. Filter the query where the version date value equals the max ve rsion date value.\nUse the result from the grouping operation to filter the origin al dataset, keeping only the rows where\nVersionDate matches the calculated maximum version date for each CustomerID .\n3. Remove duplicates based on CustomerID.\nEnsure that only one row per CustomerID remains, in case there are multiple records with the same\nmaximum VersionDate .\n4. Remove the max version date column.\nTo ensure no new columns are added to the semantic model, remov e the calculated column after filtering\nthe data.", "boxes": [], "image": "images/p112.jpg"}, {"id": 96, "shown": "43", "type": "mc", "stem": "You have a Fabric warehouse that contains a table named Staging .Sales. Staging.Sales contains the\nfollowing columns.\nYou need to write a T-SQL query that will return data for the y ear 2023 that displays ProductID and\nProductName and has a summarized Amount that is higher than 10, 000.\nWhich query should you use?", "options": [{"key": "A", "text": ""}, {"key": "B", "text": ""}, {"key": "C", "text": ""}, {"key": "D", "text": ""}], "correct": "A", "multi": false, "explanation": "SELECT - GROUP BY- Transact-SQLSELECT statement clause that divides the query result into grou ps of rows, usually by performing one or\nmore aggregations on each group. The SELECT statement returns o ne row per group.\nNote: General Remarks\nHow GROUP BY interacts with the SELECT statementSELECT list:\nVector aggregates. If aggregate functions are included in the S ELECT list, GROUP BY calculates a\nsummary value for each group. These are known as vector aggrega tes.\nDistinct aggregates. The aggregates AVG (DISTINCT column_name), COUNT (DISTINCT column_name),\nand SUM (DISTINCT column_name) are supported with ROLLUP, CUBE, and GROUPING SETS.\nWHERE clause:SQL removes Rows that do not meet the conditions in the WHERE c lause before any grouping operation\nis performed.\n*-> HAVING clause:\nSQL uses the having clause to filter groups in the result set.\nIncorrect:\nNot B: Put the 2023 filtering in the WHERE clause, not in the H AVING clause.\nNot C: Need a GROUP BY clause-Not D: Can't use the alias TOTALAMOUNT in the HAVING clause.", "reference": "Explanation:\nSELECT - GROUP BY- Transact-SQLSELECT statement clause that divides the query result into grou ps of rows, usually by performing one or\nmore aggregations on each group. The SELECT statement returns o ne row per group.\nNote: General Remarks\nHow GROUP BY interacts with the SELECT statementSELECT list:\nVector aggregates. If aggregate functions are included in the S ELECT list, GROUP BY calculates a\nsummary value for each group. These are known as vector aggrega tes.\nDistinct aggregates. The aggregates AVG (DISTINCT column_name), COUNT (DISTINCT column_name),\nand SUM (DISTINCT column_name) are supported with ROLLUP, CUBE, and GROUPING SETS.\nWHERE clause:SQL removes Rows that do not meet the conditions in the WHERE c lause before any grouping operation\nis performed.\n*-> HAVING clause:\nSQL uses the having clause to filter groups in the result set.\nIncorrect:\nNot B: Put the 2023 filtering in the WHERE clause, not in the H AVING clause.\nNot C: Need a GROUP BY clause-Not D: Can't use the alias TOTALAMOUNT in the HAVING clause.Reference:\nhttps://learn.microsoft.com/en-us/sql/t-sql/queries/select-grou p-by-transact-sql", "boxes": [], "image": "images/p113.jpg"}, {"id": 97, "shown": "44", "type": "hotspot", "stem": "You have a data warehouse that contains a table named Stage.Cus tomers. Stage.Customers contains all\nthe customer record updates from a customer relationship manage ment (CRM) system. There can be\nmultiple updates per customer.\nYou need to write a T-SQL query that will return the customer I D, name, postal code, and the last updated\ntime of the most recent row for each customer ID.\nHow should you complete the code? To answer, select the appropr iate options in the answer area.\nNOTE:  Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Transact SQL LAST_ValueTransact NTILE\nBox 1: ROW_NUMBER() ()\n* ROW_NUMBER()\n\nNumbers the output of a result set. More specifically, returns the sequential number of a row within a\npartition of a result set, starting at 1 for the first row in e ach partition.\nSyntax: ROW_NUMBER ( )\n OVER ( [ PARTITION BY value_expression , ... [ n ] ] order_ by_clause )\nIncorrect:\n* LAST_VALUE()Incorrect syntax.\nNote: LAST_VALUE (Transact-SQL)\nReturns the last value in an ordered set of values.\nSyntax\nLAST_VALUE ( [ scalar_expression ] ) [ IGNORE NULLS | RESPECT N ULLS ]\n OVER ( [ partition_by_clause ] order_by_clause [ rows_range _clause ] )\n* LAST_Value()* NTILE()\nIncorrect syntax used.\nNote: NTILE (Transact-SQL)\nDistributes the rows in an ordered partition into a specified n umber of groups. The groups are numbered,\nstarting at one. For each row, NTILE returns the number of the group to which the row belongs.\nNTILE (integer_expression) OVER ( [ <partition_by_clause> ] < o rder_by_clause > )\nBox 2: WHERE X = 1", "reference": "Explanation:\nTransact SQL LAST_ValueTransact NTILE\nBox 1: ROW_NUMBER() ()\n* ROW_NUMBER()\n\nNumbers the output of a result set. More specifically, returns the sequential number of a row within a\npartition of a result set, starting at 1 for the first row in e ach partition.\nSyntax: ROW_NUMBER ( )\n OVER ( [ PARTITION BY value_expression , ... [ n ] ] order_ by_clause )\nIncorrect:\n* LAST_VALUE()Incorrect syntax.\nNote: LAST_VALUE (Transact-SQL)\nReturns the last value in an ordered set of values.\nSyntax\nLAST_VALUE ( [ scalar_expression ] ) [ IGNORE NULLS | RESPECT N ULLS ]\n OVER ( [ partition_by_clause ] order_by_clause [ rows_range _clause ] )\n* LAST_Value()* NTILE()\nIncorrect syntax used.\nNote: NTILE (Transact-SQL)\nDistributes the rows in an ordered partition into a specified n umber of groups. The groups are numbered,\nstarting at one. For each row, NTILE returns the number of the group to which the row belongs.\nNTILE (integer_expression) OVER ( [ <partition_by_clause> ] < o rder_by_clause > )\nBox 2: WHERE X = 1Reference:\nhttps://learn.microsoft.com/en-us/sql/t-sql/functions/row-numbe r-transact-sql\nhttps://learn.microsoft.com/en-us/sql/t-sql/functions/ntile-tra nsact-sql\nhttps://learn.microsoft.com/en-us/sql/t-sql/functions/last-valu e-transact-sql", "boxes": [{"box": 1, "value": "ROW_NUMBER() ()"}, {"box": 2, "value": "WHERE X = 1Reference:"}], "images": ["images/p114.jpg", "images/p115.jpg"]}, {"id": 98, "shown": "45", "type": "mc", "stem": "You have a Fabric tenant that contains a warehouse.\nYou use a dataflow to load a new dataset from OneLake to the wa rehouse.\nYou need to add a PowerQuery step to identify the maximum value s for the numeric columns.\nWhich function should you include in the step?", "options": [{"key": "A", "text": "Table.MaxN"}, {"key": "B", "text": "Table.Max"}, {"key": "C", "text": "Table.Range"}, {"key": "D", "text": "Table.Profile"}], "correct": "D", "multi": false, "explanation": "", "reference": "", "boxes": []}, {"id": 99, "shown": "46", "type": "mc", "stem": "You have a Fabric tenant that contains a Microsoft Power BI rep ort named Report1. Report1 includes a\nPython visual.\nData displayed by the visual is grouped automatically and dupli cate rows are NOT displayed.\nYou need all rows to appear in the visual.\nWhat should you do?", "options": [{"key": "A", "text": "Reference the columns in the Python code by index."}, {"key": "B", "text": "Modify the Sort Column By property for all columns."}, {"key": "C", "text": "Add a unique field to each row."}, {"key": "D", "text": "Modify the Summarize By property for all columns."}], "correct": "C", "multi": false, "explanation": "", "reference": "", "boxes": []}, {"id": 100, "shown": "47", "type": "mc", "stem": "You have a Fabric workspace named Workspace1 that contains a da taflow named Dataflow1. Dataflow1\nhas a query that returns 2,000 rows.\nYou view the query in Power Query as shown in the following exh ibit.\nWhat can you identify about the pickupLongitude column?", "options": [{"key": "A", "text": "The column has duplicate values."}, {"key": "B", "text": "All the table rows are profiled."}, {"key": "C", "text": "The column has missing values."}, {"key": "D", "text": "There are 935 values that occur only once."}], "correct": "A", "multi": false, "explanation": "Count is 1000.Distinct count is 935.\nThere are duplicate values.Note: The Count Distinct aggregate function in Power BI is usef ul for counting the number of unique values\nin a column of a dataset. It provides fast, accurate, and effic ient results when you need to calculate the\nnumber of unique items in a data set, such as the number of uni que customers, products, or transactions.\nIncorrect:\nNot B: Null count is 0.Not D: Unique count is 871.", "reference": "Explanation:\nCount is 1000.Distinct count is 935.\nThere are duplicate values.Note: The Count Distinct aggregate function in Power BI is usef ul for counting the number of unique values\nin a column of a dataset. It provides fast, accurate, and effic ient results when you need to calculate the\nnumber of unique items in a data set, such as the number of uni que customers, products, or transactions.\nIncorrect:\nNot B: Null count is 0.Not D: Unique count is 871.\n\nReference:\nhttps://www.onlc.com/blog/what-is-count-distinct-in-power-bi", "boxes": [], "image": "images/p117.jpg"}, {"id": 101, "shown": "48", "type": "mc", "stem": "You have a Fabric workspace named Workspace1 that contains a da ta flow named Dataflow1. Dataflow1\ncontains a query that returns the data shown in the following e xhibit.\nYou need to transform the date columns into attribute-value pai rs, where columns become rows.\nYou select the VendorID column.Which transformation should you select from the context menu of  the VendorID column?", "options": [{"key": "A", "text": "Group by"}, {"key": "B", "text": "Unpivot columns"}, {"key": "C", "text": "Unpivot other columns"}, {"key": "D", "text": "Split column"}, {"key": "E", "text": "Remove other columns"}], "correct": "C", "multi": false, "explanation": "For the VendorID column we see: 2 distinct, 2 unique\nUnpivot columns\nIn Power Query, you can transform columns into attribute-value pairs, where columns become rows.\nFor example, given a table like the following, where country ro ws and date columns create a matrix of\nvalues, it's difficult to analyze the data in a scalable way.\n\nInstead, you can transform the table into a table with unpivote d columns, as shown in the following image.\nIn the transformed table, it's easier to use the date as an att ribute to filter on.\nUnpivot other columns\nYou can select the columns that you don't want to unpivot and u npivot the rest of the columns in the table.\nThis operation is where Unpivot other columns comes into play.", "reference": "Explanation:\nFor the VendorID column we see: 2 distinct, 2 unique\nUnpivot columns\nIn Power Query, you can transform columns into attribute-value pairs, where columns become rows.\nFor example, given a table like the following, where country ro ws and date columns create a matrix of\nvalues, it's difficult to analyze the data in a scalable way.\n\nInstead, you can transform the table into a table with unpivote d columns, as shown in the following image.\nIn the transformed table, it's easier to use the date as an att ribute to filter on.\nUnpivot other columns\nYou can select the columns that you don't want to unpivot and u npivot the rest of the columns in the table.\nThis operation is where Unpivot other columns comes into play.\nReference:\nhttps://learn.microsoft.com/en-us/power-query/unpivot-column", "boxes": [], "image": "images/p118.jpg"}, {"id": 102, "shown": "49", "type": "hotspot", "stem": "You have a Fabric workspace that uses the default Spark starter  pool and runtime version 1.2.\nYou plan to read a CSV file named Sales_raw.csv in a lakehouse,  select columns, and save the data as a\nDelta table to the managed area of the lakehouse. Sales_raw.csv  contains 12 columns.\nYou have the following code.\nFor each of the following statements, select Yes if the stateme nt is true. Otherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: Yes\n\nPySpark Select Columns From DataFrame\nIn PySpark, select() function is used to select single, multipl e, column by index, all columns from the list\nand the nested columns from a DataFrame, PySpark select() is a transformation function hence it returns a\nnew DataFrame with the selected columns.\nSelect Single & Multiple Columns From PySpark\nYou can select the single or multiple columns of the DataFrame by passing the column names you wanted\nto select to the select() function. Since DataFrame is immutabl e, this creates a new DataFrame with\nselected columns. show() function is used to show the Dataframe contents.\nBox 2: NoBox 3: Yes\npyspark.sql.DataFrameReader.csvLoads a CSV file and returns the result as a DataFrame.\nThis function will go through the input once to determine the i nput schema if inferSchema is enabled. To\navoid going through the entire data once, disable inferSchema o ption or specify the schema explicitly using\nschema.\nNote: pyspark.sql.DataFrameWriter.saveAsTable\nSaves the content of the DataFrame as the specified table.", "reference": "Explanation:\nBox 1: Yes\n\nPySpark Select Columns From DataFrame\nIn PySpark, select() function is used to select single, multipl e, column by index, all columns from the list\nand the nested columns from a DataFrame, PySpark select() is a transformation function hence it returns a\nnew DataFrame with the selected columns.\nSelect Single & Multiple Columns From PySpark\nYou can select the single or multiple columns of the DataFrame by passing the column names you wanted\nto select to the select() function. Since DataFrame is immutabl e, this creates a new DataFrame with\nselected columns. show() function is used to show the Dataframe contents.\nBox 2: NoBox 3: Yes\npyspark.sql.DataFrameReader.csvLoads a CSV file and returns the result as a DataFrame.\nThis function will go through the input once to determine the i nput schema if inferSchema is enabled. To\navoid going through the entire data once, disable inferSchema o ption or specify the schema explicitly using\nschema.\nNote: pyspark.sql.DataFrameWriter.saveAsTable\nSaves the content of the DataFrame as the specified table.\nReference:\nhttps://sparkbyexamples.com/pyspark/select-columns-from-pyspark -dataframe/\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspa rk.sql/api/\npyspark.sql.DataFrameReader.csv.html", "boxes": [{"box": 1, "value": "Yes"}, {"box": 2, "value": "NoBox 3: Yes"}], "images": ["images/p119.jpg", "images/p120.jpg"]}, {"id": 103, "shown": "50", "type": "mc", "stem": "You are analyzing customer purchases in a Fabric notebook by us ing PySpark.\nYou have the following DataFrames:\ntransactions:  Contains five columns named transaction_id, customer_id, product_id,\namount , and date  and has 10 million rows, with each row representing a transact ion.\ncustomers:  Contains customer details in 1,000 rows and three columns name d customer_id,\nname , and country .\nYou need to join the DataFrames on the customer_id  column. The solution must minimize data shuffling.\nYou write the following code.\nfrom pyspark.sql import functions as F\nresults =\nWhich code should you run to populate the results DataFrame?\ncustomers.customer_id)\ncustomers.customer_id).distinct()\ncustomers.customer_id)\ncustomers.customer_id)", "options": [{"key": "A", "text": "transactions.join(F.broadcast(customers), transactions.customer_id =="}, {"key": "B", "text": "transactions.join(customers, transactions.customer_id =="}, {"key": "C", "text": "transactions.join(customers, transactions.customer_id =="}, {"key": "D", "text": "transactions.crossJoin(customers).where(transactions.customer_id =="}], "correct": "A", "multi": false, "explanation": "Broadcasting DataFrames in PySpark\n\n In PySpark, working with small DataFrames that are used repeat edly across multiple stages in a\ndistributed processing pipeline can cause performance issues. T o optimize the performance of these\noperations, PySpark provides a mechanism called broadcasting.\nWhen to Use Broadcasting\nBroadcasting should be used when you have a small DataFrame tha t is used multiple times in your\nprocessing pipeline, especially in join operations. Broadcastin g the small DataFrame can significantly\nimprove performance by reducing the amount of data that needs t o be exchanged between worker nodes.\nBroadcasting DataFrames in PySpark\nIn PySpark, you can broadcast a DataFrame using the broadcast() function from the pyspark.sql.functions\nmodule. To use the broadcast() function, simply pass the DataFr ame you want to broadcast as an\nargument:\nExample In Pyspark\nfrom pyspark.sql.functions import broadcast broadcast_dataframe = broadcast(dataframe)\nUsing a Broadcasted DataFrame in a Join Operation\nNow, let's assume we have a larger DataFrame containing sales d ata and want to join it with the\nbroadcasted DataFrame to apply the corresponding discounts:\nExample In Pyspark\n# Create a larger DataFrame with sales data sales_data = [(\"product1\", \"A\", 100), (\"product2\", \"B\", 200), ( \"product3\", \"C\", 300)]\nsales_df = spark.createDataFrame(sales_data, [\"product\", \"categ ory\", \"revenue\"])\n# Join the sales DataFrame with the broadcasted category DataFr ame\nresult_df = sales_df.join(broadcast_category_df, on=\"category\")\nThe join operation will now use the broadcasted DataFrame, sign ificantly reducing the communication\noverhead and improving performance.\nNote: Join\nSyntax: dataframe1.join(dataframe2,dataframe1.column_name == d ataframe2.column_name,”type”)\nwhere,dataframe1 is the first dataframe\ndataframe2 is the second dataframecolumn_name is the column which are matching in both the datafr ames\ntype is the join type we have to join\nIncorrect:\nNot B: Incorrect join syntax.Not C: Normal working join statement. Not optimal here.Not D: Incorrect join syntax.", "reference": "Explanation:\nBroadcasting DataFrames in PySpark\n\n In PySpark, working with small DataFrames that are used repeat edly across multiple stages in a\ndistributed processing pipeline can cause performance issues. T o optimize the performance of these\noperations, PySpark provides a mechanism called broadcasting.\nWhen to Use Broadcasting\nBroadcasting should be used when you have a small DataFrame tha t is used multiple times in your\nprocessing pipeline, especially in join operations. Broadcastin g the small DataFrame can significantly\nimprove performance by reducing the amount of data that needs t o be exchanged between worker nodes.\nBroadcasting DataFrames in PySpark\nIn PySpark, you can broadcast a DataFrame using the broadcast() function from the pyspark.sql.functions\nmodule. To use the broadcast() function, simply pass the DataFr ame you want to broadcast as an\nargument:\nExample In Pyspark\nfrom pyspark.sql.functions import broadcast broadcast_dataframe = broadcast(dataframe)\nUsing a Broadcasted DataFrame in a Join Operation\nNow, let's assume we have a larger DataFrame containing sales d ata and want to join it with the\nbroadcasted DataFrame to apply the corresponding discounts:\nExample In Pyspark\n# Create a larger DataFrame with sales data sales_data = [(\"product1\", \"A\", 100), (\"product2\", \"B\", 200), ( \"product3\", \"C\", 300)]\nsales_df = spark.createDataFrame(sales_data, [\"product\", \"categ ory\", \"revenue\"])\n# Join the sales DataFrame with the broadcasted category DataFr ame\nresult_df = sales_df.join(broadcast_category_df, on=\"category\")\nThe join operation will now use the broadcasted DataFrame, sign ificantly reducing the communication\noverhead and improving performance.\nNote: Join\nSyntax: dataframe1.join(dataframe2,dataframe1.column_name == d ataframe2.column_name,”type”)\nwhere,dataframe1 is the first dataframe\ndataframe2 is the second dataframecolumn_name is the column which are matching in both the datafr ames\ntype is the join type we have to join\nIncorrect:\nNot B: Incorrect join syntax.Not C: Normal working join statement. Not optimal here.Not D: Incorrect join syntax.\nReference:\nhttps://www.sparkcodehub.com/broadcasting-dataframes-in-pyspark\nhttps://www.geeksforgeeks.org/pyspark-join-types-join-two-dataf rames/", "boxes": [], "image": "images/p121.jpg"}, {"id": 104, "shown": "51", "type": "hotspot", "stem": "You have a Fabric warehouse that contains a table named Sales.O rders. Sales.Orders contains the\nfollowing columns.\nYou need to write a T-SQL query that will return the following columns.\nHow should you complete the code? To answer, select the appropr iate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: COALESCE\nCOALESCE (Transact-SQL)Evaluates the arguments in order and returns the current value of the first expression that initially doesn't\nevaluate to NULL. For example, SELECT COALESCE(NULL, NULL, 'thi rd_value', 'fourth_value'); returns\nthe third value because the third value is the first value that isn't null.\nBox 2: LEAST\nLogical functions - LEAST (Transact-SQL)This function returns the minimum value from a list of one or m ore expressions.\nIf one or more arguments aren't NULL, then NULL arguments are i gnored during comparison. If all\narguments are NULL, then LEAST returns NULL.\nSyntax\nLEAST ( expression1 [ , ...expressionN ] )\nIncorrect:\n* MIN (Transact-SQL)Returns the minimum value in the expression. May be followed by the OVER clause.\nSyntax\nMIN ( [ ALL | DISTINCT ] expression )", "reference": "Explanation:\nBox 1: COALESCE\nCOALESCE (Transact-SQL)Evaluates the arguments in order and returns the current value of the first expression that initially doesn't\nevaluate to NULL. For example, SELECT COALESCE(NULL, NULL, 'thi rd_value', 'fourth_value'); returns\nthe third value because the third value is the first value that isn't null.\nBox 2: LEAST\nLogical functions - LEAST (Transact-SQL)This function returns the minimum value from a list of one or m ore expressions.\nIf one or more arguments aren't NULL, then NULL arguments are i gnored during comparison. If all\narguments are NULL, then LEAST returns NULL.\nSyntax\nLEAST ( expression1 [ , ...expressionN ] )\nIncorrect:\n* MIN (Transact-SQL)Returns the minimum value in the expression. May be followed by the OVER clause.\nSyntax\nMIN ( [ ALL | DISTINCT ] expression )\nReference:\nhttps://learn.microsoft.com/en-us/sql/t-sql/language-elements/c oalesce-transact-sql\nhttps://learn.microsoft.com/en-us/sql/t-sql/functions/logical-f unctions-least-transact-sql", "boxes": [{"box": 1, "value": "COALESCE"}, {"box": 2, "value": "LEAST"}], "image": "images/p123.jpg"}, {"id": 105, "shown": "52", "type": "hotspot", "stem": "You have a Fabric warehouse that contains a table named Sales.P roducts. Sales.Products contains the\nfollowing columns.\nYou need to write a T-SQL query that will return the following columns.\nHow should you complete the code? To answer, select the appropr iate options in the answer area.\nNOTE: Each correct answer is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: GREATEST\nLogical functions - GREATEST (Transact-SQL)This function returns the maximum value from a list of one or m ore expressions.\n Transact-SQL syntax conventionsSyntax\nGREATEST ( expression1 [ , ...expressionN ] )\nArguments\nexpression1, expressionNA list of comma-separated expressions of any comparable data ty pe. The GREATEST function requires at\nleast one argument and supports no more than 254 arguments.\nEach expression can be a constant, variable, column name or fun ction, and any combination of arithmetic,\nbitwise, and string operators. Aggregate functions and scalar s ubqueries are permitted.\nIncorrect:\n* MAX (Transact-SQL)Returns the maximum value in the expression.\nBox 2: COALESCE\nCOALESCE (Transact-SQL)Evaluates the arguments in order and returns the current value of the first expression that initially doesn't\nevaluate to NULL. For example, SELECT COALESCE(NULL, NULL, 'thi rd_value', 'fourth_value'); returns\nthe third value because the third value is the first value that isn't null.\nNote: The order of the arguments seems incorrect though. Should be COALESCE(AgentPrice,\nWholesalePrice, ListPrice)\nIncorrect:\n* CHOOSELogical Functions - CHOOSE (Transact-SQL)Returns the item at the specified index from a list of values i n SQL Server.\n Transact-SQL syntax conventionsSyntax\n\nCHOOSE ( index, val_1, val_2 [, val_n ] )\n*IIF\nLogical Functions - IIF (Transact-SQL)Returns one of two values, depending on whether the Boolean exp ression evaluates to true or false in SQL\nServer.\n Transact-SQL syntax conventionsSyntaxIIF( boolean_expression, true_value, false_value )", "reference": "Explanation:\nBox 1: GREATEST\nLogical functions - GREATEST (Transact-SQL)This function returns the maximum value from a list of one or m ore expressions.\n Transact-SQL syntax conventionsSyntax\nGREATEST ( expression1 [ , ...expressionN ] )\nArguments\nexpression1, expressionNA list of comma-separated expressions of any comparable data ty pe. The GREATEST function requires at\nleast one argument and supports no more than 254 arguments.\nEach expression can be a constant, variable, column name or fun ction, and any combination of arithmetic,\nbitwise, and string operators. Aggregate functions and scalar s ubqueries are permitted.\nIncorrect:\n* MAX (Transact-SQL)Returns the maximum value in the expression.\nBox 2: COALESCE\nCOALESCE (Transact-SQL)Evaluates the arguments in order and returns the current value of the first expression that initially doesn't\nevaluate to NULL. For example, SELECT COALESCE(NULL, NULL, 'thi rd_value', 'fourth_value'); returns\nthe third value because the third value is the first value that isn't null.\nNote: The order of the arguments seems incorrect though. Should be COALESCE(AgentPrice,\nWholesalePrice, ListPrice)\nIncorrect:\n* CHOOSELogical Functions - CHOOSE (Transact-SQL)Returns the item at the specified index from a list of values i n SQL Server.\n Transact-SQL syntax conventionsSyntax\n\nCHOOSE ( index, val_1, val_2 [, val_n ] )\n*IIF\nLogical Functions - IIF (Transact-SQL)Returns one of two values, depending on whether the Boolean exp ression evaluates to true or false in SQL\nServer.\n Transact-SQL syntax conventionsSyntaxIIF( boolean_expression, true_value, false_value )Reference:\nhttps://learn.microsoft.com/en-us/sql/t-sql/functions/logical-f unctions-greatest-transact-sql\nhttps://learn.microsoft.com/en-us/sql/t-sql/language-elements/c oalesce-transact-sql", "boxes": [{"box": 1, "value": "GREATEST"}, {"box": 2, "value": "COALESCE"}], "images": ["images/p124.jpg", "images/p125.jpg", "images/p126.jpg"]}, {"id": 106, "shown": "53", "type": "hotspot", "stem": "You have a Fabric warehouse that contains a table named Sales.O rders. Sales.Orders contains the\nfollowing columns.\nYou need to write a T-SQL query that will return the following columns.\nHow should you complete the code? To answer, select the appropr iate options in the answer area.\nNOTE: Each correct answer is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: DATETRUNC\nQuestion: PeriodDate: Returns a date representing the first day of the month for OrderDate\nDATETRUNC (Transact-SQL)\nThe DATETRUNC function returns an input date truncated to a spe cified datepart.\nSyntax\nDATETRUNC ( datepart, date )\nArguments\ndatepartSpecifies the precision for truncation. This table lists all th e valid datepart values for DATETRUNC, given\nthat it's also a valid part of the input date type.\n\nBox 2: weekday\nQuestion: DayName returns the name of the day for OrderDate, su ch as Wednesday\nNote: DATENAME (Transact-SQL)\nThis function returns a character string representing the speci fied datepart of the specified date.\nSyntax\nDATENAME ( datepart , date )\ndatepart\nThe specific part of the date argument that DATENAME will retur n. This table lists all valid datepart\narguments.\nweekday\nEtc.\nIncorrect:\n* DATE_BUCKET (Transact-SQL)This function returns the date-time value corresponding to the start of each date-time bucket from the\ntimestamp defined by the origin parameter, or the default origi n value of 1900-01-01 00:00:00.000 if the\norigin parameter isn't specified.\nSee Date and Time Data Types and Functions (Transact-SQL) for a n overview of all Transact-SQL date\nand time data types and functions.\nSyntax\nDATE_BUCKET (datepart, number, date [, origin ] )\n* DATEFROMPARTS\nThis function returns a date value that maps to the specified y ear, month, and day values.\nSyntax\nDATEFROMPARTS ( year, month, day )\n* DATEPART (Transact-SQL)\nThis function returns an integer representing the specified dat epart of the specified date.", "reference": "Explanation:\nBox 1: DATETRUNC\nQuestion: PeriodDate: Returns a date representing the first day of the month for OrderDate\nDATETRUNC (Transact-SQL)\nThe DATETRUNC function returns an input date truncated to a spe cified datepart.\nSyntax\nDATETRUNC ( datepart, date )\nArguments\ndatepartSpecifies the precision for truncation. This table lists all th e valid datepart values for DATETRUNC, given\nthat it's also a valid part of the input date type.\n\nBox 2: weekday\nQuestion: DayName returns the name of the day for OrderDate, su ch as Wednesday\nNote: DATENAME (Transact-SQL)\nThis function returns a character string representing the speci fied datepart of the specified date.\nSyntax\nDATENAME ( datepart , date )\ndatepart\nThe specific part of the date argument that DATENAME will retur n. This table lists all valid datepart\narguments.\nweekday\nEtc.\nIncorrect:\n* DATE_BUCKET (Transact-SQL)This function returns the date-time value corresponding to the start of each date-time bucket from the\ntimestamp defined by the origin parameter, or the default origi n value of 1900-01-01 00:00:00.000 if the\norigin parameter isn't specified.\nSee Date and Time Data Types and Functions (Transact-SQL) for a n overview of all Transact-SQL date\nand time data types and functions.\nSyntax\nDATE_BUCKET (datepart, number, date [, origin ] )\n* DATEFROMPARTS\nThis function returns a date value that maps to the specified y ear, month, and day values.\nSyntax\nDATEFROMPARTS ( year, month, day )\n* DATEPART (Transact-SQL)\nThis function returns an integer representing the specified dat epart of the specified date.\nReference:\nhttps://learn.microsoft.com/en-us/sql/t-sql/functions/datetrunc -transact-sql", "boxes": [{"box": 1, "value": "DATETRUNC"}, {"box": 2, "value": "weekday"}], "images": ["images/p127.jpg", "images/p128.jpg"]}, {"id": 107, "shown": "54", "type": "mc", "stem": "Note: This question is part of a series of questions that prese nt the same scenario. Each question\nin the series contains a unique solution that might meet the st ated goals. Some question sets\nmight have more than one correct solution, while others might n ot have a correct solution.\nAfter you answer a question in this section, you will NOT be ab le to return to it. As a result, these\nquestions will not appear in the review screen.\nYou have a Fabric tenant that contains a lakehouse named Lakeho use1. Lakehouse1 contains a Delta\ntable named Customer.\nWhen you query Customer, you discover that the query is slow to  execute. You suspect that maintenance\nwas NOT performed on the table.\nYou need to identify whether maintenance tasks were performed o n Customer.\nSolution: You run the following Spark SQL statement:\nDESCRIBE DETAIL customer\nDoes this meet the goal?", "options": [{"key": "A", "text": "Yes"}, {"key": "B", "text": "No"}], "correct": "B", "multi": false, "explanation": "Correct Solution: You run the following Spark SQL statement:\nDESCRIBE HISTORY customer\nDESCRIBE HISTORY\nApplies to: Databricks SQL, Databricks Runtime\nReturns provenance information, including the operation, user, and so on, for each write to a table. Table\nhistory is retained for 30 days.\nSyntax\nDESCRIBE HISTORY table_name\nNote: Work with Delta Lake table history\nEach operation that modifies a Delta Lake table creates a new t able version. You can use history\ninformation to audit operations, rollback a table, or query a t able at a specific point in time using time travel.\nRetrieve Delta table history\nYou can retrieve information including the operations, user, an d timestamp for each write to a Delta table\nby running the history command. The operations are returned in reverse chronological order.\nDESCRIBE HISTORY '/data/events/' -- get the full histo ry of the table\nDESCRIBE HISTORY delta.`/data/events/`DESCRIBE HISTORY '/data/events/' LIMIT 1 -- get the last opera tion only\nDESCRIBE HISTORY eventsTableIncorrect:\n* DESCRIBE DETAIL customer\nDESCRIBE TABLE statement returns the basic metadata information of a table. The metadata information\nincludes column name, column type and column comment. Optionall y a partition spec or column name\nmay be specified to return the metadata pertaining to a partiti on or column respectively.\n* EXPLAIN TABLE customer\n* REFRESH TABLE\nREFRESH TABLE statement invalidates the cached entries, which i nclude data and metadata of the given\ntable or view. The invalidated cache is populated in lazy manne r when the cached table or the query\nassociated with it is executed again.\nSyntax\nREFRESH [TABLE] tableIdentifier", "reference": "Explanation:\nCorrect Solution: You run the following Spark SQL statement:\nDESCRIBE HISTORY customer\nDESCRIBE HISTORY\nApplies to: Databricks SQL, Databricks Runtime\nReturns provenance information, including the operation, user, and so on, for each write to a table. Table\nhistory is retained for 30 days.\nSyntax\nDESCRIBE HISTORY table_name\nNote: Work with Delta Lake table history\nEach operation that modifies a Delta Lake table creates a new t able version. You can use history\ninformation to audit operations, rollback a table, or query a t able at a specific point in time using time travel.\nRetrieve Delta table history\nYou can retrieve information including the operations, user, an d timestamp for each write to a Delta table\nby running the history command. The operations are returned in reverse chronological order.\nDESCRIBE HISTORY '/data/events/' -- get the full histo ry of the table\nDESCRIBE HISTORY delta.`/data/events/`DESCRIBE HISTORY '/data/events/' LIMIT 1 -- get the last opera tion only\nDESCRIBE HISTORY eventsTableIncorrect:\n* DESCRIBE DETAIL customer\nDESCRIBE TABLE statement returns the basic metadata information of a table. The metadata information\nincludes column name, column type and column comment. Optionall y a partition spec or column name\nmay be specified to return the metadata pertaining to a partiti on or column respectively.\n* EXPLAIN TABLE customer\n* REFRESH TABLE\nREFRESH TABLE statement invalidates the cached entries, which i nclude data and metadata of the given\ntable or view. The invalidated cache is populated in lazy manne r when the cached table or the query\nassociated with it is executed again.\nSyntax\nREFRESH [TABLE] tableIdentifier\nReference:\nhttps://learn.microsoft.com/en-us/azure/databricks/sql/language -manual/delta-describe-history\nhttps://docs.gcp.databricks.com/en/delta/history.html\nhttps://spark.apache.org/docs/3.0.0-preview/sql-ref-syntax-aux- refresh-table.html", "boxes": []}, {"id": 108, "shown": "55", "type": "dragdrop", "stem": "You have a Fabric tenant that contains a data warehouse named D W1. DW1 contains a table named\nDimCustomer. DimCustomer contains the fields shown in the follo wing table.\nYou need to identify duplicate email addresses in DimCustomer. The solution must return a maximum of\n1,000 records.\nWhich four T-SQL statements should you run in sequence? To answ er, move the appropriate statements\nfrom the list of statements to the answer area and arrange them  in the correct order.\nSelect and Place:", "options": [], "correct": "", "multi": false, "explanation": "Step 1: SELECT TOP(1000) CustomerAltKey, Count(*)Use TOP(1000) to return maximum 1000 records.Step 2: FROM DimCustomerSQL HAVING Example:\nThe following SQL statement lists the number of customers in ea ch country. Only include countries with\nmore than 5 customers:\nSELECT COUNT(CustomerID), Country\nFROM CustomersGROUP BY CountryHAVING COUNT(CustomerID) > 5;\nStep 3: GROUP BY CustomerAltKeyStep 4: HAVING COUNT(*) > 1\nThe SQL HAVING ClauseThe HAVING clause was added to SQL because the WHERE keyword ca nnot be used with aggregate\nfunctions.", "reference": "Explanation:\nStep 1: SELECT TOP(1000) CustomerAltKey, Count(*)Use TOP(1000) to return maximum 1000 records.Step 2: FROM DimCustomerSQL HAVING Example:\nThe following SQL statement lists the number of customers in ea ch country. Only include countries with\nmore than 5 customers:\nSELECT COUNT(CustomerID), Country\nFROM CustomersGROUP BY CountryHAVING COUNT(CustomerID) > 5;\nStep 3: GROUP BY CustomerAltKeyStep 4: HAVING COUNT(*) > 1\nThe SQL HAVING ClauseThe HAVING clause was added to SQL because the WHERE keyword ca nnot be used with aggregate\nfunctions.\nReference:\nhttps://www.w3schools.com/SQL/sql_having.asp", "boxes": [], "image": "images/p131.jpg"}, {"id": 109, "shown": "56", "type": "hotspot", "stem": "You have a Fabric tenant that contains a warehouse named WH1.You run the following T-SQL query against WH1.\nFor each of the following statements, select Yes if the stateme nt is true. Otherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: Yes\nYes - Dimension.GetDirectReports is a scalar T-SQL function.\nOUTER APPLY\nThe OUTER APPLY operator returns all the rows from the left tab le expression irrespective of whether it\nmatches the expression from the right table. For rows with no c orresponding matches in the right table\nexpression, it contains NULL values in columns of the right tab le expression. The OUTER APPLY is\nequivalent to a LEFT OUTER JOIN.\nIf you can achieve the same results with a regular JOIN clause, why and when do you use the APPLY\noperator? Although you can achieve the same with a regular JOIN , the need for APPLY arises if you have\na table-valued expression on the right part.\nSQL Server OUTER APPLY vs LEFT OUTER JOIN\nLet's take a minute and look at another example. The first quer y in the code block below, selects data from\nthe Department table. It uses an OUTER APPLY to evaluate the Em ployee table for each record of the\nDepartment table. For those rows for which there is no match in the Employee table, SQL returns NULL,\nas you can see in the screenshots below.\nThe second query uses a LEFT OUTER JOIN between the Department and Employee tables. As\nexpected, the query returns all rows from the Department table, even for those rows for which there is no\nmatch in the Employee table.\n-- Query #1\nSELECT *FROM Department D OUTER APPLY (SELECT * FROM Employee E WHERE E.DepartmentID = D.Departmen tID) A;\nGO\n-- Query #2\nSELECT *FROM Department D LEFT OUTER JOIN Employee E ON D.DepartmentID = E.DepartmentID;\n\nBox 2: No\nIt runs for each record/row.\nBox 3: Yes", "reference": "Explanation:\nBox 1: Yes\nYes - Dimension.GetDirectReports is a scalar T-SQL function.\nOUTER APPLY\nThe OUTER APPLY operator returns all the rows from the left tab le expression irrespective of whether it\nmatches the expression from the right table. For rows with no c orresponding matches in the right table\nexpression, it contains NULL values in columns of the right tab le expression. The OUTER APPLY is\nequivalent to a LEFT OUTER JOIN.\nIf you can achieve the same results with a regular JOIN clause, why and when do you use the APPLY\noperator? Although you can achieve the same with a regular JOIN , the need for APPLY arises if you have\na table-valued expression on the right part.\nSQL Server OUTER APPLY vs LEFT OUTER JOIN\nLet's take a minute and look at another example. The first quer y in the code block below, selects data from\nthe Department table. It uses an OUTER APPLY to evaluate the Em ployee table for each record of the\nDepartment table. For those rows for which there is no match in the Employee table, SQL returns NULL,\nas you can see in the screenshots below.\nThe second query uses a LEFT OUTER JOIN between the Department and Employee tables. As\nexpected, the query returns all rows from the Department table, even for those rows for which there is no\nmatch in the Employee table.\n-- Query #1\nSELECT *FROM Department D OUTER APPLY (SELECT * FROM Employee E WHERE E.DepartmentID = D.Departmen tID) A;\nGO\n-- Query #2\nSELECT *FROM Department D LEFT OUTER JOIN Employee E ON D.DepartmentID = E.DepartmentID;\n\nBox 2: No\nIt runs for each record/row.\nBox 3: YesReference:\nhttps://www.mssqltips.com/sqlservertip/1958/sql-server-cross-ap ply-and-outer-apply", "boxes": [{"box": 1, "value": "Yes"}, {"box": 2, "value": "No"}, {"box": 3, "value": "YesReference:"}], "images": ["images/p132.jpg", "images/p133.jpg"]}, {"id": 110, "shown": "57", "type": "hotspot", "stem": "You have the following KQL query.\nFor each of the following statements, select Yes if the stateme nt is true. Otherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "where Status != \"Cancelled\" : Excludes records where the Status is \"Cancelled\".\nwhere OrderDate >= ago(30d) : Filters for records where the OrderDate is within the last 30 days.\nsummarize TotalSales = sum(SalesAmount) by ProductCategory : Calculates the total sales\n(SalesAmount ) for each product category.\nwhere TotalSales > 0 : Filters out product categories where the total sales are zero or less.\nThe query excludes sales that have a Status of Cancelled - Yes\nThe where Status != \"Cancelled\" condition ensures that rows with a \"Cancelled\" status are\nexcluded.\nThe query calculates the total sales of each product category f or the last 30 days - Yes\nThe combination of where OrderDate >= ago(30d) and summarize TotalSales = sum\n(SalesAmount) by ProductCategory calculates the total sales for each product category for the l ast\n30 days.\nThe query includes product categories that have had zero sales during the last 30 days - No\nThe where TotalSales > 0 condition filters out product categories with zero sales.", "reference": "Explanation:\nwhere Status != \"Cancelled\" : Excludes records where the Status is \"Cancelled\".\nwhere OrderDate >= ago(30d) : Filters for records where the OrderDate is within the last 30 days.\nsummarize TotalSales = sum(SalesAmount) by ProductCategory : Calculates the total sales\n(SalesAmount ) for each product category.\nwhere TotalSales > 0 : Filters out product categories where the total sales are zero or less.\nThe query excludes sales that have a Status of Cancelled - Yes\nThe where Status != \"Cancelled\" condition ensures that rows with a \"Cancelled\" status are\nexcluded.\nThe query calculates the total sales of each product category f or the last 30 days - Yes\nThe combination of where OrderDate >= ago(30d) and summarize TotalSales = sum\n(SalesAmount) by ProductCategory calculates the total sales for each product category for the l ast\n30 days.\nThe query includes product categories that have had zero sales during the last 30 days - No\nThe where TotalSales > 0 condition filters out product categories with zero sales.", "boxes": [], "image": "images/p134.jpg"}, {"id": 111, "shown": "58", "type": "mc", "stem": "You have a Fabric warehouse that contains a table named SalesOr derDetail, SalesOrderDetail contains\nthree columns named OrderQty, ProductID and SalesOrderlD. Sales OrderDetail contains one row per\ncombination of SalesOrderlD and ProductID.\nYou need to calculate the proportion of the total quantity of e ach sales order represented by each product\nwithin the sales order.\nWhich T-SQL statement should you run?", "options": [{"key": "A", "text": ""}, {"key": "B", "text": ""}, {"key": "C", "text": ""}, {"key": "D", "text": ""}], "correct": "D", "multi": false, "explanation": "The goal is to calculate the proportion of the total quantity f or each sales order ( SalesOrderID )\nrepresented by each product ( ProductID ) within that sales order. To achieve this, you need to:\n1. Use a PARTITION BY SalesOrderID clause to calculate the total quantity ( SUM(OrderQty) ) per\nsales order.\n2. Divide the quantity of each product ( OrderQty ) by the total quantity of the sales order.\n3. Multiply by 100 to calculate the percentage.\n4. Use the CAST function to format the result to a specific precision (e.g., DECIMAL(5,2) ).", "reference": "Explanation:\nThe goal is to calculate the proportion of the total quantity f or each sales order ( SalesOrderID )\nrepresented by each product ( ProductID ) within that sales order. To achieve this, you need to:\n1. Use a PARTITION BY SalesOrderID clause to calculate the total quantity ( SUM(OrderQty) ) per\nsales order.\n2. Divide the quantity of each product ( OrderQty ) by the total quantity of the sales order.\n3. Multiply by 100 to calculate the percentage.\n4. Use the CAST function to format the result to a specific precision (e.g., DECIMAL(5,2) ).", "boxes": [], "image": "images/p135.jpg"}, {"id": 112, "shown": "59", "type": "hotspot", "stem": "You have a Fabric lakehouse named Lakehouse1 that contains the following data.\nYou need build a T-SQL statement that will return the total sal es amount by OrderDate only for the days\nthat are holidays in Australia. The total sales amount must sum  the quantity multiplied by the price on each\nrow in the dbo.sales table.\nHow should you complete the statement? To answer, select the ap propriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: Sum(s.Quantity * s.UnitPrice)\nCalculate the sum of Quantity * Unitprice.\nIncorrect:\n* s.Quantity * S.UnitPriceNeed to use the SUM function.\n* Sum (s.Quantity) * S.UnitPrice)\nIncorrect parenthesis.\n* Sum (s.Quantity) * S.UnitPrice\nNot the sum of the Quantity.\n* Sum (s.Quantity) * SUM(S.UnitPrice)\nNot separate sums of the Quantity and the UnitPrice.\n\nBox 2: Inner\nStandard inner join", "reference": "Explanation:\nBox 1: Sum(s.Quantity * s.UnitPrice)\nCalculate the sum of Quantity * Unitprice.\nIncorrect:\n* s.Quantity * S.UnitPriceNeed to use the SUM function.\n* Sum (s.Quantity) * S.UnitPrice)\nIncorrect parenthesis.\n* Sum (s.Quantity) * S.UnitPrice\nNot the sum of the Quantity.\n* Sum (s.Quantity) * SUM(S.UnitPrice)\nNot separate sums of the Quantity and the UnitPrice.\n\nBox 2: Inner\nStandard inner join", "boxes": [{"box": 1, "value": "Sum(s.Quantity * s.UnitPrice)"}, {"box": 2, "value": "Inner"}], "images": ["images/p136.jpg", "images/p137.jpg"]}, {"id": 113, "shown": "60", "type": "mc", "stem": "You need to create a Microsoft Power BI file that will be used to create multiple reports. The solution must\nmeet the following requirements:\nThe file must include predefined data source connections.\nThe file must include the report structure and formatting.\nThe file must NOT contain any data.\nWhich file format should you use?", "options": [{"key": "A", "text": "PBIT"}, {"key": "B", "text": "PBIDS"}, {"key": "C", "text": "PBIX"}, {"key": "D", "text": "PBIP"}], "correct": "B", "multi": false, "explanation": "Use PBIDS files to get dataPBIDS files are Power BI Desktop files that have a specific str ucture and a .pbids extension to identify\nthem as Power BI data source files.\nYou can create a PBIDS file to streamline the Get Data experien ce for new or beginner report creators in\nyour organization. If you create the PBIDS file from existing r eports, it's easier for beginning report authors\nto build new reports from the same data.\nNote: How to create a PBIDS connection file\nIf you have an existing Power BI Desktop PBIX file already conn ected to the data you’re interested in, you\ncan export the connection files from within Power BI Desktop. T his method is recommended, since the\nPBIDS file can be autogenerated from Desktop. You can also stil l edit or manually create the file in a text\neditor.\n1. To create the PBIDS file, select File > Options and settings > Data source settings.\n2. In the dialog that appears, select the data source you want to export as a PBIDS file, and then select\nExport PBIDS.\n3. In the Save As dialog box, give the file a name, and select Save. Power BI Desktop generates the\nPBIDS file, which you can rename and save in your directory, an d share with others.\nIncorrect:\nNot A: PBIT is used for sharing report structures without data.\nNot C: PBIX is the default format for saving reports in Power B I Desktop.", "reference": "Explanation:\nUse PBIDS files to get dataPBIDS files are Power BI Desktop files that have a specific str ucture and a .pbids extension to identify\nthem as Power BI data source files.\nYou can create a PBIDS file to streamline the Get Data experien ce for new or beginner report creators in\nyour organization. If you create the PBIDS file from existing r eports, it's easier for beginning report authors\nto build new reports from the same data.\nNote: How to create a PBIDS connection file\nIf you have an existing Power BI Desktop PBIX file already conn ected to the data you’re interested in, you\ncan export the connection files from within Power BI Desktop. T his method is recommended, since the\nPBIDS file can be autogenerated from Desktop. You can also stil l edit or manually create the file in a text\neditor.\n1. To create the PBIDS file, select File > Options and settings > Data source settings.\n2. In the dialog that appears, select the data source you want to export as a PBIDS file, and then select\nExport PBIDS.\n3. In the Save As dialog box, give the file a name, and select Save. Power BI Desktop generates the\nPBIDS file, which you can rename and save in your directory, an d share with others.\nIncorrect:\nNot A: PBIT is used for sharing report structures without data.\nNot C: PBIX is the default format for saving reports in Power B I Desktop.\nReference:\nhttps://learn.microsoft.com/en-us/power-bi/connect-data/desktop -data-sources", "boxes": [], "image": "images/p138.jpg"}, {"id": 114, "shown": "61", "type": "hotspot", "stem": "You have a Fabric warehouse that contains two tables named DimD ate and Trips.\nDimDate contains the following fields.\nTrips contains the following fields.\nYou need to compare the average miles per trip for statutory ho lidays versus non-statutory holidays.\nHow should you complete the T-SQL statement? To answer, select the appropriate options in the answer\narea.\nNOTE: Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: (Sum(t.tripDistance)/count(t.tripID))\naverage miles per triptotal miles: Sum(t.tripDistance)number of trips: count(t.tripID)\nBox 2: group by\nGroup by on the Holiday column to get results for both statutor y holidays and non-statutory holidays.", "reference": "Explanation:\nBox 1: (Sum(t.tripDistance)/count(t.tripID))\naverage miles per triptotal miles: Sum(t.tripDistance)number of trips: count(t.tripID)\nBox 2: group by\nGroup by on the Holiday column to get results for both statutor y holidays and non-statutory holidays.", "boxes": [{"box": 1, "value": "(Sum(t.tripDistance)/count(t.tripID))"}, {"box": 2, "value": "group by"}], "images": ["images/p138.jpg", "images/p139.jpg"]}, {"id": 115, "shown": "62", "type": "hotspot", "stem": "You have a Fabric tenant that contains a workspace named Worksp ace1. Workspace1 contains a\nlakehouse named LH1 and a warehouse named DW1. LH1 contains a t able named signindata that is in\nthe dbo schema.\nYou need to create a stored procedure in DW1 that deduplicates the data in the signindata table.\nHow should you complete the T-SQL statement? To answer, select the appropriate options in the answer\narea.\nNOTE: Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: BEGIN\nMatching the END clause.\n\nBox 2: DISTINCT\nDistinct SQL : How to Eliminate Duplicate DataThe SQL Distinct Keyword serves as the bedrock for eradicating duplicate records in SQL databases.\nWhen employed in a query, it acts as a filter that ensures the result set contains only unique records.\nSELECT DISTINCT column_name\nFROM table_name;", "reference": "Explanation:\nBox 1: BEGIN\nMatching the END clause.\n\nBox 2: DISTINCT\nDistinct SQL : How to Eliminate Duplicate DataThe SQL Distinct Keyword serves as the bedrock for eradicating duplicate records in SQL databases.\nWhen employed in a query, it acts as a filter that ensures the result set contains only unique records.\nSELECT DISTINCT column_name\nFROM table_name;\nReference:\nhttps://www.ituonline.com/blogs/distinct-sql/\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-p rocedure-transact-sql", "boxes": [{"box": 1, "value": "BEGIN"}, {"box": 2, "value": "DISTINCT"}], "images": ["images/p140.jpg", "images/p141.jpg"]}, {"id": 116, "shown": "63", "type": "hotspot", "stem": "You have the following T-SQL statement.\nFor each of the following statements, select Yes if the stateme nt is true. Otherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: Yes\nYes - The statement returns Region values when a Sales item has a RefundStatus of Refunded.\nAs per code: WHEN RefundStatus <> 'Refunded' SalesAmount ELSE 0 END\nIt returns Region values for Sales item regardless what the Ref undStatus is.\nBox 2: Yes\nYes - The statement only returns TransactionDate values that oc curred during the current year.\nAs per code: WHERE YEAR(TransactionDate) = YEAR(GETDATE())Box 3: No\nNo: The TotalRevenue calculation aggregates SalesAmount values that have a RefundStatus of Refunded.\nAs per code: WHEN RefundStatus <> 'Refunded' SalesAmount ELSE 0 END\nThe TotalRevenue calculation aggregates SalesAmount values that have a RefundStatus other than\nRefunded.", "reference": "Explanation:\nBox 1: Yes\nYes - The statement returns Region values when a Sales item has a RefundStatus of Refunded.\nAs per code: WHEN RefundStatus <> 'Refunded' SalesAmount ELSE 0 END\nIt returns Region values for Sales item regardless what the Ref undStatus is.\nBox 2: Yes\nYes - The statement only returns TransactionDate values that oc curred during the current year.\nAs per code: WHERE YEAR(TransactionDate) = YEAR(GETDATE())Box 3: No\nNo: The TotalRevenue calculation aggregates SalesAmount values that have a RefundStatus of Refunded.\nAs per code: WHEN RefundStatus <> 'Refunded' SalesAmount ELSE 0 END\nThe TotalRevenue calculation aggregates SalesAmount values that have a RefundStatus other than\nRefunded.", "boxes": [{"box": 1, "value": "Yes"}, {"box": 2, "value": "Yes"}, {"box": 3, "value": "No"}], "image": "images/p142.jpg"}, {"id": 117, "shown": "64", "type": "hotspot", "stem": "You have a Fabric warehouse that contains the following data.\nThe data has the following characteristics:\nEach customer is assigned a unique CustomerID value.\nEach customer is associated to a single SalesRegion value.\nEach customer is associated to a single CustomerAddress value.\nThe Customer table contains 5 million rows.\nAll foreign key values are non-null.\nYou need to create a view to denormalize the data into a custom er dimension that contains one row per\ndistinct CustomerID value. The solution must minimize query pro cessing time and resources.\nHow should you complete the T-SQL statement? To answer, select the appropriate options in the answer\narea.\nNOTE: Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: left outer joinA join between the Customer table and the SalesRegion table.\nWe denormalize with a left out join.Incorrect:\n* Inner join\nBox 2: A.AdressID = CA.AddressID\nThe Address Table, abbreviated A, and the CustomerAddress table , abbreviated CA, both have AddressID\ncolumns.", "reference": "Explanation:\n Box 1: left outer joinA join between the Customer table and the SalesRegion table.\nWe denormalize with a left out join.Incorrect:\n* Inner join\nBox 2: A.AdressID = CA.AddressID\nThe Address Table, abbreviated A, and the CustomerAddress table , abbreviated CA, both have AddressID\ncolumns.", "boxes": [{"box": 1, "value": "left outer joinA join between the Customer table and the SalesRegion table."}, {"box": 2, "value": "A.AdressID = CA.AddressID"}], "images": ["images/p143.jpg", "images/p144.jpg"]}, {"id": 118, "shown": "65", "type": "mc", "stem": "You have a query in Microsoft Power BI Desktop that contains tw o columns named Order_Date and\nShipping_Date.\nYou need to create a column that will calculate the number of d ays between Order_Date and\nShipping_Date for each row.\nWhich Power Query function should you use?", "options": [{"key": "A", "text": "DateTime.LocalNow"}, {"key": "B", "text": "Duration.Days"}, {"key": "C", "text": "Duration.From"}, {"key": "D", "text": "Date.AddDays"}], "correct": "B", "multi": false, "explanation": "Power Query M, Duration.Days\nSyntax\nDuration.Days(duration as nullable duration) as nullable number\nAbout\nReturns the days portion of duration.\nExample 1\nExtract the number of days between two dates.\nUsage\nDuration.Days(#date(2022, 3, 4) - #date(2022, 2, 25))\nOutput\n7", "reference": "Explanation:\nPower Query M, Duration.Days\nSyntax\nDuration.Days(duration as nullable duration) as nullable number\nAbout\nReturns the days portion of duration.\nExample 1\nExtract the number of days between two dates.\nUsage\nDuration.Days(#date(2022, 3, 4) - #date(2022, 2, 25))\nOutput\n7\nReference:\nhttps://learn.microsoft.com/en-us/powerquery-m/duration-days", "boxes": [], "image": "images/p145.jpg"}, {"id": 119, "shown": "66", "type": "mc", "stem": "You plan to use Fabric to store data.\nYou need to create a data store that supports the following:\nWriting data by using T-SQL\nMulti-table transactions\nDynamic data masking\nWhich type of data store should you create?", "options": [{"key": "A", "text": "KQL database"}, {"key": "B", "text": "lakehouse"}, {"key": "C", "text": "warehouse"}, {"key": "D", "text": "semantic model"}], "correct": "C", "multi": false, "explanation": "You can use dynamic data masking in Fabric data warehousing.\nTransactions in Warehouse tables in Microsoft Fabric\nWarehouse in Microsoft Fabric supports transactions that span a cross databases that are within the same\nworkspace including reading from the SQL analytics endpoint of the Lakehouse.", "reference": "Explanation:\nYou can use dynamic data masking in Fabric data warehousing.\nTransactions in Warehouse tables in Microsoft Fabric\nWarehouse in Microsoft Fabric supports transactions that span a cross databases that are within the same\nworkspace including reading from the SQL analytics endpoint of the Lakehouse.\nReference:\nhttps://learn.microsoft.com/en-us/fabric/data-warehouse/dynamic -data-masking\nhttps://learn.microsoft.com/en-us/fabric/data-warehouse/transac tions", "boxes": [], "image": "images/p146.jpg"}, {"id": 120, "shown": "67", "type": "hotspot", "stem": "You have two Microsoft Power BI queries named Employee and Reti red Roles.\nYou need to merge the Employee query with the Retired Roles que ry. The solution must ensure that\nduplicate rows in each query are removed.\nWhich column and Join Kind should you use in Power Query Editor ? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct answer is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: Division\nPower Query, Merge queries overview\nThe Division column and the Role column appear in both tables.Box 2: Inner JoinInner join as duplicate rows in each query must be removed.", "reference": "Explanation:\nBox 1: Division\nPower Query, Merge queries overview\nThe Division column and the Role column appear in both tables.Box 2: Inner JoinInner join as duplicate rows in each query must be removed.Reference:\nhttps://learn.microsoft.com/en-us/power-query/merge-queries-ove rview", "boxes": [{"box": 1, "value": "Division"}, {"box": 2, "value": "Inner JoinInner join as duplicate rows in each query must be removed.Reference:"}], "image": "images/p147.jpg"}, {"id": 121, "shown": "68", "type": "mc", "stem": "You have a Fabric tenant named Tenant1 that contains a lakehous e named Lakehouse1.\nYou need to add data to Lakehouse1 from a CSV file in an Azure Storage account outside of Fabric. The\nsolution must minimize development effort.\nWhat should you use to add the data?", "options": [{"key": "A", "text": "copy job"}, {"key": "B", "text": "shortcut"}, {"key": "C", "text": "pipeline"}, {"key": "D", "text": "Dataflow Gen2"}], "correct": "B", "multi": false, "explanation": "A shortcut in Microsoft Fabric allows you to reference data stored in an external Azure Storage account\n(such as ADLS Gen2 or Azure Blob Storage) without physically co pying or duplicating the data. This\nminimizes development effort because it eliminates the need for data ingestion, transformation, and\nstorage replication.\nWhen using a shortcut, Fabric users can directly access the external data in its original location while\ntreating it as part of their Lakehouse. This provides a seamles s integration without the need for extra data\nprocessing or movement.", "reference": "Explanation:\nA shortcut in Microsoft Fabric allows you to reference data stored in an external Azure Storage account\n(such as ADLS Gen2 or Azure Blob Storage) without physically co pying or duplicating the data. This\nminimizes development effort because it eliminates the need for data ingestion, transformation, and\nstorage replication.\nWhen using a shortcut, Fabric users can directly access the external data in its original location while\ntreating it as part of their Lakehouse. This provides a seamles s integration without the need for extra data\nprocessing or movement.", "boxes": [], "image": "images/p148.jpg"}, {"id": 122, "shown": "69", "type": "hotspot", "stem": "You have a Fabric warehouse that contains a table named Table1.  Table1 contains the following data.\nYou need to create a T-SQL statement that meets the following r equirements:\nOutputs the item name of each item and returns a null value if the item name is longer than 20\ncharacters\nOutputs the PurchaseDate value in the format of МММ dd, yy\nHow should you complete the statement? To answer, select the ap propriate options in the answer area.\nNOTE : Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Handling Item Name (Null if > 20 characters)\nThe correct choice is TRY_CAST(item_name AS VARCHAR(20)) .\nTRY_CAST attempts to cast item_name to VARCHAR(20) .\nIf item_name is longer than 20 characters, it returns NULL instead of truncating or causing an error.\nFormatting PurchaseDate (MMM dd, yy)\n\nThe correct choice is CONVERT(VARCHAR, purchase_date, 7) .\nThe format style 7 in SQL Server returns dates in the MMM dd, yy format (e.g., \"Feb 20, 25\").", "reference": "Explanation:\nHandling Item Name (Null if > 20 characters)\nThe correct choice is TRY_CAST(item_name AS VARCHAR(20)) .\nTRY_CAST attempts to cast item_name to VARCHAR(20) .\nIf item_name is longer than 20 characters, it returns NULL instead of truncating or causing an error.\nFormatting PurchaseDate (MMM dd, yy)\n\nThe correct choice is CONVERT(VARCHAR, purchase_date, 7) .\nThe format style 7 in SQL Server returns dates in the MMM dd, yy format (e.g., \"Feb 20, 25\").", "boxes": [], "images": ["images/p149.jpg", "images/p150.jpg", "images/p151.jpg"]}, {"id": 123, "shown": "70", "type": "dragdrop", "stem": "You have a Fabric warehouse named Warehouse1 that contains a ta ble named dbo.Product.\ndbo.Product contains the following columns.\nYou need to use a T-SQL query to add a column named PriceRange to dbo.Product. The column must\ncategorize each product based on UnitPrice. The solution must m eet the following requirements:\nIf UnitPrice is 0, PriceRange is \"Not for resale\".\nIf UnitPrice is less than 50, PriceRange is \"Under $50\".\nIf UnitPrice is between 50 and 250, PriceRange is \"Under $250“.\nIn all other instances, PriceRange is \"$250+\".\nHow should you complete the query? To answer, drag the appropri ate values to the correct targets. Each\nvalue may be used once, more than once, or not at all. You may need to drag the split bar between panes\nor scroll to view content.\nNOTE : Each correct selection is worth one point.\nSelect and Place:", "options": [], "correct": "", "multi": false, "explanation": "Using CASE for Conditional Logic:\nThe CASE statement is used to categorize values based on condit ions.\nIt allows us to define multiple conditions for PriceRange based on UnitPrice.\nDefining Conditions in Order of Priority:\nUnitPrice = 0 → 'Not for resale' (Ensuring products not for sale are labeled correctly)\nUnitPrice < 50 → 'Under $50' (Categorizi ng low-priced products)\nUnitPrice between 50 and 250 → 'Under $250' (Using >= 50 AND < 250 ensures the correct rang e)\nAll other prices → '$250+' (Handled by the ELSE clause)", "reference": "Explanation:\nUsing CASE for Conditional Logic:\nThe CASE statement is used to categorize values based on condit ions.\nIt allows us to define multiple conditions for PriceRange based on UnitPrice.\nDefining Conditions in Order of Priority:\nUnitPrice = 0 → 'Not for resale' (Ensuring products not for sale are labeled correctly)\nUnitPrice < 50 → 'Under $50' (Categorizi ng low-priced products)\nUnitPrice between 50 and 250 → 'Under $250' (Using >= 50 AND < 250 ensures the correct rang e)\nAll other prices → '$250+' (Handled by the ELSE clause)", "boxes": [], "image": "images/p152.jpg"}, {"id": 124, "shown": "71", "type": "mc", "stem": "You have a Fabric tenant.\nYou are creating a Fabric Data Factory pipeline.You have a stored procedure that returns the number of active c ustomers and their average sales for the\ncurrent month.\nYou need to add an activity that will execute the stored proced ure in a warehouse. The returned values\nmust be available to the downstream activities of the pipeline.\nWhich type of activity should you add?", "options": [{"key": "A", "text": "Append variable"}, {"key": "B", "text": "Script"}, {"key": "C", "text": "Stored procedure"}, {"key": "D", "text": "Get metadata"}], "correct": "C", "multi": false, "explanation": "The Stored procedure activity in Fabric Data Factory is used to execute a stored procedure in a\nwarehouse and capture the returned values. Since the requiremen t specifies that the output (active\ncustomers and average sales) must be available for downstream a ctivities, this activity ensures the values\nare accessible for subsequent steps in the pipeline.", "reference": "Explanation:\nThe Stored procedure activity in Fabric Data Factory is used to execute a stored procedure in a\nwarehouse and capture the returned values. Since the requiremen t specifies that the output (active\ncustomers and average sales) must be available for downstream a ctivities, this activity ensures the values\nare accessible for subsequent steps in the pipeline.", "boxes": [], "image": "images/p153.jpg"}, {"id": 125, "shown": "72", "type": "hotspot", "stem": "You have a Fabric workspace that contains a warehouse named War ehouse1. Warehouse1 contains the\nfollowing data.\nYou need to create a T-SQL statement that will denormalize the tables and include the ContractType and\nStartDate attributes in the results. The solution must meet the  following requirements:\nInclude attributes from matching rows in the Contract table.\nEnsure that all the rows from the Employee table are preserved.\nReturn the total number of employees per contract type for all the contract types that have more than\ntwo employees.\nHow should you complete the statement? To answer, select the ap propriate options in the answer area.\nNOTE : Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "LEFT OUTER JOIN\nEnsures that all employees are included even if they do not have a matching contract record in the\nContract table. This satisfies the requirement that all Employee table rows must be preserved .\nHAVING COUNT(DISTINCT EmployeeID) > 2\nFilters out contract types that have fewer than or equal to two employees , ensuring that only contract\ntypes with more than two employees are included in the results.", "reference": "Explanation:\n\nLEFT OUTER JOIN\nEnsures that all employees are included even if they do not have a matching contract record in the\nContract table. This satisfies the requirement that all Employee table rows must be preserved .\nHAVING COUNT(DISTINCT EmployeeID) > 2\nFilters out contract types that have fewer than or equal to two employees , ensuring that only contract\ntypes with more than two employees are included in the results.", "boxes": [], "images": ["images/p154.jpg", "images/p155.jpg", "images/p156.jpg"]}, {"id": 126, "shown": "73", "type": "hotspot", "stem": "You have a KQL database that contains a table named Readings.You need to query Readings and return the results shown in the following table.\nHow should you complete the query? To answer, select the approp riate options in the answer area.\nNOTE : Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "| extend\nThe prev() function retrieves the previous row's value based on the sorted order .\nextend PrevMeterReading = prev(MeterReading), PrevDatetime = prev(Datetime)\n\nassigns these previous values.\n| project\nUsed to select specific columns for the final result set. Ensures that City, Area, MeterReading,\nDatetime, PrevMeterReading, and PrevDatetime are included in the output.", "reference": "Explanation:\n| extend\nThe prev() function retrieves the previous row's value based on the sorted order .\nextend PrevMeterReading = prev(MeterReading), PrevDatetime = prev(Datetime)\n\nassigns these previous values.\n| project\nUsed to select specific columns for the final result set. Ensures that City, Area, MeterReading,\nDatetime, PrevMeterReading, and PrevDatetime are included in the output.", "boxes": [], "images": ["images/p157.jpg", "images/p158.jpg"]}, {"id": 127, "shown": "74", "type": "hotspot", "stem": "You have a Fabric workspace that contains a warehouse named DW1 . DW1 contains the following tables\nand columns.\nYou need to summarize order quantities by year and product. The  solution must include the yearly sum of\norder quantities for all the products in each row.\nHow should you complete the T-SQL statement? To answer, select the appropriate options in the answer\narea.\nNOTE : Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "YEAR(SO.ModifiedDate)\nExtracts the year from the ModifiedDate column to aggregate order quantities by year .\nROLLUP(YEAR(SO.ModifiedDate), P.Name)\nThe ROLLUP function generates subtotal rows :\nSummarized total order quantity per product per year .\nOverall total order quantity per year (product subtotal).", "reference": "Explanation:\nYEAR(SO.ModifiedDate)\nExtracts the year from the ModifiedDate column to aggregate order quantities by year .\nROLLUP(YEAR(SO.ModifiedDate), P.Name)\nThe ROLLUP function generates subtotal rows :\nSummarized total order quantity per product per year .\nOverall total order quantity per year (product subtotal).", "boxes": [], "image": "images/p159.jpg"}, {"id": 128, "shown": "75", "type": "dragdrop", "stem": "You have a Fabric lakehouse named Lakehouse1.You have the data sources shown in the following table.\nYou need to ingest data from both data sources into Lakehouse1.  The solution must minimize\nmaintenance and development time.\nWhat should you use to ingest the data into each data source? T o answer, drag the appropriate options to\nthe correct data sources. Each option may be used once, more th an once, or not at all. You may need to\ndrag the split bar between panes or scroll to view content.\nNOTE : Each correct selection is worth one point.\nSelect and Place:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: ADLS Gen2\nDatasource1Contains data from an Azure SQL managed instance that has a cha nge data capture (CDC) enabled for all\ntables.Has a daily data load that is more than 500,000 rows and 20 GB.\nChange data capture in Azure Data Factory and Azure Synapse Ana lytics.\nAuto incremental extraction in mapping data flow.The newly updated rows or updated files can be automatically de tected and extracted by ADF mapping\ndata flow from the source stores. When you want to get delta da ta from the databases, the incremental\ncolumn is required to identify the changes. When you want to lo ad new files or updated files only from a\nstorage store, ADF mapping data flow just works through files’ last modify time.\nSupported connectors\nAzure Blob StorageADLS Gen2ADLS Gen1Etc.\nBoth ADLS Gen1 and ADLS Gen2 works in this scenario.ADLS Gen2 is generally cheaper than ADLS Gen1. Gen2 leverages A zure Blob Storage and its tiered\npricing, offering lower storage costs per GB than the HDFS-base d Gen1.\nBox 2: An eventstream\nDatasource2Contains data from a manufacturing machine that includes the it em count, item weight, and item volume.\nContains data that is streamed to an Azure event hub in Azure.\nIngest, filter, and transform real-time events and send them to a Microsoft Fabric lakehouse\n1. Create Eventstream and Lakehouse items in Microsoft Fabric.2. Add an Azure Event Hubs source to the eventstream.3. Create an event hub cloud connection.4. Add a Lakehouse destination to the eventstream", "reference": "Explanation:\nBox 1: ADLS Gen2\nDatasource1Contains data from an Azure SQL managed instance that has a cha nge data capture (CDC) enabled for all\ntables.Has a daily data load that is more than 500,000 rows and 20 GB.\nChange data capture in Azure Data Factory and Azure Synapse Ana lytics.\nAuto incremental extraction in mapping data flow.The newly updated rows or updated files can be automatically de tected and extracted by ADF mapping\ndata flow from the source stores. When you want to get delta da ta from the databases, the incremental\ncolumn is required to identify the changes. When you want to lo ad new files or updated files only from a\nstorage store, ADF mapping data flow just works through files’ last modify time.\nSupported connectors\nAzure Blob StorageADLS Gen2ADLS Gen1Etc.\nBoth ADLS Gen1 and ADLS Gen2 works in this scenario.ADLS Gen2 is generally cheaper than ADLS Gen1. Gen2 leverages A zure Blob Storage and its tiered\npricing, offering lower storage costs per GB than the HDFS-base d Gen1.\nBox 2: An eventstream\nDatasource2Contains data from a manufacturing machine that includes the it em count, item weight, and item volume.\nContains data that is streamed to an Azure event hub in Azure.\nIngest, filter, and transform real-time events and send them to a Microsoft Fabric lakehouse\n1. Create Eventstream and Lakehouse items in Microsoft Fabric.2. Add an Azure Event Hubs source to the eventstream.3. Create an event hub cloud connection.4. Add a Lakehouse destination to the eventstream\nReference:\nhttps://learn.microsoft.com/en-us/azure/data-factory/concepts-c hange-data-capture\nhttps://learn.microsoft.com/en-us/fabric/real-time-intelligence /event-streams/transform-and-stream-real-\ntime-events-to-lakehouse", "boxes": [{"box": 1, "value": "ADLS Gen2"}, {"box": 2, "value": "An eventstream"}], "images": ["images/p160.jpg", "images/p161.jpg"]}, {"id": 129, "shown": "76", "type": "dragdrop", "stem": "You have a Fabric eventhouse named Eventhouse1 that contains a table named Weatherdata. A sample\nof the data in Weatherdata is shown in the following table.\nYou plan to use a KQL queryset to manipulate the data in Eventh ouse1. The result set must have the\nfollowing columns.\nYou need to build the query.\nWhich four KQL statements should you use in sequence? To answer , move the appropriate statements\nfrom the list of statements to the answer area and arrange them  in the correct order.\nSelect and Place:", "options": [], "correct": "", "multi": false, "explanation": "Step 1: weatherdata\nThe following example calculates the number of unique storm eve nt types for each state and sorts the\nresults by the number of unique storm types:\nStormEvents\n| summarize TypesOfStorms=dcount(EventType) by State| sort by TypesOfStorms\n\nStep 2: | extend Delta_temperature = Temperature - prev(Tempe rature)\nThe extend operator creates calculated columns and append them to the result set.\nExtend needs to occur before summarize as Delta_temperature is used in the summarize statement.\nUse prev, not next, to get the previous temperature.\nDo not use prev(Temperature,2), instead use the default offset 1 with prev(Temperature).\nNote: Kusto prev()\nReturns the value of a specific column in a specified row. The specified row is at a specified offset from the\ncurrent row in a serialized row set.\nSyntax\nprev(column, [ offset ], [ default_value ] )\nParameters\n* Column stringThe column from which to get the values.\n* offset int\nThe offset to go back in rows. The default is 1.\n* default_value scalar\nThe default value to be used when there are no previous rows fr om which to take the value. The default is\nnull.\nStep 3: | summarize …\nSummarize after extend.\nStep 4: | sort by Datetime asc\nPut sort last.", "reference": "Explanation:\nStep 1: weatherdata\nThe following example calculates the number of unique storm eve nt types for each state and sorts the\nresults by the number of unique storm types:\nStormEvents\n| summarize TypesOfStorms=dcount(EventType) by State| sort by TypesOfStorms\n\nStep 2: | extend Delta_temperature = Temperature - prev(Tempe rature)\nThe extend operator creates calculated columns and append them to the result set.\nExtend needs to occur before summarize as Delta_temperature is used in the summarize statement.\nUse prev, not next, to get the previous temperature.\nDo not use prev(Temperature,2), instead use the default offset 1 with prev(Temperature).\nNote: Kusto prev()\nReturns the value of a specific column in a specified row. The specified row is at a specified offset from the\ncurrent row in a serialized row set.\nSyntax\nprev(column, [ offset ], [ default_value ] )\nParameters\n* Column stringThe column from which to get the values.\n* offset int\nThe offset to go back in rows. The default is 1.\n* default_value scalar\nThe default value to be used when there are no previous rows fr om which to take the value. The default is\nnull.\nStep 3: | summarize …\nSummarize after extend.\nStep 4: | sort by Datetime asc\nPut sort last.\nReference:\nhttps://learn.microsoft.com/en-us/kusto/query/prev-function\nhttps://learn.microsoft.com/en-us/kusto/query/summarize-operato r\nhttps://learn.microsoft.com/en-us/kusto/query/extend-operator", "boxes": [], "images": ["images/p162.jpg", "images/p163.jpg"]}, {"id": 130, "shown": "77", "type": "mc", "stem": "You have a Fabric workspace named Workspace1 that contains an e ventstream named Eventstream1.\nEventstream1 reads data from an Azure event hub named Eventhub1 .\nEventhub1 contains the following columns.\nYou need to add a continuous percentile calculation to the Payl oad column. The solution must minimize\ndevelopment effort.\nWhat should you do?", "options": [{"key": "A", "text": "Add a KQL queryset to Workspace1."}, {"key": "B", "text": "Add a Group by transformation to Eventstream1."}, {"key": "C", "text": "Add a Manage fields transformation to Eventstream1."}, {"key": "D", "text": "Add an Aggregate transformation to Eventstream1."}], "correct": "D", "multi": false, "explanation": "The Aggregate transformation in Fabric event streams can be use d to calculate a continuous percentile.\nYou can add a \"Percentile (continuous)\" aggregation to the tran sformation to achieve this.\nBy using the Aggregate transformation and selecting the \"Percen tile (continuous)\" option, you can\ncalculate the percentile of a column's values within the event stream, making it a valuable tool for real-time\nanalytics in Fabric.\n[Not B]\nThe \"Group by\" transformation can be used for more complex aggr egations and time windowing options,\nbut the \"Aggregate\" transformation provides a simpler approach for calculating continuous percentiles.\nNote:\nAggregate Transformation:This transformation is used to calculate aggregations (like sum , average, min, max, and in this case,\npercentile) across a stream of events.\nContinuous Percentile:\nThe \"Percentile (continuous)\" option allows you to calculate th e percentile of a column's values within a\nspecified time window.\nEvent Streams:\nFabric event streams capture real-time events, allowing you to ingest, transform, and route these events to\nvarious destinations.\nNo-Code Experience:\nEvent streams offer a no-code approach to configuring the strea m, allowing you to easily add\ntransformations, destinations, and data sources.", "reference": "Explanation:\nThe Aggregate transformation in Fabric event streams can be use d to calculate a continuous percentile.\nYou can add a \"Percentile (continuous)\" aggregation to the tran sformation to achieve this.\nBy using the Aggregate transformation and selecting the \"Percen tile (continuous)\" option, you can\ncalculate the percentile of a column's values within the event stream, making it a valuable tool for real-time\nanalytics in Fabric.\n[Not B]\nThe \"Group by\" transformation can be used for more complex aggr egations and time windowing options,\nbut the \"Aggregate\" transformation provides a simpler approach for calculating continuous percentiles.\nNote:\nAggregate Transformation:This transformation is used to calculate aggregations (like sum , average, min, max, and in this case,\npercentile) across a stream of events.\nContinuous Percentile:\nThe \"Percentile (continuous)\" option allows you to calculate th e percentile of a column's values within a\nspecified time window.\nEvent Streams:\nFabric event streams capture real-time events, allowing you to ingest, transform, and route these events to\nvarious destinations.\nNo-Code Experience:\nEvent streams offer a no-code approach to configuring the strea m, allowing you to easily add\ntransformations, destinations, and data sources.\nReference:\nhttps://learn.microsoft.com/en-us/fabric/real-time-intelligence /event-streams/overview", "boxes": [], "image": "images/p164.jpg"}, {"id": 131, "shown": "78", "type": "mc", "stem": "You have a Fabric warehouse that contains a table named Table1.  Table1 contains three columns named\nSalesAmount, ProductCategory, and TransactionDate.\nYou need to create a Microsoft Power BI query that will calcula te the total sales amount of each product\ncategory for transactions that occurred during the last quarter .\nWhich two actions should you perform in the visual query editor ? Each correct answer presents part of the\nsolution.\nNOTE:  Each correct selection is worth one point.\nthen set the value to 90.", "options": [{"key": "A", "text": "For the TransactionDate column, select Remove duplicates ."}, {"key": "B", "text": "For the TransactionDate column, select Keep top rows  and set Number of rows to 90."}, {"key": "C", "text": "For the TransactionDate column, select Filter rows  and set the value to last quarter ."}, {"key": "D", "text": "For the ProductCategory column, select Group by  and sum the SalesAmount column."}, {"key": "E", "text": "For the SalesAmount column, select Transformation number column - Standard , select Add, and"}], "correct": "BD", "multi": true, "explanation": "[B] Last quarter is approximate equal to the last 90 days.If we assume the table is sorted on the TransactionDate column, or we can sort on it as well, Keep top\nrows and set Number of rows to 90 would filter on the last quar ter of sales.\n[D]\nTo calculate the sum of sales and aggregate on product category in the Power BI visual query editor using\ndata from a Fabric warehouse table, follow these steps: 1. Access the Visual Query Editor: Open your Power BI report an d click on the \"Visual Query Editor\" button\nin the report view.\n2. Add the Data Source: Specify the Fabric warehouse table as t he data source for your visual.\nDefine the Aggregation:Sum of Sales: Use the SUM() function in your query to calculate the total sales amount.\n3. Aggregate by Product Category: Use the GROUP BY clause in yo ur query, specifying the product\ncategory column as the grouping criteria.\nExample Query:\nCode\n SELECT\n Product_Category, -- Select the product category SUM(Sales_Amount) AS Total_Sales -- Calculate the sum o f sales for each category\n FROM Your_Fabric_Warehouse_Table GROUP BY Product_Category -- Aggregate by product category\nIncorrect:\n[Not C]There is no option for last quarter, when you Filter rows.", "reference": "Explanation:\n[B] Last quarter is approximate equal to the last 90 days.If we assume the table is sorted on the TransactionDate column, or we can sort on it as well, Keep top\nrows and set Number of rows to 90 would filter on the last quar ter of sales.\n[D]\nTo calculate the sum of sales and aggregate on product category in the Power BI visual query editor using\ndata from a Fabric warehouse table, follow these steps: 1. Access the Visual Query Editor: Open your Power BI report an d click on the \"Visual Query Editor\" button\nin the report view.\n2. Add the Data Source: Specify the Fabric warehouse table as t he data source for your visual.\nDefine the Aggregation:Sum of Sales: Use the SUM() function in your query to calculate the total sales amount.\n3. Aggregate by Product Category: Use the GROUP BY clause in yo ur query, specifying the product\ncategory column as the grouping criteria.\nExample Query:\nCode\n SELECT\n Product_Category, -- Select the product category SUM(Sales_Amount) AS Total_Sales -- Calculate the sum o f sales for each category\n FROM Your_Fabric_Warehouse_Table GROUP BY Product_Category -- Aggregate by product category\nIncorrect:\n[Not C]There is no option for last quarter, when you Filter rows.\nReference:\nhttps://learn.microsoft.com/en-us/power-bi/create-reports/servi ce-aggregates", "boxes": []}, {"id": 132, "shown": "79", "type": "mc", "stem": "You have a Fabric tenant that contains a warehouse.\nYou use a dataflow to load a new dataset from OneLake to the wa rehouse.\nYou need to add a Power Query step to identify the maximum valu es for the numeric columns.\nWhich function should you include in the step?", "options": [{"key": "A", "text": "Table.Range"}, {"key": "B", "text": "Table.MaxN"}, {"key": "C", "text": "Table.Profile"}], "correct": "C", "multi": false, "explanation": "Table. Profile is a Power Query M function that generates a pro file for each column in a table, providing\nvarious statistics. The function returns a table with informati on such as minimum, maximum, average,\nstandard deviation, count, null count, and distinct count for e ach column.\nNote: Power Query M, Table.ProfileReturns a profile for the columns in table.\n\nSyntax\nTable.Profile(table as table, optional additionalAggregates as nullable list) as table\nThe following information is returned for each column (when app licable):\nminimum\nmaximumaveragestandard deviationcountnull countdistinct count\nIncorrect:\n* Table.Max\nThe Table.Max function returns the row in a table with the larg est value based on a specified column or\ncustom condition.\n* Table.MaxN\nThe Table.MaxN function in Power Query returns the largest row( s) in a table, based on a specified\ncomparison criteria and a count or condition. It effectively ex tracts the top rows that meet the specified\ncriteria.", "reference": "Explanation:\nTable. Profile is a Power Query M function that generates a pro file for each column in a table, providing\nvarious statistics. The function returns a table with informati on such as minimum, maximum, average,\nstandard deviation, count, null count, and distinct count for e ach column.\nNote: Power Query M, Table.ProfileReturns a profile for the columns in table.\n\nSyntax\nTable.Profile(table as table, optional additionalAggregates as nullable list) as table\nThe following information is returned for each column (when app licable):\nminimum\nmaximumaveragestandard deviationcountnull countdistinct count\nIncorrect:\n* Table.Max\nThe Table.Max function returns the row in a table with the larg est value based on a specified column or\ncustom condition.\n* Table.MaxN\nThe Table.MaxN function in Power Query returns the largest row( s) in a table, based on a specified\ncomparison criteria and a count or condition. It effectively ex tracts the top rows that meet the specified\ncriteria.\nReference:\nhttps://learn.microsoft.com/en-us/powerquery-m/table-profile", "boxes": []}, {"id": 133, "shown": "80", "type": "hotspot", "stem": "You have a Fabric warehouse named Warehouse1 that contains a ta ble named TaxiTrips.\nTaxiTrips contains the following columns.\nYou need to create a query that shows the top three taxi compan ies based on the total miles traveled.\nHow should you complete the T-SQL statement? To answer, select the appropriate options in the answer\narea.\nNOTE : Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: Top (3)\nTOP (Transact-SQL)Limits the rows returned in a query result set to a specified n umber of rows or percentage of rows in SQL\nServer. When you use TOP with the ORDER BY clause, the result s et is limited to the first n number of\nordered rows. Otherwise, TOP returns the first n number of rows in an undefined order.\nBox 2: GROUP BY company\nWe want to group by taxi companies, to use the sum of trip dist ances for each company.\nNote: SUM() is a SQL aggregate function that computes the sum o f the given values. GROUP BY is a SQL\nclause that partitions rows into groups and computes a stated a ggregate function for each group. Using\nthese two functions together, you can compute total sums for a group of rows.\nIncorrect:\n* TaxiCompanyCan't use an alias in the GROUP BY statement.\nThe GROUP BY column expression cannot contain:\n*-> A column alias that is defined in the SELECT list. It can u se a column alias for a derived table that is\ndefined in the FROM clause.\n\nBox 3: ORDER BY sum(tripDistance)\nWe sort by the sum of the tripDistance in descending order.", "reference": "Explanation:\nBox 1: Top (3)\nTOP (Transact-SQL)Limits the rows returned in a query result set to a specified n umber of rows or percentage of rows in SQL\nServer. When you use TOP with the ORDER BY clause, the result s et is limited to the first n number of\nordered rows. Otherwise, TOP returns the first n number of rows in an undefined order.\nBox 2: GROUP BY company\nWe want to group by taxi companies, to use the sum of trip dist ances for each company.\nNote: SUM() is a SQL aggregate function that computes the sum o f the given values. GROUP BY is a SQL\nclause that partitions rows into groups and computes a stated a ggregate function for each group. Using\nthese two functions together, you can compute total sums for a group of rows.\nIncorrect:\n* TaxiCompanyCan't use an alias in the GROUP BY statement.\nThe GROUP BY column expression cannot contain:\n*-> A column alias that is defined in the SELECT list. It can u se a column alias for a derived table that is\ndefined in the FROM clause.\n\nBox 3: ORDER BY sum(tripDistance)\nWe sort by the sum of the tripDistance in descending order.\nReference:\nhttps://learn.microsoft.com/en-us/sql/t-sql/queries/top-transac t-sql\nhttps://learn.microsoft.com/en-us/sql/t-sql/queries/select-grou p-by-transact-sql", "boxes": [{"box": 1, "value": "Top (3)"}, {"box": 2, "value": "GROUP BY company"}, {"box": 3, "value": "ORDER BY sum(tripDistance)"}], "images": ["images/p167.jpg", "images/p168.jpg", "images/p169.jpg"]}, {"id": 134, "shown": "81", "type": "mc", "stem": "You have a Fabric tenant that contains two workspaces named Wor kspace1 and Workspace2.\nWorkspace1 is used as the development environment. Workspace2 i s used as the production\nenvironment. Each environment uses a different storage account.\nWorkspace1 contains a Dataflow Gen2 named Dataflow1. The data s ource of Dataflow1 is a CSV file in\nblob storage.\nYou plan to implement a deployment pipeline to deploy items fro m Workspace1 to Workspace2.\nYou need to ensure that the data source references the correct location in the production environment.\nWhat should you do?", "options": [{"key": "A", "text": "Create a data source rule only."}, {"key": "B", "text": "Create a parameter rule only."}, {"key": "C", "text": "Create a data source rule and a parameter rule."}, {"key": "D", "text": "After implementing the deployment pipeline, manually change t he data source"}], "correct": "C", "multi": false, "explanation": "Both parameter rules and deployment rules can be necessary when using deployment pipelines in\nMicrosoft Fabric with Dataflow Gen2 to ensure correct data sour ce references when deploying to a second\nworkspace. Parameter rules are used to manage the values of par ameters within a dataflow, while\ndeployment rules are used to configure how data sources are han dled during deployment.\nParameter Rules:\nThese allow you to change the values of parameters defined with in a Dataflow Gen2 during deployment.\nFor example, you might have a parameter that determines the dat abase connection string, and you would\nuse a parameter rule to update this string to point to a differ ent database in the target workspace.\nDeployment Rules:\nDeployment rules are used to configure how data sources are han dled during deployment. You can define\nrules that specify how to map data sources from the source work space to the target workspace. For\ninstance, you could define a rule that changes the data source of a Dataflow Gen2 from a source\nlakehouse to a target lakehouse during deployment.\nIn summary:\nParameter rulesallow you to change parameter values in the Dataflow Gen2 itsel f during deployment.\nDeployment rules\nallow you to configure how the data sources used by the Dataflo w Gen2 are mapped to the target\nworkspace during deployment. By using both parameter rules and deployment rules, you can ens ure that your Dataflow Gen2 correctly\nreferences the appropriate data sources and parameters in the t arget workspace.\nIncorrect:\n[Not A] A parameter rule is generally needed when deploying a D ataflow Gen2 with a CSV file in blob\nstorage in a Fabric workspace using a deployment pipeline. This ensures the correct data source\nreferences are maintained after deployment to a second workspac e. Without a parameter rule, the\n\ndeployment may not automatically re-bind the Dataflow to the co rrect data source in the new workspace,\npotentially leading to errors.\nDynamic Connections:\nDeployment pipelines can move assets between workspaces, and th e data source connections (like blob\nstorage credentials) might be different in each workspace. Para meter rules allow you to define these\nconnections dynamically, so that the Dataflow can point to the correct data source after deployment.\nData Source References:\nWhen a CSV file is in blob storage, the Dataflow's data source connection will usually include the blob\nstorage credentials and the path to the file. These details can change between the source and target\nworkspaces. Parameter rules help update those details during de ployment.\nFunctionality:\nWhen a parameter controls the connection, autobinding after dep loyment doesn't automatically occur. By\ndefining parameter rules, you can ensure the connection is corr ectly established in the new workspace.\nRebinding:\nAfter deployment, you might need to rebind items to the new wor kspace's data source. Parameter rules\nprovide a method to rebind these items to the correct data sour ces, as autobinding doesn't occur when a\nparameter controls the connection.", "reference": "Explanation:\nBoth parameter rules and deployment rules can be necessary when using deployment pipelines in\nMicrosoft Fabric with Dataflow Gen2 to ensure correct data sour ce references when deploying to a second\nworkspace. Parameter rules are used to manage the values of par ameters within a dataflow, while\ndeployment rules are used to configure how data sources are han dled during deployment.\nParameter Rules:\nThese allow you to change the values of parameters defined with in a Dataflow Gen2 during deployment.\nFor example, you might have a parameter that determines the dat abase connection string, and you would\nuse a parameter rule to update this string to point to a differ ent database in the target workspace.\nDeployment Rules:\nDeployment rules are used to configure how data sources are han dled during deployment. You can define\nrules that specify how to map data sources from the source work space to the target workspace. For\ninstance, you could define a rule that changes the data source of a Dataflow Gen2 from a source\nlakehouse to a target lakehouse during deployment.\nIn summary:\nParameter rulesallow you to change parameter values in the Dataflow Gen2 itsel f during deployment.\nDeployment rules\nallow you to configure how the data sources used by the Dataflo w Gen2 are mapped to the target\nworkspace during deployment. By using both parameter rules and deployment rules, you can ens ure that your Dataflow Gen2 correctly\nreferences the appropriate data sources and parameters in the t arget workspace.\nIncorrect:\n[Not A] A parameter rule is generally needed when deploying a D ataflow Gen2 with a CSV file in blob\nstorage in a Fabric workspace using a deployment pipeline. This ensures the correct data source\nreferences are maintained after deployment to a second workspac e. Without a parameter rule, the\n\ndeployment may not automatically re-bind the Dataflow to the co rrect data source in the new workspace,\npotentially leading to errors.\nDynamic Connections:\nDeployment pipelines can move assets between workspaces, and th e data source connections (like blob\nstorage credentials) might be different in each workspace. Para meter rules allow you to define these\nconnections dynamically, so that the Dataflow can point to the correct data source after deployment.\nData Source References:\nWhen a CSV file is in blob storage, the Dataflow's data source connection will usually include the blob\nstorage credentials and the path to the file. These details can change between the source and target\nworkspaces. Parameter rules help update those details during de ployment.\nFunctionality:\nWhen a parameter controls the connection, autobinding after dep loyment doesn't automatically occur. By\ndefining parameter rules, you can ensure the connection is corr ectly established in the new workspace.\nRebinding:\nAfter deployment, you might need to rebind items to the new wor kspace's data source. Parameter rules\nprovide a method to rebind these items to the correct data sour ces, as autobinding doesn't occur when a\nparameter controls the connection.\nReference:\nhttps://learn.microsoft.com/en-us/fabric/cicd/deployment-pipeli nes/understand-the-deployment-process", "boxes": []}, {"id": 135, "shown": "82", "type": "dragdrop", "stem": "You are implementing two dimension tables named Customers and P roducts in a Fabric warehouse.\nYou need to create two slowly changing dimensions that meet the  requirements shown in the following\ntable.\nWhich type of SCD should you use for each table? To answer, dra g the appropriate SCD types to the\ncorrect tables. Each SCD type may be used once, more than once,  or not at all. You may need to drag the\nsplit bar between panes or scroll to view content.\nNOTE : Each correct selection is worth one point.\nSelect and Place:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: Type 2\nCustomersCreate a new version of the customer record while keeping the p revious versions.\nSlowly changing dimension type 2 is a method used in data wareh ousing to manage and track historical\nchanges in dimension data. When an attribute value changes, a n ew record is created with a unique\nidentifier, and the old record is retained. It allows for a com plete historical record of changes over time,\nenabling accurate reporting and analysis based on different poi nts in time.\nIncorrect:\n* Type 3Slowly Changing Dimension (SCD) type 3 tracks changes in a dime nsion by adding a new column for the\nhistorical value while keeping the current value in the same co lumn. It essentially stores the last two\nversions of an attribute side-by-side within the same row.\nSlowly Changing Dimension (SCD) type 2 and type 3 are methods f or handling changes in dimension data\nin a data warehouse. Type 2 creates a new record for every chan ge, preserving full historical data. Type 3\nadds a new column to the existing record to store the new value , effectively tracking only the most recent\nchange and a limited amount of history.\nBox 2: Type 1\nProductsOverwrite the existing value in the current record\nUsing slowly changing dimension type 1, if a record in a dimens ion table changes, the existing record is\nupdated or overwritten. Otherwise, the new record is inserted i nto the dimension table. This means records\nin the dimension table always reflect the current state, and no historical data is maintained. This design\napproach is common for columns that store supplementary values, like the email address or phone\nnumber of a customer. When a customer’s email address or phone number changes, the dimension table\nupdates the customer row with the new values. It's as if the cu stomer always had this contact information.\nIncorrect:\n* Type 0Type 0, also known as the \"Retain Original\" type, is a techniqu e used in data warehousing to handle\ndimensions that never change. It's essentially a fixed dimensio n where the data in the table remains the\nsame over time, and no historical tracking is needed.", "reference": "Explanation:\nBox 1: Type 2\nCustomersCreate a new version of the customer record while keeping the p revious versions.\nSlowly changing dimension type 2 is a method used in data wareh ousing to manage and track historical\nchanges in dimension data. When an attribute value changes, a n ew record is created with a unique\nidentifier, and the old record is retained. It allows for a com plete historical record of changes over time,\nenabling accurate reporting and analysis based on different poi nts in time.\nIncorrect:\n* Type 3Slowly Changing Dimension (SCD) type 3 tracks changes in a dime nsion by adding a new column for the\nhistorical value while keeping the current value in the same co lumn. It essentially stores the last two\nversions of an attribute side-by-side within the same row.\nSlowly Changing Dimension (SCD) type 2 and type 3 are methods f or handling changes in dimension data\nin a data warehouse. Type 2 creates a new record for every chan ge, preserving full historical data. Type 3\nadds a new column to the existing record to store the new value , effectively tracking only the most recent\nchange and a limited amount of history.\nBox 2: Type 1\nProductsOverwrite the existing value in the current record\nUsing slowly changing dimension type 1, if a record in a dimens ion table changes, the existing record is\nupdated or overwritten. Otherwise, the new record is inserted i nto the dimension table. This means records\nin the dimension table always reflect the current state, and no historical data is maintained. This design\napproach is common for columns that store supplementary values, like the email address or phone\nnumber of a customer. When a customer’s email address or phone number changes, the dimension table\nupdates the customer row with the new values. It's as if the cu stomer always had this contact information.\nIncorrect:\n* Type 0Type 0, also known as the \"Retain Original\" type, is a techniqu e used in data warehousing to handle\ndimensions that never change. It's essentially a fixed dimensio n where the data in the table remains the\nsame over time, and no historical tracking is needed.\nReference:\nhttps://learn.microsoft.com/en-us/fabric/data-factory/slowly-ch anging-dimension-type-two\nhttps://learn.microsoft.com/en-us/fabric/data-factory/slowly-ch anging-dimension-type-one\nhttps://en.wikipedia.org/wiki/Slowly_changing_dimension", "boxes": [{"box": 1, "value": "Type 2"}, {"box": 2, "value": "Type 1"}], "images": ["images/p171.jpg", "images/p172.jpg"]}, {"id": 136, "shown": "83", "type": "hotspot", "stem": "You have a Fabric eventhouse that contains a KQL database. The database contains a table named\nTaxiData that stores the following data.\nYou need to create a column named FirstPickupDateTime that will  contain the first value of each hour from\ntpep_pickup_datetime partitioned by payment_type.\nHow should you complete the query? To answer, select the approp riate options in the answer area.\nNOTE : Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: Row_Window_Session\nKusto, KQL Windowing Functions – Row_Window_SessionThe row_window_session function can be used to group rows of da ta in a time range, and will return the\nstarting time for that range of data in each row.\nExample:\n\nBox 2: !=\nHere we are using an equation, which will compare the payment_t ype for the current row to the one of the\nprevious row using the prev Windowing Function. If they are not equal, the comparison will return true and\ntrigger Row_Window_Session to begin a Row_Window_Session groupi ng.\nNote: prev()\nReturns the value of a specific column in a specified row. The specified row is at a specified offset from the\ncurrent row in a serialized row set.\nSyntax\nprev(column, [ offset ], [ default_value ] )\nParameters\n* Column stringThe column from which to get the values.\n* offset int\nThe offset to go back in rows. The default is 1.\n* default_value scalar\nThe default value to be used when there are no previous rows fr om which to take the value. The default is\nnull.\nIncorrect:\n* Row_rank_minKusto, row_rank_min()Returns the current row's minimal rank in a serialized row set.\nThe rank is the minimal row number that the current row's Term appears in.\nSyntax\nrow_rank_min ( Term )\nParameters\n* Term string (required)An expression indicating the value to consider for the rank. Th e rank is the minimal row number for Term.\n* Restart* row_rank_dense()\nReturns the current row's dense rank in a serialized row set.\nThe row rank starts by default at 1 for the first row, and is i ncremented by 1 whenever the provided Term is\ndifferent than the previous row's Term.\n\n* ==\nIn Kusto == is the Equals operator.", "reference": "Explanation:\nBox 1: Row_Window_Session\nKusto, KQL Windowing Functions – Row_Window_SessionThe row_window_session function can be used to group rows of da ta in a time range, and will return the\nstarting time for that range of data in each row.\nExample:\n\nBox 2: !=\nHere we are using an equation, which will compare the payment_t ype for the current row to the one of the\nprevious row using the prev Windowing Function. If they are not equal, the comparison will return true and\ntrigger Row_Window_Session to begin a Row_Window_Session groupi ng.\nNote: prev()\nReturns the value of a specific column in a specified row. The specified row is at a specified offset from the\ncurrent row in a serialized row set.\nSyntax\nprev(column, [ offset ], [ default_value ] )\nParameters\n* Column stringThe column from which to get the values.\n* offset int\nThe offset to go back in rows. The default is 1.\n* default_value scalar\nThe default value to be used when there are no previous rows fr om which to take the value. The default is\nnull.\nIncorrect:\n* Row_rank_minKusto, row_rank_min()Returns the current row's minimal rank in a serialized row set.\nThe rank is the minimal row number that the current row's Term appears in.\nSyntax\nrow_rank_min ( Term )\nParameters\n* Term string (required)An expression indicating the value to consider for the rank. Th e rank is the minimal row number for Term.\n* Restart* row_rank_dense()\nReturns the current row's dense rank in a serialized row set.\nThe row rank starts by default at 1 for the first row, and is i ncremented by 1 whenever the provided Term is\ndifferent than the previous row's Term.\n\n* ==\nIn Kusto == is the Equals operator.\nReference:\nhttps://arcanecode.com/category/kusto/\n\nImplement and manage semantic models\nTestlet 1Case study\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you\nwould like to complete each case. However, there may be additional case studies and sections on this\nexam. You must manage your time to ensure that you are able to complete all questions included on this\nexam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in\nthe case study. Case studies might contain exhibits and other r esources that provide more information\nabout the scenario that is described in the case study. Each qu estion is independent of the other questions\nin this case study.\nAt the end of this case study, a review screen will appear. Thi s screen allows you to review your answers\nand to make changes before you move to the next section of the exam. After you begin a new section, you\ncannot return to this section.\nTo start the case study\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to\nexplore the content of the case study before you answer the que stions. Clicking these buttons displays\ninformation such as business requirements, existing environment , and problem statements. If the case\nstudy has an All Information tab, note that the information displayed is identical to the i nformation\ndisplayed on the subsequent tabs. When you are ready to answer a question, click the Question button to\nreturn to the question.\nOverview\nLitware, Inc. is a manufacturing company that has offices throu ghout North America. The analytics team at\nLitware contains data engineers, analytics engineers, data anal ysts, and data scientists.\nExisting Environment\nFabric Environment\nLitware has been using a Microsoft Power BI tenant for three ye ars. Litware has NOT enabled any Fabric\ncapacities and features.\nAvailable Data\nLitware has data that must be analyzed as shown in the followin g table.\nThe Product data contains a single table and the following colu mns.\n\nThe customer satisfaction data contains the following tables:\nSurvey\nQuestion\nResponse\nFor each survey submitted, the following occurs:\nOne row is added to the Survey table.\nOne row is added to the Response table for each question in the survey.\nThe Question table contains the text of each survey question. T he third question in each survey response\nis an overall satisfaction score. Customers can submit a survey after each purchase.\nUser Problems\nThe analytics team has large volumes of data, some of which is semi-structured. The team wants to use\nFabric to create a new data store.\nProduct data is often classified into three pricing groups: hig h, medium, and low. This logic is implemented\nin several databases and semantic models, but the logic does NOT always match across implementations.\nRequirements\nPlanned Changes\nLitware plans to enable Fabric features in the existing tenant. The analytics team will create a new data\nstore as a proof of concept (PoC). The remaining Litware users will only get access to the Fabric features\nonce the PoC is complete. The PoC will be completed by using a Fabric trial capacity.\nThe following three workspaces will be created:\nAnalyticsPOC: Will contain the data store, semantic models, rep orts pipelines, dataflow, and notebooks\nused to populate the data store\nDataEngPOC: Will contain all the pipelines, dataflows, and note books used to populate OneLake\nDataSciPOC: Will contain all the notebooks and reports created by the data scientists\nThe following will be created in the AnalyticsPOC workspace:\nA data store (type to be decided)\nA custom semantic model\nA default semantic model\nInteractive reports\nThe data engineers will create data pipelines to load data to O neLake either hourly or daily depending on\nthe data source. The analytics engineers will create processes to ingest, transform, and load the data to\nthe data store in the AnalyticsPOC workspace daily. Whenever po ssible, the data engineers will use low-\ncode tools for data ingestion. The choice of which data cleansi ng and transformation tools to use will be at\nthe data engineers’ discretion.\nAll the semantic models and reports in the Analytics POC worksp ace will use the data store as the sole\ndata source.\nTechnical Requirements\nThe data store must support the following:\nRead access by using T-SQL or Python\nSemi-structured and unstructured data\nRow-level security (RLS) for users executing T-SQL queries\nFiles loaded by the data engineers to OneLake will be stored in the Parquet format and will meet Delta\nLake specifications.\n\nData will be loaded without transformation in one area of the A nalyticsPOC data store. The data will then\nbe cleansed, merged, and transformed into a dimensional model.\nThe data load process must ensure that the raw and cleansed dat a is updated completely before\npopulating the dimensional model.\nThe dimensional model must contain a date dimension. There is n o existing data source for the date\ndimension. The Litware fiscal year matches the calendar year. T he date dimension must always contain\ndates from 2010 through the end of the current year.\nThe product pricing group logic must be maintained by the analy tics engineers in a single location. The\npricing group data must be made available in the data store for T-SQL queries and in the default semantic\nmodel. The following logic must be used:\nList prices that are less than or equal to 50 are in the low pr icing group.\nList prices that are greater than 50 and less than or equal to 1,000 are in the medium pricing group.\nList prices that are greater than 1,000 are in the high pricing group.\nSecurity Requirements\nOnly Fabric administrators and the analytics team must be able to see the Fabric items created as part of\nthe PoC.\nLitware identifies the following security requirements for the Fabric items in the AnalyticsPOC workspace:\nFabric administrators will be the workspace administrators.\nThe data engineers must be able to read from and write to the d ata store. No access must be granted\nto datasets or reports.\nThe analytics engineers must be able to read from, write to, an d create schemas in the data store. They\nalso must be able to create and share semantic models with the data analysts and view and modify all\nreports in the workspace.\nThe data scientists must be able to read from the data store, b ut not write to it. They will access the\ndata by using a Spark notebook.\nThe data analysts must have read access to only the dimensional model objects in the data store. They\nalso must have access to create Power BI reports by using the s emantic models created by the\nanalytics engineers.\nThe date dimension must be available to all users of the data s tore.\nThe principle of least privilege must be followed.\nBoth the default and custom semantic models must include only t ables or views from the dimensional\nmodel in the data store. Litware already has the following Micr osoft Entra security groups:\nFabricAdmins: Fabric administrators\nAnalyticsTeam: All the members of the analytics team\nDataAnalysts: The data analysts on the analytics team\nDataScientists: The data scientists on the analytics team\nDataEngineers: The data engineers on the analytics team\nAnalyticsEngineers: The analytics engineers on the analytics te am\nReport Requirements\nThe data analysts must create a customer satisfaction report th at meets the following requirements:\nEnables a user to select a product to filter customer survey re sponses to only those who have\npurchased that product.\nDisplays the average overall satisfaction score of all the surv eys submitted during the last 12 months\nup to a selected date.\nShows data as soon as the data is updated in the data store.\nEnsures that the report and the semantic model only contain dat a from the current and previous year.\nEnsures that the report respects any table-level security speci fied in the source data store.\nMinimizes the execution time of report queries.", "boxes": [{"box": 1, "value": "Row_Window_Session"}, {"box": 2, "value": "!="}], "images": ["images/p173.jpg", "images/p174.jpg", "images/p176.jpg", "images/p177.jpg", "images/p178.jpg"]}, {"id": 137, "shown": "1", "type": "hotspot", "stem": "You need to create a DAX measure to calculate the average overa ll satisfaction score.\nHow should you complete the DAX code? To answer, select the app ropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: AVERAGEX\nAVERAGEX: Calculates the average (arithmetic mean) of a set of expressions evaluated over a table.\nSyntax: AVERAGEX(<table>,<expression>)\nThe AVERAGEX function enables you to evaluate expressions for e ach row of a table, and then take the\nresulting set of values and calculate its arithmetic mean. Ther efore, the function takes a table as its first\nargument, and an expression as the second argument.\nExample\nThe following example calculates the average freight and tax on each order in the InternetSales table, by\nfirst summing Freight plus TaxAmt in each row, and then averagi ng those sums.\n= AVERAGEX(InternetSales, InternetSales[Freight]+ InternetSales [TaxAmt])\nIncorrect:\n* AVERAGEAVERAGE: Returns the average (arithmetic mean) of all the numbe rs in a column.\nAVERAGE(<column>)\n* AVERAGEA\nAVERAGEA: Returns the average (arithmetic mean) of the values i n a column.\nAVERAGEA(<column>)\nBox 2: PERIOD\nDATESINPERIODReturns a table that contains a column of dates that begins wit h a specified start date and continues for the\nspecified number and type of date intervals.\n\nThis function is suited to pass as a filter to the CALCULATE fu nction. Use it to filter an expression by\nstandard date intervals such as days, months, quarters, or year s.\nNote: VALUES\nWhen the input parameter is a column name, returns a one-column table that contains the distinct values\nfrom the specified column. Duplicate values are removed and onl y unique values are returned. A BLANK\nvalue can be added. When the input parameter is a table name, r eturns the rows from the specified table.\nDuplicate rows are preserved. A BLANK row can be added.\nSyntax\nVALUES(<TableNameOrColumnName>)\nParameters\nTableName or ColumnNameA column from which unique values are to be returned, or a tabl e from which rows are to be returned.\nScenario:\nFor each survey submitted, the following occurs:\nOne row is added to the Survey table.\nOne row is added to the Response table for each question in the survey.\nThe Question table contains the text of each survey question. T he third question in each survey response\nis an overall satisfaction score. Customers can submit a survey after each purchase.", "reference": "Explanation:\nBox 1: AVERAGEX\nAVERAGEX: Calculates the average (arithmetic mean) of a set of expressions evaluated over a table.\nSyntax: AVERAGEX(<table>,<expression>)\nThe AVERAGEX function enables you to evaluate expressions for e ach row of a table, and then take the\nresulting set of values and calculate its arithmetic mean. Ther efore, the function takes a table as its first\nargument, and an expression as the second argument.\nExample\nThe following example calculates the average freight and tax on each order in the InternetSales table, by\nfirst summing Freight plus TaxAmt in each row, and then averagi ng those sums.\n= AVERAGEX(InternetSales, InternetSales[Freight]+ InternetSales [TaxAmt])\nIncorrect:\n* AVERAGEAVERAGE: Returns the average (arithmetic mean) of all the numbe rs in a column.\nAVERAGE(<column>)\n* AVERAGEA\nAVERAGEA: Returns the average (arithmetic mean) of the values i n a column.\nAVERAGEA(<column>)\nBox 2: PERIOD\nDATESINPERIODReturns a table that contains a column of dates that begins wit h a specified start date and continues for the\nspecified number and type of date intervals.\n\nThis function is suited to pass as a filter to the CALCULATE fu nction. Use it to filter an expression by\nstandard date intervals such as days, months, quarters, or year s.\nNote: VALUES\nWhen the input parameter is a column name, returns a one-column table that contains the distinct values\nfrom the specified column. Duplicate values are removed and onl y unique values are returned. A BLANK\nvalue can be added. When the input parameter is a table name, r eturns the rows from the specified table.\nDuplicate rows are preserved. A BLANK row can be added.\nSyntax\nVALUES(<TableNameOrColumnName>)\nParameters\nTableName or ColumnNameA column from which unique values are to be returned, or a tabl e from which rows are to be returned.\nScenario:\nFor each survey submitted, the following occurs:\nOne row is added to the Survey table.\nOne row is added to the Response table for each question in the survey.\nThe Question table contains the text of each survey question. T he third question in each survey response\nis an overall satisfaction score. Customers can submit a survey after each purchase.\nReference:\nhttps://learn.microsoft.com/en-us/dax/averagex-function-dax\nhttps://learn.microsoft.com/en-us/dax/datesinperiod-function-da x\nhttps://learn.microsoft.com/en-us/dax/values-function-dax", "boxes": [{"box": 1, "value": "AVERAGEX"}, {"box": 2, "value": "PERIOD"}], "images": ["images/p179.jpg", "images/p180.jpg"]}, {"id": 138, "shown": "2", "type": "mc", "stem": "Which type of data store should you recommend in the AnalyticsP OC workspace?", "options": [{"key": "A", "text": "a data lake"}, {"key": "B", "text": "a warehouse"}, {"key": "C", "text": "a lakehouse"}, {"key": "D", "text": "an external Hive metastore"}], "correct": "C", "multi": false, "explanation": "Within the Data Lakehouse:You can store unstructured, semi-structured, or structured data .\nThe data is organized by folders and files, lake databases, and delta tables.\nScenario:\nAll the semantic models and reports in the Analytics POC worksp ace will use the data store as the sole\ndata source.\nTechnical Requirements\nThe data store must support the following:\nRead access by using T-SQL or Python\n*-> Semi-structured and unstructured dataRow-level security (RLS) for users executing T-SQL queries\nIncorrect:\nNot A: a data lakeA data lake would be the data source of the lakehouse.\n\nNot B: a warehouse\nWithin the Data Warehouse:\n*-> You can store structured data.\nThe data is organized by databases, schemas, and tables (delta tables behind the scenes)", "reference": "Explanation:\nWithin the Data Lakehouse:You can store unstructured, semi-structured, or structured data .\nThe data is organized by folders and files, lake databases, and delta tables.\nScenario:\nAll the semantic models and reports in the Analytics POC worksp ace will use the data store as the sole\ndata source.\nTechnical Requirements\nThe data store must support the following:\nRead access by using T-SQL or Python\n*-> Semi-structured and unstructured dataRow-level security (RLS) for users executing T-SQL queries\nIncorrect:\nNot A: a data lakeA data lake would be the data source of the lakehouse.\n\nNot B: a warehouse\nWithin the Data Warehouse:\n*-> You can store structured data.\nThe data is organized by databases, schemas, and tables (delta tables behind the scenes)\nReference:\nhttps://blog.fabric.microsoft.com/en-US/blog/lakehouse-vs-data- warehouse-deep-dive-into-use-cases-\ndifferences-and-architecture-designs/", "boxes": []}, {"id": 139, "shown": "3", "type": "hotspot", "stem": "You need to design a semantic model for the customer satisfacti on report.\nWhich data source authentication method and mode should you use ? To answer, select the appropriate\noptions in the answer area.\nNOTE : Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: Single sign-on (SSO) authentication\nAuthentication method:\nNeed to be able to map the user roles to the model table-level security.\nScenario:\nReport:* Ensure that the report respects any table-level security spec ified in the source data store.\nNote: Embed a report with token-based identity (SSO)\nThe token-based identity allows an ISV to use a Microsoft Entra access token to pass the identity of a\ncustomer to an Azure SQL database managed in the customer's ten ant.\nISV customers that keep and manage their data in Azure SQL Data base can keep their data secure in their\ntenant when integrating with Power BI Embedded in the ISV app.\nWhen generating the embed token, specify the identity of the us er in Azure SQL by passing that user's\nMicrosoft Entra access token for the Azure SQL server. The acce ss token is then used to pull only the\nrelevant data for that user from Azure SQL, for that specific s ession.\nBox 2: Direct Lake\nMode: Scenario: Shows data as soon as the data is updated in the data store.Minimizes the execution time of report queries.\nDirect Lake mode is a groundbreaking semantic model capability for analyzing very large data volumes in\nPower BI. Direct Lake is based on loading parquet-formatted fil es directly from a data lake without having\nto query a Lakehouse or Warehouse endpoint, and without having to import or duplicate data into a Power\nBI model. Direct Lake is a fast-path to load the data from the lake straight into the Power BI engine, ready\nfor analysis. The following diagram shows how classic import an d DirectQuery modes compare with Direct\nLake mode.\nIn DirectQuery mode, the Power BI engine queries the data at th e source, which can be slow but avoids\nhaving to copy the data like with import mode. Any changes at t he data source are immediately reflected in\nthe query results.\nOn the other hand, with import mode, performance can be better because the data is cached and\n\noptimized for DAX and MDX report queries without having to tran slate and pass SQL or other types of\nqueries to the data source. However, the Power BI engine must f irst copy any new data into the model\nduring refresh. Any changes at the source are only picked up wi th the next model refresh.\nDirect Lake mode eliminates the import requirement by loading t he data directly from OneLake. Unlike\nDirectQuery, there is no translation from DAX or MDX to other q uery languages or query execution on\nother database systems, yielding performance similar to import mode. Because there's no explicit import\nprocess, it's possible to pick up any changes at the data sourc e as they occur, combining the advantages\nof both DirectQuery and import modes while avoiding their disad vantages. Direct Lake mode can be the\nideal choice for analyzing very large models and models with fr equent updates at the data source.\nNote Scenario:\nReport Requirements\nThe data analysts must create a customer satisfaction report th at meets the following requirements:\nEnables a user to select a product to filter customer survey re sponses to only those who have purchased\nthat product.Displays the average overall satisfaction score of all the surv eys submitted during the last 12 months up to\na selected dat.*-> Shows data as soon as the data is updated in the data store .\nEnsures that the report and the semantic model only contain dat a from the current and previous year.\n*-> Ensures that the report respects any table-level security s pecified in the source data store.\n*-> Minimizes the execution time of report queries.", "reference": "Explanation:\nBox 1: Single sign-on (SSO) authentication\nAuthentication method:\nNeed to be able to map the user roles to the model table-level security.\nScenario:\nReport:* Ensure that the report respects any table-level security spec ified in the source data store.\nNote: Embed a report with token-based identity (SSO)\nThe token-based identity allows an ISV to use a Microsoft Entra access token to pass the identity of a\ncustomer to an Azure SQL database managed in the customer's ten ant.\nISV customers that keep and manage their data in Azure SQL Data base can keep their data secure in their\ntenant when integrating with Power BI Embedded in the ISV app.\nWhen generating the embed token, specify the identity of the us er in Azure SQL by passing that user's\nMicrosoft Entra access token for the Azure SQL server. The acce ss token is then used to pull only the\nrelevant data for that user from Azure SQL, for that specific s ession.\nBox 2: Direct Lake\nMode: Scenario: Shows data as soon as the data is updated in the data store.Minimizes the execution time of report queries.\nDirect Lake mode is a groundbreaking semantic model capability for analyzing very large data volumes in\nPower BI. Direct Lake is based on loading parquet-formatted fil es directly from a data lake without having\nto query a Lakehouse or Warehouse endpoint, and without having to import or duplicate data into a Power\nBI model. Direct Lake is a fast-path to load the data from the lake straight into the Power BI engine, ready\nfor analysis. The following diagram shows how classic import an d DirectQuery modes compare with Direct\nLake mode.\nIn DirectQuery mode, the Power BI engine queries the data at th e source, which can be slow but avoids\nhaving to copy the data like with import mode. Any changes at t he data source are immediately reflected in\nthe query results.\nOn the other hand, with import mode, performance can be better because the data is cached and\n\noptimized for DAX and MDX report queries without having to tran slate and pass SQL or other types of\nqueries to the data source. However, the Power BI engine must f irst copy any new data into the model\nduring refresh. Any changes at the source are only picked up wi th the next model refresh.\nDirect Lake mode eliminates the import requirement by loading t he data directly from OneLake. Unlike\nDirectQuery, there is no translation from DAX or MDX to other q uery languages or query execution on\nother database systems, yielding performance similar to import mode. Because there's no explicit import\nprocess, it's possible to pick up any changes at the data sourc e as they occur, combining the advantages\nof both DirectQuery and import modes while avoiding their disad vantages. Direct Lake mode can be the\nideal choice for analyzing very large models and models with fr equent updates at the data source.\nNote Scenario:\nReport Requirements\nThe data analysts must create a customer satisfaction report th at meets the following requirements:\nEnables a user to select a product to filter customer survey re sponses to only those who have purchased\nthat product.Displays the average overall satisfaction score of all the surv eys submitted during the last 12 months up to\na selected dat.*-> Shows data as soon as the data is updated in the data store .\nEnsures that the report and the semantic model only contain dat a from the current and previous year.\n*-> Ensures that the report respects any table-level security s pecified in the source data store.\n*-> Minimizes the execution time of report queries.\nReference:\nhttps://learn.microsoft.com/en-us/power-bi/developer/embedded/r ls-sso\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/directlak e-overview\n\nImplement and manage semantic models\nTestlet 2Case studyThis is a case study. Case studies are not timed separately. You can use as much exam time as you\nwould like to complete each case. However, there may be additional case studies and sections on this\nexam. You must manage your time to ensure that you are able to complete all questions included on this\nexam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in\nthe case study. Case studies might contain exhibits and other r esources that provide more information\nabout the scenario that is described in the case study. Each qu estion is independent of the other questions\nin this case study.\nAt the end of this case study, a review screen will appear. Thi s screen allows you to review your answers\nand to make changes before you move to the next section of the exam. After you begin a new section, you\ncannot return to this section.\nTo start the case study\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to\nexplore the content of the case study before you answer the que stions. Clicking these buttons displays\ninformation such as business requirements, existing environment , and problem statements. If the case\nstudy has an All Information tab, note that the information displayed is identical to the i nformation\ndisplayed on the subsequent tabs. When you are ready to answer a question, click the Question button to\nreturn to the question.\nOverview\nContoso, Ltd. is a US-based health supplements company. Contoso has two divisions named Sales and\nResearch. The Sales division contains two departments named Onl ine Sales and Retail Sales. The\nResearch division assigns internally developed product lines to individual teams of researchers and\nanalysts.\nExisting Environment\nIdentity Environment\nContoso has a Microsoft Entra tenant named contoso.com. The ten ant contains two groups named\nResearchReviewersGroup1 and ResearchReviewersGroup2.\nData Environment\nContoso has the following data environment:\nThe Sales division uses a Microsoft Power BI Premium capacity.\nThe semantic model of the Online Sales department includes a fa ct table named Orders that uses\nImport mode. In the system of origin, the OrderID value represe nts the sequence in which orders are\ncreated.\nThe Research department uses an on-premises, third-party data w arehousing product.\nFabric is enabled for contoso.com.\nAn Azure Data Lake Storage Gen2 storage account named storage1 contains Research division data\nfor a product line named Productline1. The data is in the delta format.\nA Data Lake Storage Gen2 storage account named storage2 contain s Research division data for a\nproduct line named Productline2. The data is in the CSV format.\nRequirements\nPlanned Changes\nContoso plans to make the following changes:\n\nEnable support for Fabric in the Power BI Premium capacity used by the Sales division.\nMake all the data for the Sales division and the Research divis ion available in Fabric.\nFor the Research division, create two Fabric workspaces named P roductline1ws and Productline2ws.\nIn Productline1ws, create a lakehouse named Lakehouse1.\nIn Lakehouse1, create a shortcut to storage1 named ResearchProd uct.\nData Analytics Requirements\nContoso identifies the following data analytics requirements:\nAll the workspaces for the Sales division and the Research divi sion must support all Fabric\nexperiences.\nThe Research division workspaces must use a dedicated, on-deman d capacity that has per-minute\nbilling.\nThe Research division workspaces must be grouped together logic ally to support OneLake data hub\nfiltering based on the department name.\nFor the Research division workspaces, the members of ResearchRe viewersGroup1 must be able to\nread lakehouse and warehouse data and shortcuts by using SQL en dpoints.\nFor the Research division workspaces, the members of ResearchRe viewersGroup2 must be able to\nread lakehouse data by using Lakehouse explorer.\nAll the semantic models and reports for the Research division m ust use version control that supports\nbranching.\nData Preparation Requirements\nContoso identifies the following data preparation requirements:\nThe Research division data for Productline2 must be retrieved f rom Lakehouse1 by using Fabric\nnotebooks.\nAll the Research division data in the lakehouses must be presen ted as managed tables in Lakehouse\nexplorer.\nSemantic Model Requirements\nContoso identifies the following requirements for implementing and managing semantic models:\nThe number of rows added to the Orders table during refreshes m ust be minimized.\nThe semantic models in the Research division workspaces must us e Direct Lake mode.\nGeneral Requirements\nContoso identifies the following high-level requirements that m ust be considered for all solutions:\nFollow the principle of least privilege when applicable.\nMinimize implementation and maintenance effort when possible.", "boxes": [{"box": 1, "value": "Single sign-on (SSO) authentication"}, {"box": 2, "value": "Direct Lake"}], "images": ["images/p182.jpg", "images/p183.jpg", "images/p185.jpg"]}, {"id": 140, "shown": "1", "type": "mc", "stem": "What should you use to implement calculation groups for the Res earch division semantic models?", "options": [{"key": "A", "text": "Microsoft Power BI Desktop"}, {"key": "B", "text": "the Power BI service"}, {"key": "C", "text": "DAX Studio"}, {"key": "D", "text": "Tabular Editor"}], "correct": "D", "multi": false, "explanation": "", "reference": "Implement and manage semantic models\nQuestion Set 3", "boxes": [], "image": "images/p186.jpg"}, {"id": 141, "shown": "1", "type": "mc", "stem": "You have a Fabric tenant that contains a workspace named Worksp ace1. Workspace1 is assigned to a\nFabric capacity.\nYou need to recommend a solution to provide users with the abil ity to create and publish custom Direct\nLake semantic models by using external tools. The solution must  follow the principle of least privilege.\nWhich three actions in the Fabric Admin portal should you inclu de in the recommendation? Each correct\nanswer presents part of the solution.\nNOTE:  Each correct answer is worth one point.\nto Enabled.\nEnabled .", "options": [{"key": "A", "text": "From the Tenant settings, set Allow XMLA Endpoints and Analyz e in Excel with on-premises datasets"}, {"key": "B", "text": "From the Tenant settings, set Allow Microsoft Entra ID guest users to access Microsoft Fabric to"}, {"key": "C", "text": "From the Tenant settings, select Users can edit data model in the Power BI service ."}, {"key": "D", "text": "From the Capacity settings, set XMLA Endpoint to Read Write ."}, {"key": "E", "text": "From the Tenant settings, set Users can create Fabric items to Enabled ."}, {"key": "F", "text": "From the Tenant settings, enable Publish to Web ."}], "correct": "ADE", "multi": true, "explanation": "", "reference": "", "boxes": []}, {"id": 142, "shown": "2", "type": "mc", "stem": "You are creating a semantic model in Microsoft Power BI Desktop .\nYou plan to make bulk changes to the model by using the Tabular  Model Definition Language (TMDL)\nextension for Microsoft Visual Studio Code.\nYou need to save the semantic model to a file.Which file format should you use?", "options": [{"key": "A", "text": "PBIP"}, {"key": "B", "text": "PBIX"}, {"key": "C", "text": "PBIT"}, {"key": "D", "text": "PBIDS"}], "correct": "A", "multi": false, "explanation": "", "reference": "", "boxes": []}, {"id": 143, "shown": "3", "type": "mc", "stem": "You have a Fabric tenant that contains 30 CSV files in OneLake.  The files are updated daily.\nYou create a Microsoft Power BI semantic model named Model1 tha t uses the CSV files as a data source.\nYou configure incremental refresh for Model1 and publish the mo del to a Premium capacity in the Fabric\ntenant.\nWhen you initiate a refresh of Model1, the refresh fails after running out of resources.\nWhat is a possible cause of the failure?", "options": [{"key": "A", "text": "Query folding is occurring."}, {"key": "B", "text": "Only refresh complete days  is selected."}, {"key": "C", "text": "XMLA Endpoint is set to Read Only ."}, {"key": "D", "text": "Query folding is NOT  occurring."}, {"key": "E", "text": "The data type of the column used to partition the data has ch anged."}], "correct": "D", "multi": false, "explanation": "Incremental refresh and real-time data for semantic models, Tro ubleshoot incremental refresh and real-\ntime data\nD (not A): Most problems that occur when configuring incrementa l refresh and real-time data have to do\nwith query folding. Your data source must support query folding .\nIf the incremental refresh policy includes getting real-time da ta with DirectQuery, non-folding\ntransformations can't be used.\nBecause support for query folding is different for different ty pes of data sources, verification should be\nperformed to ensure the filter logic is included in the queries being run against the data source.\nNote: Cause: Data type mismatch\nThis issue can be caused by a data type mismatch where Date/Tim e is the required data type for the\nRangeStart and RangeEnd parameters, but the table date column o n which the filters are applied aren't\nDate/Time data type, or vice-versa. Both the parameters data ty pe and the filtered data column must be\nDate/Time data type and the format must be the same. If not, th e query can't be folded.\nIncorrect:\nNot B: The Only refresh complete days setting ensures all rows for the entire day are included in the\nrefresh operation.", "reference": "Explanation:\nIncremental refresh and real-time data for semantic models, Tro ubleshoot incremental refresh and real-\ntime data\nD (not A): Most problems that occur when configuring incrementa l refresh and real-time data have to do\nwith query folding. Your data source must support query folding .\nIf the incremental refresh policy includes getting real-time da ta with DirectQuery, non-folding\ntransformations can't be used.\nBecause support for query folding is different for different ty pes of data sources, verification should be\nperformed to ensure the filter logic is included in the queries being run against the data source.\nNote: Cause: Data type mismatch\nThis issue can be caused by a data type mismatch where Date/Tim e is the required data type for the\nRangeStart and RangeEnd parameters, but the table date column o n which the filters are applied aren't\nDate/Time data type, or vice-versa. Both the parameters data ty pe and the filtered data column must be\nDate/Time data type and the format must be the same. If not, th e query can't be folded.\nIncorrect:\nNot B: The Only refresh complete days setting ensures all rows for the entire day are included in the\nrefresh operation.\nReference:\nhttps://learn.microsoft.com/en-us/power-bi/connect-data/increme ntal-refresh-troubleshoot\nhttps://learn.microsoft.com/en-us/power-bi/connect-data/increme ntal-refresh-overview", "boxes": []}, {"id": 144, "shown": "4", "type": "mc", "stem": "You have a Fabric tenant that uses a Microsoft Power BI Premium  capacity.\nYou need to enable scale-out for a semantic model.What should you do first?", "options": [{"key": "A", "text": "At the semantic model level, set Large Semantic model storage f ormat to Off."}, {"key": "B", "text": "At the tenant level, set Create and use Metrics to Enabled ."}, {"key": "C", "text": "At the semantic model level, set Large Semantic model storage f ormat to On."}, {"key": "D", "text": "At the tenant level, set Data Activator to Enabled ."}], "correct": "C", "multi": false, "explanation": "Power BI semantic model scale-out\nPrerequisites\nBy default, scale-out is enabled for your tenant, but it's not enabled for semantic models in your tenant. To\nenable scale-out for a semantic model, you must use the Power B I REST APIs. Before enabling, the\n\nfollowing prerequisites must be met:\n* The Scale-out queries for large semantic models setting for y our tenant is enabled (default).\n* Your workspace resides on a Power BI Premium capacity*-> The Large semantic model storage format setting is enabled .\n* Etc. Note: Power BI semantic models can store data in a highly compr essed in-memory cache for optimized\nquery performance, enabling fast user interactivity. With Premi um capacities, large semantic models\nbeyond the default limit can be enabled with the Large semantic model storage format setting. When\nenabled, semantic model size is limited by the Premium capacity size or the maximum size set by the\nadministrator.\nEnable large semantic models\nSteps here describe enabling large semantic models for a new mo del published to the service. For existing\nsemantic models, only step 3 is necessary.\n1. Create a model in Power BI Desktop. If your semantic model w ill become larger and progressively\nconsume more memory, be sure to configure Incremental refresh.\n2. Publish the model as a semantic model to the service.3. In the service > semantic model > Settings, expand Large sem antic model storage format, set the slider\nto On, and then select Apply.\n4. Invoke a refresh to load historical data based on the increm ental refresh policy. The first refresh could\ntake a while to load the history. Subsequent refreshes should b e faster, depending on your incremental\nrefresh policy.", "reference": "Explanation:\nPower BI semantic model scale-out\nPrerequisites\nBy default, scale-out is enabled for your tenant, but it's not enabled for semantic models in your tenant. To\nenable scale-out for a semantic model, you must use the Power B I REST APIs. Before enabling, the\n\nfollowing prerequisites must be met:\n* The Scale-out queries for large semantic models setting for y our tenant is enabled (default).\n* Your workspace resides on a Power BI Premium capacity*-> The Large semantic model storage format setting is enabled .\n* Etc. Note: Power BI semantic models can store data in a highly compr essed in-memory cache for optimized\nquery performance, enabling fast user interactivity. With Premi um capacities, large semantic models\nbeyond the default limit can be enabled with the Large semantic model storage format setting. When\nenabled, semantic model size is limited by the Premium capacity size or the maximum size set by the\nadministrator.\nEnable large semantic models\nSteps here describe enabling large semantic models for a new mo del published to the service. For existing\nsemantic models, only step 3 is necessary.\n1. Create a model in Power BI Desktop. If your semantic model w ill become larger and progressively\nconsume more memory, be sure to configure Incremental refresh.\n2. Publish the model as a semantic model to the service.3. In the service > semantic model > Settings, expand Large sem antic model storage format, set the slider\nto On, and then select Apply.\n4. Invoke a refresh to load historical data based on the increm ental refresh policy. The first refresh could\ntake a while to load the history. Subsequent refreshes should b e faster, depending on your incremental\nrefresh policy.\nReference:\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-p remium-large-models\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-p remium-scale-out", "boxes": []}, {"id": 145, "shown": "5", "type": "mc", "stem": "You have a Fabric tenant that contains a warehouse. The warehou se uses row-level security (RLS).\nYou create a Direct Lake semantic model that uses the Delta tab les and RLS of the warehouse.\nWhen users interact with a report built from the model, which m ode will be used by the DAX queries?", "options": [{"key": "A", "text": "DirectQuery"}, {"key": "B", "text": "Dual"}, {"key": "C", "text": "Direct Lake"}, {"key": "D", "text": "Import"}], "correct": "C", "multi": false, "explanation": "", "reference": "", "boxes": [], "image": "images/p189.jpg"}, {"id": 146, "shown": "6", "type": "mc", "stem": "You have a Fabric tenant that contains a complex semantic model . The model is based on a star schema\nand contains many tables, including a fact table named Sales.\nYou need to create a diagram of the model. The diagram must con tain only the Sales table and related\ntables.\nWhat should you use from Microsoft Power BI Desktop?", "options": [{"key": "A", "text": "data categories"}, {"key": "B", "text": "Data view"}, {"key": "C", "text": "Model view"}, {"key": "D", "text": "DAX query view"}], "correct": "C", "multi": false, "explanation": "Model view in Power BI DesktopModel view shows all of the tables, columns, and relationships in your model. This view can be especially\nhelpful when your model has complex relationships between many tables.\nSelect the Model view icon near the side of the window to see a view of the existing model. Hover your\ncursor over a relationship line to show the columns used.", "reference": "Explanation:\nModel view in Power BI DesktopModel view shows all of the tables, columns, and relationships in your model. This view can be especially\nhelpful when your model has complex relationships between many tables.\nSelect the Model view icon near the side of the window to see a view of the existing model. Hover your\ncursor over a relationship line to show the columns used.\nReference:https://learn.microsoft.com/en-us/power-bi/transform-model/desk top-relationship-view", "boxes": [], "image": "images/p190.jpg"}, {"id": 147, "shown": "7", "type": "hotspot", "stem": "You have the source data model shown in the following exhibit.\nThe primary keys of the tables are indicated by a key symbol be side the columns involved in each key.\nYou need to create a dimensional data model that will enable th e analysis of order items by date, product,\nand customer.\nWhat should you include in the solution? To answer, select the appropriate options in the answer area.\nNOTE:  Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: Both the CompanyID and the productID columns.\nThe relationship between OrderItem and Product must be based on :\nNeed to enable the analysis of order items by date, product, an d customer.\nIncorrect:\n* The productID column.Need the CompanyID column as well.\nBox 2: Denormalized in the Customer and Product entities\nThe Company entity must be:\nBoth the Customer and the Product tables use CompanyID as part of their primary key.", "reference": "Explanation:\nBox 1: Both the CompanyID and the productID columns.\nThe relationship between OrderItem and Product must be based on :\nNeed to enable the analysis of order items by date, product, an d customer.\nIncorrect:\n* The productID column.Need the CompanyID column as well.\nBox 2: Denormalized in the Customer and Product entities\nThe Company entity must be:\nBoth the Customer and the Product tables use CompanyID as part of their primary key.", "boxes": [{"box": 1, "value": "Both the CompanyID and the productID columns."}, {"box": 2, "value": "Denormalized in the Customer and Product entities"}], "images": ["images/p190.jpg", "images/p191.jpg"]}, {"id": 148, "shown": "8", "type": "mc", "stem": "You have a Fabric tenant that contains a semantic model.\nYou need to prevent report creators from populating visuals by using implicit measures.\nWhat are two tools that you can use to achieve the goal? Each c orrect answer presents a complete\nsolution.\nNOTE:  Each correct answer is worth one point.", "options": [{"key": "A", "text": "Microsoft Power BI Desktop"}, {"key": "B", "text": "Tabular Editor"}, {"key": "C", "text": "Microsoft SQL Server Management Studio (SSMS)"}, {"key": "D", "text": "DAX Studio"}], "correct": "AB", "multi": true, "explanation": "", "reference": "", "boxes": [], "image": "images/p192.jpg"}, {"id": 149, "shown": "9", "type": "mc", "stem": "You need to create a data loading pattern for a Type 1 slowly c hanging dimension (SCD).\nWhich two actions should you include in the process? Each corre ct answer presents part of the solution.\nNOTE:  Each correct answer is worth one point.\nhave changed.", "options": [{"key": "A", "text": "Update rows when the non-key attributes have changed."}, {"key": "B", "text": "Insert new rows when the natural key exists in the dimension table, and the non-key attribute values"}, {"key": "C", "text": "Update the effective end date of rows when the non-key attrib ute values have changed."}, {"key": "D", "text": "Insert new records when the natural key is a new value in the  table."}], "correct": "AD", "multi": true, "explanation": "", "reference": "", "boxes": [], "image": "images/p192.jpg"}, {"id": 150, "shown": "10", "type": "mc", "stem": "You have a Microsoft Power BI semantic model.\nYou need to identify any surrogate key columns in the model tha t have the Summarize By property set to a\nvalue other than to None. The solution must minimize effort.\nWhat should you use?", "options": [{"key": "A", "text": "DAX Formatter in DAX Studio"}, {"key": "B", "text": "Model explorer in Microsoft Power BI Desktop"}, {"key": "C", "text": "Model view in Microsoft Power BI Desktop"}, {"key": "D", "text": "Best Practice Analyzer in Tabular Editor"}], "correct": "D", "multi": false, "explanation": "BPA lets you define rules on the metadata of your model, to enc ourage certain conventions and best\npractices while developing in SSAS Tabular.\nClicking one of the rules in the top list, will show you all ob jects that satisfy the conditions of the given rule\nin the bottom list:\nNote: The Best Practice Analyzer (BPA) lets you define rules on the metadata of your model, to encourage\ncertain conventions and best practices while developing your Po wer BI or Analysis Services Model.\nPBA Overview\nThe BPA overview shows you all the rules defined in your model that are currently being broken:\n\nIncorrect:\n* DAX Formatter in DAX StudioDAX Formatter can be used within DAX Studio to align parenthese s with their associated functions.\n* Model explorer in Microsoft Power BI Desktop\nWith Model explorer in the Model view in Power BI, you can view and work with complex semantic models\nwith many tables, relationships, measures, roles, calculation g roups, translations, and perspectives.\n* Model view in Microsoft Power BI Desktop\nModel view shows all of the tables, columns, and relationships in your model. This view can be especially\nhelpful when your model has complex relationships between many tables.", "reference": "Explanation:\nBPA lets you define rules on the metadata of your model, to enc ourage certain conventions and best\npractices while developing in SSAS Tabular.\nClicking one of the rules in the top list, will show you all ob jects that satisfy the conditions of the given rule\nin the bottom list:\nNote: The Best Practice Analyzer (BPA) lets you define rules on the metadata of your model, to encourage\ncertain conventions and best practices while developing your Po wer BI or Analysis Services Model.\nPBA Overview\nThe BPA overview shows you all the rules defined in your model that are currently being broken:\n\nIncorrect:\n* DAX Formatter in DAX StudioDAX Formatter can be used within DAX Studio to align parenthese s with their associated functions.\n* Model explorer in Microsoft Power BI Desktop\nWith Model explorer in the Model view in Power BI, you can view and work with complex semantic models\nwith many tables, relationships, measures, roles, calculation g roups, translations, and perspectives.\n* Model view in Microsoft Power BI Desktop\nModel view shows all of the tables, columns, and relationships in your model. This view can be especially\nhelpful when your model has complex relationships between many tables.\n\nReference:\nhttps://docs.tabulareditor.com/te2/Best-Practice-Analyzer.html\nhttps://docs.tabulareditor.com/common/using-bpa.html?tabs=TE3Ru les\nhttps://learn.microsoft.com/en-us/power-bi/transform-model/mode l-explorer", "boxes": [], "images": ["images/p193.jpg", "images/p194.jpg"]}, {"id": 151, "shown": "11", "type": "mc", "stem": "You have a Fabric tenant that contains a semantic model. The mo del contains 15 tables.\nYou need to programmatically change each column that ends in th e word Key to meet the following\nrequirements:\nHide the column.\nSet Nullable to False\nSet Summarize By to None.\nSet Available in MDX to False.\nMark the column as a key column.\nWhat should you use?", "options": [{"key": "A", "text": "Microsoft Power BI Desktop"}, {"key": "B", "text": "ALM Toolkit"}, {"key": "C", "text": "Tabular Editor"}, {"key": "D", "text": "DAX Studio"}], "correct": "C", "multi": false, "explanation": "Tabular Editor can be a helpful tool for managing datasets depl oyed to Fabric. With Tabular Editor, you\ncan connect to and manage different types of datasets from a si ngle interface. This is ideal for supporting\nand auditing data models, such as in a managed self-service BI environment.\nEnhance productivity: Tabular Editor contains features that hel p you write DAX, manage your dataset and\neven automate and scale development, programmatically.\n\n* Manage tables and columns: While most people use Tabular Edit or to manage DAX, you can also\nmanage data tables and columns. In Tabular Editor, you can view and edit Power Query code, and even\nautomatically detect schema changes in the data source. With th e Table Import Wizard, you can add new\ntables from supported sources like Power BI dataflows, SQL Serv er (or Serverless Pools) and Databricks,\nas Tabular Editor automatically generates the appropriate Power Query code and metadata for you.\nFinally, you can refresh selected tables to view any changes, w ith the ability to track refresh performance\nin real-time or even cancel and pause refreshes with the user i nterface.\n* Etc. Incorrect:\nNot B: ALM Toolkit is a free and open-source tool to manage Mic rosoft Power BI datasets: Database\ncompare, Code merging, Easy deployment, Source-control integrat ion, Reuse definitions, Self-service to\ncorporate BI.It is based on the source code of BISM Normalizer, which provid es similar features for Tabular models.", "reference": "Explanation:\nTabular Editor can be a helpful tool for managing datasets depl oyed to Fabric. With Tabular Editor, you\ncan connect to and manage different types of datasets from a si ngle interface. This is ideal for supporting\nand auditing data models, such as in a managed self-service BI environment.\nEnhance productivity: Tabular Editor contains features that hel p you write DAX, manage your dataset and\neven automate and scale development, programmatically.\n\n* Manage tables and columns: While most people use Tabular Edit or to manage DAX, you can also\nmanage data tables and columns. In Tabular Editor, you can view and edit Power Query code, and even\nautomatically detect schema changes in the data source. With th e Table Import Wizard, you can add new\ntables from supported sources like Power BI dataflows, SQL Serv er (or Serverless Pools) and Databricks,\nas Tabular Editor automatically generates the appropriate Power Query code and metadata for you.\nFinally, you can refresh selected tables to view any changes, w ith the ability to track refresh performance\nin real-time or even cancel and pause refreshes with the user i nterface.\n* Etc. Incorrect:\nNot B: ALM Toolkit is a free and open-source tool to manage Mic rosoft Power BI datasets: Database\ncompare, Code merging, Easy deployment, Source-control integrat ion, Reuse definitions, Self-service to\ncorporate BI.It is based on the source code of BISM Normalizer, which provid es similar features for Tabular models.\nReference:\nhttps://blog.tabulareditor.com/2023/07/13/using-tabular-editor- in-microsoft-fabric", "boxes": [], "image": "images/p195.jpg"}, {"id": 152, "shown": "12", "type": "hotspot", "stem": "You have a Microsoft Power BI semantic model.You plan to implement calculation groups.You need to create a calculation item that will change the cont ext from the selected date to month-to-date\n(MTD).\nHow should you complete the DAX expression? To answer, select t he appropriate options in the answer\narea.\nNOTE : Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: CALCULATE\nThe following calculation item expression calculates the year-t o-date for whatever the measure is in\ncontext.\nCALCULATE(SELECTEDMEASURE(), DATESYTD(DimDate[Date]))\nNote: CALCULATE\nEvaluates an expression in a modified filter context.\nSyntax\nDAXCALCULATE(<expression>[, <filter1> [, <filter2> [, …]]])\nBox 2: SELECTEDMEASURE\nIncorrect:\n* GENERATEReturns a table with the Cartesian product between each row in table1 and the table that results from\nevaluating table2 in the context of the current row from table1 .\nSyntax\nGENERATE(<table1>, <table2>)\n* MEASURE\nIntroduces a measure definition in a DEFINE statement of a DAX query.\nSyntax\n [DEFINE ( MEASURE <table name>[<measure name>] = <scalar expression >\n ) + ]\n(EVALUATE <table expression>) +\nParameters\n\n* SELECTEDVALUE\nReturns the value when the context for columnName has been filt ered down to one distinct value only.\nOtherwise returns alternateResult.\nSyntax\nSELECTEDVALUE(<columnName>[, <alternateResult>])\nNote: DATESMTD\nReturns a table that contains a column of the dates for the mon th to date, in the current context.\nSyntax\nDATESMTD(<dates>)", "reference": "Explanation:\nBox 1: CALCULATE\nThe following calculation item expression calculates the year-t o-date for whatever the measure is in\ncontext.\nCALCULATE(SELECTEDMEASURE(), DATESYTD(DimDate[Date]))\nNote: CALCULATE\nEvaluates an expression in a modified filter context.\nSyntax\nDAXCALCULATE(<expression>[, <filter1> [, <filter2> [, …]]])\nBox 2: SELECTEDMEASURE\nIncorrect:\n* GENERATEReturns a table with the Cartesian product between each row in table1 and the table that results from\nevaluating table2 in the context of the current row from table1 .\nSyntax\nGENERATE(<table1>, <table2>)\n* MEASURE\nIntroduces a measure definition in a DEFINE statement of a DAX query.\nSyntax\n [DEFINE ( MEASURE <table name>[<measure name>] = <scalar expression >\n ) + ]\n(EVALUATE <table expression>) +\nParameters\n\n* SELECTEDVALUE\nReturns the value when the context for columnName has been filt ered down to one distinct value only.\nOtherwise returns alternateResult.\nSyntax\nSELECTEDVALUE(<columnName>[, <alternateResult>])\nNote: DATESMTD\nReturns a table that contains a column of the dates for the mon th to date, in the current context.\nSyntax\nDATESMTD(<dates>)\nReference:\nhttps://learn.microsoft.com/en-us/dax/selectedmeasure-function- dax", "boxes": [{"box": 1, "value": "CALCULATE"}, {"box": 2, "value": "SELECTEDMEASURE"}], "images": ["images/p196.jpg", "images/p197.jpg"]}, {"id": 153, "shown": "13", "type": "mc", "stem": "You have a Microsoft Power BI report named Report1 that uses a Fabric semantic model.\nUsers discover that Report1 renders slowly.\nYou open Performance analyzer  and identify that a visual named Orders By Date is the slowest  to render.\nThe duration breakdown for Orders By Date is shown in the follo wing table.\nWhat will provide the greatest reduction in the rendering durat ion of Report1?", "options": [{"key": "A", "text": "Enable automatic page refresh."}, {"key": "B", "text": "Optimize the DAX query of Orders By Date by using DAX Studio."}, {"key": "C", "text": "Change the visual type of Orders By Date."}, {"key": "D", "text": "Reduce the number of visuals in Report1."}], "correct": "D", "multi": false, "explanation": "Use Performance Analyzer to examine report element performance in Power BI Desktop\nEach visual's log information includes the time spent (duration ) to complete the following categories of\ntasks:\nDAX query - if a DAX query was required, this is the time betwe en the visual sending the query, and for\nAnalysis Services to return the results.\nVisual display - time required for the visual to draw on the sc reen, including time required to retrieve any\nweb images or geocoding.\nOther - time required by the visual for preparing queries, wait ing for other visuals to complete, or\nperforming other background processing.", "reference": "Explanation:\nUse Performance Analyzer to examine report element performance in Power BI Desktop\nEach visual's log information includes the time spent (duration ) to complete the following categories of\ntasks:\nDAX query - if a DAX query was required, this is the time betwe en the visual sending the query, and for\nAnalysis Services to return the results.\nVisual display - time required for the visual to draw on the sc reen, including time required to retrieve any\nweb images or geocoding.\nOther - time required by the visual for preparing queries, wait ing for other visuals to complete, or\nperforming other background processing.\nReference:\nhttps://learn.microsoft.com/en-us/power-bi/create-reports/deskt op-performance-analyzer", "boxes": [], "image": "images/p198.jpg"}, {"id": 154, "shown": "14", "type": "mc", "stem": "You have a custom Direct Lake semantic model named Model1 that has one billion rows of data.\nYou use Tabular Editor to connect to Model1 by using the XMLA e ndpoint.\nYou need to ensure that when users interact with reports based on Model1, their queries always use Direct\nLake mode.\nWhat should you do?", "options": [{"key": "A", "text": "From Model, configure the Default Mode option."}, {"key": "B", "text": "From Partitions, configure the Mode option."}, {"key": "C", "text": "From Model, configure the Storage Location option."}, {"key": "D", "text": "From Model, configure the Direct Lake Behavior option."}], "correct": "D", "multi": false, "explanation": "New Behavior PropertyWith the update to the latest TOM library a new Fabric-only pro perty is available in model properties. Direct\nLake Behavior allows control over whether the model should fall back to DirectQuery or not.\nNote: Power BI Service:\nAs of Feb 21, 2024, Web modeling experience in Power BI service has the UI option to change the Direct\nLake fallback behavior. Default is Automatic.\n\nTabular Editor:\nUpgrade to the latest version of Tabular Editor 2 (v2.21.1) whi ch has the latest AMO/TOM properties. Link\nto Tabular Editor.\nConnect to the Direct Lake semantic model using XMLA endpointSelect Model > Under Options > Direct Lake Behaviour > Change f rom Automatic to DirectLakeOnly\nSave the model", "reference": "Explanation:\nNew Behavior PropertyWith the update to the latest TOM library a new Fabric-only pro perty is available in model properties. Direct\nLake Behavior allows control over whether the model should fall back to DirectQuery or not.\nNote: Power BI Service:\nAs of Feb 21, 2024, Web modeling experience in Power BI service has the UI option to change the Direct\nLake fallback behavior. Default is Automatic.\n\nTabular Editor:\nUpgrade to the latest version of Tabular Editor 2 (v2.21.1) whi ch has the latest AMO/TOM properties. Link\nto Tabular Editor.\nConnect to the Direct Lake semantic model using XMLA endpointSelect Model > Under Options > Direct Lake Behaviour > Change f rom Automatic to DirectLakeOnly\nSave the model\nReference:\nhttps://blog.tabulareditor.com/2023/11/27/tabular-editor-3-nove mber-2023-release/\n\nhttps://fabric.guru/controlling-direct-lake-fallback-behavior", "boxes": [], "images": ["images/p198.jpg", "images/p199.jpg", "images/p200.jpg"]}, {"id": 155, "shown": "15", "type": "dragdrop", "stem": "You create a semantic model by using Microsoft Power BI Desktop . The model contains one security role\nnamed SalesRegionManager and the following tables:\nSales\nSalesRegion\nSalesAddress\nYou need to modify the model to ensure that users assigned the SalesRegionManager role cannot see a\ncolumn named Address in SalesAddress.\nWhich three actions should you perform in sequence? To answer, move the appropriate actions from the\nlist of actions to the answer area and arrange them in the corr ect order.\nSelect and Place:", "options": [], "correct": "", "multi": false, "explanation": "Note: Power Platform, Power BI, Object level security (OLS)\nObject-level security (OLS) enables model authors to secure spe cific tables or columns from report\nviewers. For example, a column that includes personal data can be restricted so that only certain viewers\ncan see and interact with it. In addition, you can also restric t object names and metadata. This added layer\nof security prevents users without the appropriate access level s from discovering business critical or\nsensitive personal information like employee or financial recor ds. For viewers that don’t have the required\npermission, it's as if the secured tables or columns don't exis t.\nStep 1: Open the model in Tabular Editor\nConfigure object level security using tabular editor1. In Power BI Desktop, create the model that will define your OLS rules.\n2. On the External Tools ribbon, select Tabular Editor. If you don’t see the Tabular Editor button, install the\nprogram. When open, Tabular Editor will automatically connect t o your model.\nStep 2: Select the Address column in SalesAddress\n\n3. In the Model view, select the drop-down menu under Roles.\n4. Select the role you want to enable an OLS definition for, an d expand the Table Permissions.\n\nStep 3: Set Object Level Security to None for SalesRegionManage r\n5. Set the permissions for the table or column to None or Read.None: OLS is enforced and the table or column will be hidden fr om that role\nRead: The table or column will be visible to that role\n6. After you define object-level security for the roles, save y our changes.", "reference": "Explanation:\nNote: Power Platform, Power BI, Object level security (OLS)\nObject-level security (OLS) enables model authors to secure spe cific tables or columns from report\nviewers. For example, a column that includes personal data can be restricted so that only certain viewers\ncan see and interact with it. In addition, you can also restric t object names and metadata. This added layer\nof security prevents users without the appropriate access level s from discovering business critical or\nsensitive personal information like employee or financial recor ds. For viewers that don’t have the required\npermission, it's as if the secured tables or columns don't exis t.\nStep 1: Open the model in Tabular Editor\nConfigure object level security using tabular editor1. In Power BI Desktop, create the model that will define your OLS rules.\n2. On the External Tools ribbon, select Tabular Editor. If you don’t see the Tabular Editor button, install the\nprogram. When open, Tabular Editor will automatically connect t o your model.\nStep 2: Select the Address column in SalesAddress\n\n3. In the Model view, select the drop-down menu under Roles.\n4. Select the role you want to enable an OLS definition for, an d expand the Table Permissions.\n\nStep 3: Set Object Level Security to None for SalesRegionManage r\n5. Set the permissions for the table or column to None or Read.None: OLS is enforced and the table or column will be hidden fr om that role\nRead: The table or column will be visible to that role\n6. After you define object-level security for the roles, save y our changes.\nReference:\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-a dmin-ols", "boxes": [], "images": ["images/p201.jpg", "images/p202.jpg"]}, {"id": 156, "shown": "16", "type": "mc", "stem": "You have a semantic model named Model1. Model1 contains five ta bles that all use Import mode. Model1\ncontains a dynamic row-level security (RLS) role named HR. The HR role filters employee data so that HR\nmanagers only see the data of the department to which they are assigned.\nYou publish Model1 to a Fabric tenant and configure RLS role me mbership. You share the model and\nrelated reports to users.\nAn HR manager reports that the data they see in a report is inc omplete.\nWhat should you do to validate the data seen by the HR Manager?", "options": [{"key": "A", "text": "Select Test as role  to view the data as the HR role."}, {"key": "B", "text": "Filter the data in the report to match the intended logic of the filter for the HR department."}, {"key": "C", "text": "Select Test as role  to view the report as the HR manager."}, {"key": "D", "text": "Ask the HR manager to open the report in Microsoft Power BI D esktop."}], "correct": "C", "multi": false, "explanation": "", "reference": "", "boxes": [], "image": "images/p203.jpg"}, {"id": 157, "shown": "17", "type": "hotspot", "stem": "You have a Fabric tenant that contains a semantic model named m odel1. The two largest columns in\nmodel1 are shown in the following table.\nYou need to optimize model1. The solution must meet the followi ng requirements:\nReduce the model size.\nIncrease refresh performance when using Import mode.\nEnsure that the datetime value for each sales transaction is av ailable in the model.\nWhat should you do on each column? To answer, select the approp riate options in the answer area.\nNOTE:  Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: Remove the column.\nTransactionKey\nRemove the surrogate key.Box 2: Split the column\nSaleDateTime\nSplit the datetime column.", "reference": "Explanation:\nBox 1: Remove the column.\nTransactionKey\nRemove the surrogate key.Box 2: Split the column\nSaleDateTime\nSplit the datetime column.\n\nReference:\nhttps://powerbi.microsoft.com/pl-pl/blog/best-practice-rules-to -improve-your-models-performance/", "boxes": [{"box": 1, "value": "Remove the column."}, {"box": 2, "value": "Split the column"}], "images": ["images/p204.jpg", "images/p205.jpg"]}, {"id": 158, "shown": "18", "type": "mc", "stem": "You have a Microsoft Power BI Premium Per User (PPU) workspace that contains a semantic model.\nYou have an Azure App Service app named App1 that modifies row- level security (RLS) for the model by\nusing the XMLA endpoint.\nApp1 requires users to sign in by using their Microsoft Entra c redentials to access the XMLA endpoint.\nYou need to configure App1 to use a service account to access t he model.\nWhat should you do first?", "options": [{"key": "A", "text": "Add a managed identity to the workspace."}, {"key": "B", "text": "Modify the XMLA Endpoint setting."}, {"key": "C", "text": "Upgrade the workspace to Premium capacity."}, {"key": "D", "text": "Add a managed identity to App1."}], "correct": "D", "multi": false, "explanation": "Adding a managed identity to App1 allows the app to authenticat e and access Azure resources securely\nwithout needing to manage credentials. This is particularly use ful for service accounts accessing the XMLA\nendpoint.Once the managed identity is set up for App1, you can then conf igure permissions in your Power BI\nworkspace to allow the managed identity access to the semantic model, facilitating row-level security\nmodifications as needed.", "reference": "Explanation:\nAdding a managed identity to App1 allows the app to authenticat e and access Azure resources securely\nwithout needing to manage credentials. This is particularly use ful for service accounts accessing the XMLA\nendpoint.Once the managed identity is set up for App1, you can then conf igure permissions in your Power BI\nworkspace to allow the managed identity access to the semantic model, facilitating row-level security\nmodifications as needed.", "boxes": [], "image": "images/p206.jpg"}, {"id": 159, "shown": "19", "type": "hotspot", "stem": "You have a Fabric tenant that contains a warehouse named WH1.You have source data in a CSV file that has the following field s:\nSalesTransactionID\nSaleDate\nCustomerCode\nCustomerName\nCustomerAddress\nProductCode\nProductName\nQuantity\nUnitPrice\nYou plan to implement a star schema for the tables in WH1. The dimension tables in WH1 will implement\nType 2 slowly changing dimension (SCD) logic.\nYou need to design the tables that will be used for sales trans action analysis and load the source data.\nWhich type of target table should you specify for the CustomerN ame, CustomerCode, and SaleDate fields?\nTo answer, select the appropriate options in the answer area.\nNOTE : Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "CustomerCode: Dimension - This field serves as a unique identifier for customers, prov iding context in\nthe star schema.\nCustomerName: Dimension - Similar to CustomerCode, this field provides descriptive att ributes about\nthe customer, making it part of the dimension table.\nSaleDate: Dimension - Similar to CustomerCode, this field provides descriptive att ributes about the\ncustomer, making it part of the dimension table.", "reference": "Explanation:\nCustomerCode: Dimension - This field serves as a unique identifier for customers, prov iding context in\nthe star schema.\nCustomerName: Dimension - Similar to CustomerCode, this field provides descriptive att ributes about\nthe customer, making it part of the dimension table.\nSaleDate: Dimension - Similar to CustomerCode, this field provides descriptive att ributes about the\ncustomer, making it part of the dimension table.", "boxes": [], "images": ["images/p206.jpg", "images/p207.jpg"]}, {"id": 160, "shown": "20", "type": "hotspot", "stem": "You have a Fabric tenant that contains a warehouse named Wareho use1. Warehouse1 contains a fact\ntable named FactSales that has one billion rows.\nYou run the following T-SQL statement.\nCREATE TABLE test.FactSales AS CLONE OF dbo.FactSales;\nFor each of the following statements, select Yes if the stateme nt is true. Otherwise, select No.\nNOTE : Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "A replica of dbo.FactSales is created in the test schema by cop ying the metadata only. - No\nThe clone operation creates a full copy of both the schema and the data, not just the metadata.\nAdditional schema changes to dbo.FactSales will also apply to t est.FactSales. - No\nOnce the clone is created, it is an independent copy. Any subse quent schema changes to the original table\nwill not affect the cloned table.\nAdditional data changes to dbo.FactSales will also apply to tes t.FactSales. - No\nSimilarly, the data in dbo.FactSales and test.FactSales are ind ependent after the cloning process.\nChanges in one will not reflect in the other.", "reference": "Explanation:\nA replica of dbo.FactSales is created in the test schema by cop ying the metadata only. - No\nThe clone operation creates a full copy of both the schema and the data, not just the metadata.\nAdditional schema changes to dbo.FactSales will also apply to t est.FactSales. - No\nOnce the clone is created, it is an independent copy. Any subse quent schema changes to the original table\nwill not affect the cloned table.\nAdditional data changes to dbo.FactSales will also apply to tes t.FactSales. - No\nSimilarly, the data in dbo.FactSales and test.FactSales are ind ependent after the cloning process.\nChanges in one will not reflect in the other.", "boxes": [], "image": "images/p208.jpg"}, {"id": 161, "shown": "21", "type": "mc", "stem": "You have a Fabric tenant that contains a data warehouse.\nYou need to load rows into a large Type 2 slowly changing dimen sion (SCD). The solution must minimize\nresource usage.\nWhich T-SQL statement should you use?", "options": [{"key": "A", "text": "UPDATE AND INSERT"}, {"key": "B", "text": "MERGE"}, {"key": "C", "text": "TRUNCATE TABLE and INSERT"}, {"key": "D", "text": "CREATE TABLE AS SELECT"}], "correct": "B", "multi": false, "explanation": "The MERGE statement is designed to perform insert and update operations in a single statement. It allows\nyou to efficiently manage Type 2 SCDs by:Inserting new records for new customers or changes in attribute s.\nUpdating existing records to mark old versions as inactive whil e adding new versions.\nThis approach minimizes resource usage compared to separate upd ate and insert statements, as it\nreduces the need for multiple passes over the data.", "reference": "Explanation:\nThe MERGE statement is designed to perform insert and update operations in a single statement. It allows\nyou to efficiently manage Type 2 SCDs by:Inserting new records for new customers or changes in attribute s.\nUpdating existing records to mark old versions as inactive whil e adding new versions.\nThis approach minimizes resource usage compared to separate upd ate and insert statements, as it\nreduces the need for multiple passes over the data.", "boxes": [], "image": "images/p208.jpg"}, {"id": 162, "shown": "22", "type": "mc", "stem": "You have a Fabric tenant that contains a lakehouse named LH1.\nYou create new tables in LH1.You need to ensure that the tables are added automatically to t he default semantic model.\nWhat should you do?", "options": [{"key": "A", "text": "Disable Query Caching for the default semantic model."}, {"key": "B", "text": "From the settings pane of LH1, enable Sync the default Power BI semantic model."}, {"key": "C", "text": "Enable Refresh for the default semantic model."}, {"key": "D", "text": "From the Endorsement and discovery settings of LH1, select Make discoverable."}], "correct": "B", "multi": false, "explanation": "Enabling the option to sync the default Power BI semantic model ensures that any new tables created in\nthe lakehouse are automatically included in the semantic model. This streamlines the process of\nintegrating data into the Power BI environment without requirin g manual intervention each time a new table\nis added.", "reference": "Explanation:\nEnabling the option to sync the default Power BI semantic model ensures that any new tables created in\nthe lakehouse are automatically included in the semantic model. This streamlines the process of\nintegrating data into the Power BI environment without requirin g manual intervention each time a new table\nis added.", "boxes": []}, {"id": 163, "shown": "23", "type": "mc", "stem": "Note: This section contains one or more sets of questions with the same scenario and problem.\nEach question presents a unique solution to the problem. You mu st determine whether the solution\nmeets the stated goals. More than one solution in the set might  solve the problem. It is also\npossible that none of the solutions in the set solve the proble m.\nAfter you answer a question in this section, you will NOT be ab le to return. As a result, these\nquestions do not appear on the Review Screen.\nYour network contains an on-premises Active Directory Domain Se rvices (AD DS) domain named\ncontoso.com that syncs with a Microsoft Entra tenant by using M icrosoft Entra Connect.\nYou have a Fabric tenant that contains a semantic model.\nYou enable dynamic row-level security (RLS) for the model and d eploy the model to the Fabric service.\nYou query a measure that includes the USERNAME()  function, and the query returns a blank result.\nYou need to ensure that the measure returns the user principal name (UPN) of a user.\nSolution: You update the measure to use the USEROBJECTID()  function.\nDoes this meet the goal?", "options": [{"key": "A", "text": "Yes"}, {"key": "B", "text": "No"}], "correct": "B", "multi": false, "explanation": "This function returns the unique identifier (Object ID) of the user in Azure Active Directory. While this can\nbe useful for certain scenarios, it does not return the UPN, wh ich is what you need.", "reference": "Explanation:\nThis function returns the unique identifier (Object ID) of the user in Azure Active Directory. While this can\nbe useful for certain scenarios, it does not return the UPN, wh ich is what you need.", "boxes": []}, {"id": 164, "shown": "24", "type": "mc", "stem": "Note: This section contains one or more sets of questions with the same scenario and problem.\nEach question presents a unique solution to the problem. You mu st determine whether the solution\nmeets the stated goals. More than one solution in the set might  solve the problem. It is also\npossible that none of the solutions in the set solve the proble m.\nAfter you answer a question in this section, you will NOT be ab le to return. As a result, these\nquestions do not appear on the Review Screen.\nYour network contains an on-premises Active Directory Domain Se rvices (AD DS) domain named\ncontoso.com that syncs with a Microsoft Entra tenant by using M icrosoft Entra Connect.\nYou have a Fabric tenant that contains a semantic model.You enable dynamic row-level security (RLS) for the model and d eploy the model to the Fabric service.\nYou query a measure that includes the USERNAME()  function, and the query returns a blank result.\nYou need to ensure that the measure returns the user principal name (UPN) of a user.\nSolution: You update the measure to use the USERPRINCIPALNAME()  function.\nDoes this meet the goal?", "options": [{"key": "A", "text": "Yes"}, {"key": "B", "text": "No"}], "correct": "A", "multi": false, "explanation": "The USERPRINCIPALNAME() function directly retrieves the UPN of the user querying the m easure. This\nis the most appropriate function to use if your goal is to obta in the UPN, which is the format typically used\nin environments that integrate with Microsoft Entra.", "reference": "Explanation:\nThe USERPRINCIPALNAME() function directly retrieves the UPN of the user querying the m easure. This\nis the most appropriate function to use if your goal is to obta in the UPN, which is the format typically used\nin environments that integrate with Microsoft Entra.", "boxes": []}, {"id": 165, "shown": "25", "type": "mc", "stem": "Note: This section contains one or more sets of questions with the same scenario and problem.\nEach question presents a unique solution to the problem. You mu st determine whether the solution\nmeets the stated goals. More than one solution in the set might  solve the problem. It is also\npossible that none of the solutions in the set solve the proble m.\nAfter you answer a question in this section, you will NOT be ab le to return. As a result, these\nquestions do not appear on the Review Screen.\nYour network contains an on-premises Active Directory Domain Se rvices (AD DS) domain named\ncontoso.com that syncs with a Microsoft Entra tenant by using M icrosoft Entra Connect.\nYou have a Fabric tenant that contains a semantic model.You enable dynamic row-level security (RLS) for the model and d eploy the model to the Fabric service.\nYou query a measure that includes the USERNAME()  function, and the query returns a blank result.\nYou need to ensure that the measure returns the user principal name (UPN) of a user.\nSolution: You add user objects to the list of synced objects in  Microsoft Entra Connect.\nDoes this meet the goal?", "options": [{"key": "A", "text": "Yes"}, {"key": "B", "text": "No"}], "correct": "A", "multi": false, "explanation": "", "reference": "", "boxes": []}, {"id": 166, "shown": "26", "type": "hotspot", "stem": "You have a Fabric tenant that contains the semantic model shown  in the following exhibit.\nUse the drop-down menus to select the answer choice that comple tes each statement based on the\ninformation presented in the graphic.\nNOTE : Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Querying SQL views from the warehouse of the model will cause a fallback to : DirectQuery\nIn Microsoft Fabric, when a semantic model is set to \"Direct La ke Only\" mode, it means it can use the\nDirect Lake connection mode to query the data directly from the lake storage. However, querying SQL\nviews typically requires a DirectQuery fallback because views are not directly accessible through the\nDirect Lake mode.\nRow and column security is : undefined\nThe exhibit does not show any configuration related to Row-Leve l Security (RLS) or Object-Level Security\n(OLS). Since there's no indication of security settings, we ass ume that row and column security is\nundefined.", "reference": "Explanation:\nQuerying SQL views from the warehouse of the model will cause a fallback to : DirectQuery\nIn Microsoft Fabric, when a semantic model is set to \"Direct La ke Only\" mode, it means it can use the\nDirect Lake connection mode to query the data directly from the lake storage. However, querying SQL\nviews typically requires a DirectQuery fallback because views are not directly accessible through the\nDirect Lake mode.\nRow and column security is : undefined\nThe exhibit does not show any configuration related to Row-Leve l Security (RLS) or Object-Level Security\n(OLS). Since there's no indication of security settings, we ass ume that row and column security is\nundefined.", "boxes": [], "image": "images/p212.jpg"}, {"id": 167, "shown": "27", "type": "mc", "stem": "Note: This section contains one or more sets of questions with the same scenario and problem.\nEach question presents a unique solution to the problem. You mu st determine whether the solution\nmeets the stated goals. More than one solution in the set might  solve the problem. It is also\npossible that none of the solutions in the set solve the proble m.\nAfter you answer a question in this section, you will NOT be ab le to return. As a result, these\nquestions do not appear on the Review Screen.\nYour network contains an on-premises Active Directory Domain Se rvices (AD DS) domain named\ncontoso.com that syncs with a Microsoft Entra tenant by using M icrosoft Entra Connect.\nYou have a Fabric tenant that contains a semantic model.You enable dynamic row-level security (RLS) for the model and d eploy the model to the Fabric service.\nYou query a measure that includes the USERNAME()  function, and the query returns a blank result.\nYou need to ensure that the measure returns the user principal name (UPN) of a user.\nSolution: You create a role in the model.Does this meet the goal?", "options": [{"key": "A", "text": "Yes"}, {"key": "B", "text": "No"}], "correct": "B", "multi": false, "explanation": "No, creating a role in the model alone does not ensure that the USERNAME() function returns the user's\n\nUser Principal Name (UPN). To achieve this, you should use the USERPRINCIPALNAME() function in your\nmeasure, as it consistently returns the user's UPN across diffe rent environments.", "reference": "Explanation:\nNo, creating a role in the model alone does not ensure that the USERNAME() function returns the user's\n\nUser Principal Name (UPN). To achieve this, you should use the USERPRINCIPALNAME() function in your\nmeasure, as it consistently returns the user's UPN across diffe rent environments.", "boxes": [], "image": "images/p213.jpg"}, {"id": 168, "shown": "28", "type": "hotspot", "stem": "You have a Microsoft Power BI project that contains a file name d definition.pbir. definition.pbir contains the\nfollowing JSON.\nFor each of the following statements, select Yes if the stateme nt is true. Otherwise, select No.\nNOTE:  Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "definition.pbir is in the PBIR-Legacy format - No\nThe JSON structure indicates a dataset reference ( datasetReference ), which aligns with the newer\nPBIR format. The PBIR-Legacy format uses a different structure that does not include these specific fields.\n\nThe semantic model referenced by definition.pbir is located in the Power BI service - No\nThe byPath property in the JSON refers to a local file path ( ../Sales.Dataset ). This indicates that the\nsemantic model is not stored in the Power BI service but is ins tead referenced locally.\nWhen the related report is opened, Power BI Desktop will open t he semantic model in full edit\nmode - Yes\nSince the semantic model is referenced by a local file path ( byPath ), Power BI Desktop can load the\nmodel in full edit mode, allowing modifications.\nThe PBIR format is used to store definitions for Power BI repor ts. The format can refer to datasets locally\n(byPath ) or in the Power BI service ( byConnection ). In this case, the byPath property indicates a local\nreference, which impacts how the report and dataset are opened and used in Power BI Desktop.", "reference": "Explanation:\ndefinition.pbir is in the PBIR-Legacy format - No\nThe JSON structure indicates a dataset reference ( datasetReference ), which aligns with the newer\nPBIR format. The PBIR-Legacy format uses a different structure that does not include these specific fields.\n\nThe semantic model referenced by definition.pbir is located in the Power BI service - No\nThe byPath property in the JSON refers to a local file path ( ../Sales.Dataset ). This indicates that the\nsemantic model is not stored in the Power BI service but is ins tead referenced locally.\nWhen the related report is opened, Power BI Desktop will open t he semantic model in full edit\nmode - Yes\nSince the semantic model is referenced by a local file path ( byPath ), Power BI Desktop can load the\nmodel in full edit mode, allowing modifications.\nThe PBIR format is used to store definitions for Power BI repor ts. The format can refer to datasets locally\n(byPath ) or in the Power BI service ( byConnection ). In this case, the byPath property indicates a local\nreference, which impacts how the report and dataset are opened and used in Power BI Desktop.", "boxes": [], "image": "images/p214.jpg"}, {"id": 169, "shown": "29", "type": "mc", "stem": "You have a Fabric tenant that contains a workspace named Worksp ace1.\nYou plan to deploy a semantic model named Model1 by using the X MLA endpoint.\nYou need to optimize the deployment of Model1. The solution mus t minimize how long it takes to deploy\nModel1.\nWhat should you do in Workspace1?", "options": [{"key": "A", "text": "Select Small semantic model storage format ."}, {"key": "B", "text": "Select Users can edit data models in the Power BI service ."}, {"key": "C", "text": "Set Enable Cache for Shortcuts to On."}, {"key": "D", "text": "Select Large semantic model storage format ."}], "correct": "D", "multi": false, "explanation": "The Large semantic model storage format is designed for handling large and complex datasets,\nimproving the efficiency of operations like loading, processing , and deploying models. It is optimized for\nscalability and performance, which helps minimize the deploymen t time for large models.", "reference": "Explanation:\nThe Large semantic model storage format is designed for handling large and complex datasets,\nimproving the efficiency of operations like loading, processing , and deploying models. It is optimized for\nscalability and performance, which helps minimize the deploymen t time for large models.", "boxes": []}, {"id": 170, "shown": "30", "type": "mc", "stem": "You have a Microsoft Power BI project that contains a semantic model.\nYou plan to use Azure DevOps for version control.You need to modify the .gitignore file to prevent the data valu es from the data sources from being pushed\nto the repository.\nWhich file should you reference?", "options": [{"key": "A", "text": "unappliedChanges.json"}, {"key": "B", "text": "cache.abf"}, {"key": "C", "text": "localSettings.json"}, {"key": "D", "text": "model.bim"}], "correct": "C", "multi": false, "explanation": "In Power BI projects, the localSettings.json file contains information specific to the local\nenvironment, such as credentials, connections, or other setting s that should not be pushed to a version\ncontrol system for security reasons.\n\nWhen using Azure DevOps (or any Git-based version control), sen sitive information like data values,\ncredentials, and configuration settings should be excluded from the repository by referencing these files in\nthe .gitignore file. The localSettings.json file is designed to hold environment-specific\nconfigurations, which often include sensitive data.", "reference": "Explanation:\nIn Power BI projects, the localSettings.json file contains information specific to the local\nenvironment, such as credentials, connections, or other setting s that should not be pushed to a version\ncontrol system for security reasons.\n\nWhen using Azure DevOps (or any Git-based version control), sen sitive information like data values,\ncredentials, and configuration settings should be excluded from the repository by referencing these files in\nthe .gitignore file. The localSettings.json file is designed to hold environment-specific\nconfigurations, which often include sensitive data.", "boxes": []}, {"id": 171, "shown": "31", "type": "mc", "stem": "You have a Fabric tenant that contains a workspace named Worksp ace1. Workspace1 uses Pro license\nmode and contains a semantic model named Model1.\nYou need to ensure that Model1 supports XMLA connections.Which setting should modify?", "options": [{"key": "A", "text": "Users can edit data models in the Power BI service"}, {"key": "B", "text": "Enforce strict access control for all data connection types"}, {"key": "C", "text": "Enable Cache for Shortcuts"}, {"key": "D", "text": "License mode"}], "correct": "D", "multi": false, "explanation": "To enable XMLA connections for a semantic model in Power BI, the workspace containing the model\nmust be set to Premium license mode . The XMLA endpoint is only supported in workspaces that are in\nPremium capacity or Premium Per User (PPU) license mode.\nSince Workspace1 is currently in Pro license mode , you need to change the license mode to either\nPremium capacity or PPU to ensure Model1 supports XMLA connecti ons.", "reference": "Explanation:\nTo enable XMLA connections for a semantic model in Power BI, the workspace containing the model\nmust be set to Premium license mode . The XMLA endpoint is only supported in workspaces that are in\nPremium capacity or Premium Per User (PPU) license mode.\nSince Workspace1 is currently in Pro license mode , you need to change the license mode to either\nPremium capacity or PPU to ensure Model1 supports XMLA connecti ons.", "boxes": []}, {"id": 172, "shown": "32", "type": "mc", "stem": "You have a Fabric tenant that contains a complex semantic model . The model is based on a star schema\nand contains many tables, including a fact table named Sales.\nYou need to visualize a diagram of the model. The diagram must contain only the Sales table and related\ntables.\nWhat should you use from Microsoft Power BI Desktop?", "options": [{"key": "A", "text": "data categories"}, {"key": "B", "text": "Data view"}, {"key": "C", "text": "Model view"}, {"key": "D", "text": "DAX query view"}], "correct": "C", "multi": false, "explanation": "The Model view in Microsoft Power BI Desktop provides a visual representation of the relationships\nbetween tables in your semantic model. It allows you to see the structure of your star schema, including\nthe Sales fact table and its related dimension tables. You can filter or focus on specific tables (like the\nSales table and its related tables) to create a simplified view.", "reference": "Explanation:\nThe Model view in Microsoft Power BI Desktop provides a visual representation of the relationships\nbetween tables in your semantic model. It allows you to see the structure of your star schema, including\nthe Sales fact table and its related dimension tables. You can filter or focus on specific tables (like the\nSales table and its related tables) to create a simplified view.", "boxes": []}, {"id": 173, "shown": "33", "type": "hotspot", "stem": "You have a Fabric tenant that contains a lakehouse named LH1.You need to deploy a new semantic model. The solution must meet  the following requirements:\nSupport complex calculated columns that include aggregate funct ions, calculated tables, and\nMultidimensional Expressions (MDX) user hierarchies.\nMinimize page rendering times.\nHow should you configure the model? To answer, select the appro priate options in the answer area.\nNOTE : Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Mode – Import\nThe Import mode allows for complex calculated columns, calculated tables, and MDX user hierarchies.\nThis mode loads the data into memory, enabling fast query perfo rmance and minimizing page rendering\ntimes.\nQuery Caching – On\nEnabling query caching improves performance by caching the resu lts of queries, reducing the time it takes\nto render pages.", "reference": "Explanation:\nMode – Import\nThe Import mode allows for complex calculated columns, calculated tables, and MDX user hierarchies.\nThis mode loads the data into memory, enabling fast query perfo rmance and minimizing page rendering\ntimes.\nQuery Caching – On\nEnabling query caching improves performance by caching the resu lts of queries, reducing the time it takes\nto render pages.", "boxes": []}, {"id": 174, "shown": "34", "type": "dragdrop", "stem": "You have a Fabric tenant that contains a semantic model. The mo del contains data about retail stores.\nYou need to write a DAX query that will be executed by using th e XMLA endpoint. The query must return a\ntable of stores that have opened since December 1, 2023.\nHow should you complete the DAX expression? To answer, drag the  appropriate values to the correct\ntargets. Each value may be used once, more than once, or not at  all. You may need to drag the split bar\nbetween panes or scroll to view content.\nNOTE:  Each correct selection is worth one point.\nSelect and Place:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: DEFINE\nDEFINEIntroduces a statement with one or more entity definitions that can be applied to one or more EVALUATE\nstatements of a DAX query.\nSyntax\n[DEFINE (\n\n (MEASURE <table name>[<measure name>] = <scalar expression >) |\n (VAR <var name> = <table or scalar expression>) | (TABLE <table name> = <table expression>) | (COLUMN <table name>[<column name>] = <scalar expression>) |\n ) + ]\n(EVALUATE <table expression>) +Box 2: EVALUATE\nEVALUATEIntroduces a statement containing a table expression required i n a DAX query.\nSyntax\nEVALUATE <table>\nBox 3: TABLE\nTable constructorReturns a table of one or more columns.\nSyntax\n{ <scalarExpr1>, <scalarExpr2>, … } { ( <scalarExpr1>, <scalarExpr2>, … ), ( <scalarExpr1>, <scalar Expr2>, … ), … }\nNote: FILTER\nReturns a table that represents a subset of another table or ex pression.\nSyntax\nFILTER(<table>,<filter>)", "reference": "Explanation:\nBox 1: DEFINE\nDEFINEIntroduces a statement with one or more entity definitions that can be applied to one or more EVALUATE\nstatements of a DAX query.\nSyntax\n[DEFINE (\n\n (MEASURE <table name>[<measure name>] = <scalar expression >) |\n (VAR <var name> = <table or scalar expression>) | (TABLE <table name> = <table expression>) | (COLUMN <table name>[<column name>] = <scalar expression>) |\n ) + ]\n(EVALUATE <table expression>) +Box 2: EVALUATE\nEVALUATEIntroduces a statement containing a table expression required i n a DAX query.\nSyntax\nEVALUATE <table>\nBox 3: TABLE\nTable constructorReturns a table of one or more columns.\nSyntax\n{ <scalarExpr1>, <scalarExpr2>, … } { ( <scalarExpr1>, <scalarExpr2>, … ), ( <scalarExpr1>, <scalar Expr2>, … ), … }\nNote: FILTER\nReturns a table that represents a subset of another table or ex pression.\nSyntax\nFILTER(<table>,<filter>)\nReference:\nhttps://learn.microsoft.com/en-us/dax/define-statement-dax\nhttps://learn.microsoft.com/en-us/dax/evaluate-statement-dax\nhttps://learn.microsoft.com/en-us/dax/table-constructor", "boxes": [{"box": 1, "value": "DEFINE"}, {"box": 2, "value": "EVALUATE"}, {"box": 3, "value": "TABLE"}], "images": ["images/p217.jpg", "images/p218.jpg"]}, {"id": 175, "shown": "35", "type": "mc", "stem": "You have a Fabric tenant that contains a semantic model. The mo del uses Direct Lake mode.\nYou suspect that some DAX queries load unnecessary columns into  memory.\nYou need to identify the frequently used columns that are loade d into memory.\nWhat are two ways to achieve the goal? Each correct answer pres ents a complete solution.\nNOTE:  Each correct answer is worth one point.\n(DMV).", "options": [{"key": "A", "text": "Use the Analyze in Excel feature."}, {"key": "B", "text": "Use the Vertipaq Analyzer tool."}, {"key": "C", "text": "Query the $System.DISCOVER_STORAGE_TABLE_COLUMN_SEGMENTS  dynamic management view"}, {"key": "D", "text": "Query the DISCOVER_MEMORYGRANT  dynamic management view (DMV)."}], "correct": "BC", "multi": true, "explanation": "", "reference": "", "boxes": []}, {"id": 176, "shown": "36", "type": "mc", "stem": "You have a Fabric tenant that contains a semantic model named M odel1. Model1 uses Import mode.\nModel1 contains a table named Orders. Orders has 100 million ro ws and the following fields.\nYou need to reduce the memory used by Model1 and the time it ta kes to refresh the model.\nWhich two actions should you perform? Each correct answer prese nts part of the solution.\nNOTE : Each correct answer is worth one point.", "options": [{"key": "A", "text": "Split OrderDateTime into separate date and time columns."}, {"key": "B", "text": "Replace TotalQuantity with a calculated column."}, {"key": "C", "text": "Convert Quantity into the Text data type."}, {"key": "D", "text": "Replace TotalSalesAmount with a measure."}], "correct": "AD", "multi": true, "explanation": "", "reference": "", "boxes": []}, {"id": 177, "shown": "37", "type": "mc", "stem": "You have a Fabric tenant that contains a warehouse.\nA user discovers that a report that usually takes two minutes t o render has been running for 45 minutes\nand has still not rendered.\nYou need to identify what is preventing the report query from c ompleting.\nWhich dynamic management view (DMV) should you use?", "options": [{"key": "A", "text": "sys.dm_exec_requests"}, {"key": "B", "text": "sys.dm_exec_sessions"}, {"key": "C", "text": "sys.dm_exec_connections"}, {"key": "D", "text": "sys.dm_pdw_exec_requests"}], "correct": "A", "multi": false, "explanation": "", "reference": "", "boxes": [], "image": "images/p220.jpg"}, {"id": 178, "shown": "38", "type": "hotspot", "stem": "You have a Microsoft Power BI report and a semantic model that uses Direct Lake mode.\nFrom Power BI Desktop, you open Performance analyzer as shown i n the following exhibit.\nUse the drop-down menus to select the answer choice that comple tes each statement based on the\ninformation presented in the graphic.\nNOTE: Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: Automatic\nThe Direct Lake fallback behavior is set to\nPower BI datasets in Direct Lake mode read delta tables directl y from OneLake — unless they have to fall\nback to DirectQuery mode. Typical fallback reasons include memo ry pressures that can prevent loading of\ncolumns required to process a DAX query, and certain features a t the data source might not support Direct\nLake mode, like SQL views in a Warehouse. In general, Direct La ke mode provides the best DAX query\nperformance unless a fallback to DirectQuery mode is necessary. Because fallback to DirectQuery mode\ncan impact DAX query performance, it's important to analyze que ry processing for a Direct Lake dataset to\nidentify if and how often fallbacks occur.\nNote: Fallback behavior\nDirect Lake models include the DirectLakeBehavior property, whi ch has three options:\nAutomatic - (Default) Specifies queries fall back to DirectQuer y mode if data can't be efficiently loaded into\nmemory.\nDirectLakeOnly - Specifies all queries use Direct Lake mode onl y. Fallback to DirectQuery mode is\ndisabled. If data can't be loaded into memory, an error is retu rned. Use this setting to determine if DAX\nqueries fail to load data into memory, forcing an error to be r eturned.\nDirectQueryOnly - Specifies all queries use DirectQuery mode on ly. Use this setting to test fallback\nperformance.\nBox 2: Direct QueryIn the Performance analyzer pane, select Refresh visuals, and t hen expand the Card visual. The card\nvisual doesn't cause any DirectQuery processing, which indicate s the dataset was able to process the\nvisual’s DAX queries in Direct Lake mode.\nIf the dataset falls back to DirectQuery mode to process the vi sual’s DAX query, you see a Direct query\nperformance metric, as shown in the following image:", "reference": "Explanation:\nBox 1: Automatic\nThe Direct Lake fallback behavior is set to\nPower BI datasets in Direct Lake mode read delta tables directl y from OneLake — unless they have to fall\nback to DirectQuery mode. Typical fallback reasons include memo ry pressures that can prevent loading of\ncolumns required to process a DAX query, and certain features a t the data source might not support Direct\nLake mode, like SQL views in a Warehouse. In general, Direct La ke mode provides the best DAX query\nperformance unless a fallback to DirectQuery mode is necessary. Because fallback to DirectQuery mode\ncan impact DAX query performance, it's important to analyze que ry processing for a Direct Lake dataset to\nidentify if and how often fallbacks occur.\nNote: Fallback behavior\nDirect Lake models include the DirectLakeBehavior property, whi ch has three options:\nAutomatic - (Default) Specifies queries fall back to DirectQuer y mode if data can't be efficiently loaded into\nmemory.\nDirectLakeOnly - Specifies all queries use Direct Lake mode onl y. Fallback to DirectQuery mode is\ndisabled. If data can't be loaded into memory, an error is retu rned. Use this setting to determine if DAX\nqueries fail to load data into memory, forcing an error to be r eturned.\nDirectQueryOnly - Specifies all queries use DirectQuery mode on ly. Use this setting to test fallback\nperformance.\nBox 2: Direct QueryIn the Performance analyzer pane, select Refresh visuals, and t hen expand the Card visual. The card\nvisual doesn't cause any DirectQuery processing, which indicate s the dataset was able to process the\nvisual’s DAX queries in Direct Lake mode.\nIf the dataset falls back to DirectQuery mode to process the vi sual’s DAX query, you see a Direct query\nperformance metric, as shown in the following image:\n\nReference:\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/directlak e-analyze-qp\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/directlak e-overview", "boxes": [{"box": 1, "value": "Automatic"}, {"box": 2, "value": "Direct QueryIn the Performance analyzer pane, select Refresh visuals, and t hen expand the Card visual. The card"}], "images": ["images/p220.jpg", "images/p221.jpg", "images/p222.jpg"]}, {"id": 179, "shown": "39", "type": "mc", "stem": "Note: This question is part of a series of questions that prese nt the same scenario. Each question\nin the series contains a unique solution that might meet the st ated goals. Some question sets\nmight have more than one correct solution, while others might n ot have a correct solution.\nAfter you answer a question in this section, you will NOT be ab le to return to it. As a result, these\nquestions will not appear in the review screen.\nYou have a Fabric tenant that contains a new semantic model in OneLake.\nYou use a Fabric notebook to read the data into a Spark DataFra me.\nYou need to evaluate the data to calculate the min, max, mean, and standard deviation values for all the\nstring and numeric columns.\nSolution: You use the following PySpark expression:\ndf.explain()\nDoes this meet the goal?", "options": [{"key": "A", "text": "Yes"}, {"key": "B", "text": "No"}], "correct": "B", "multi": false, "explanation": "Correct: You use the following PySpark expression:\ndf.summary()\nIncorrect:\n\n* df.describe().show()\n* df.show()\n* df.explain().show()\n* df.explain()\nexplain(extended=False)[source]Prints the (logical and physical) plans to the console for debu gging purpose.\nParameters: extended – boolean, default False. If False, prints only the physical plan.\n>>> df.explain()== Physical Plan ==Scan ExistingRDD[age#0,name#1]>>> df.explain(True)== Parsed Logical Plan ==...== Analyzed Logical Plan ==...== Optimized Logical Plan ==...== Physical Plan ==\nNote:\nsummary(*statistics)[source]Computes specified statistics for numeric and string columns. A vailable statistics are: - count - mean -\nstddev - min - max - arbitrary approximate percentiles specifie d as a percentage (eg, 75%)\nIf no statistics are given, this function computes count, mean, stddev, min, approximate quartiles\n(percentiles at 25%, 50%, and 75%), and max.\nNote This function is meant for exploratory data analysis, as w e make no guarantee about the backward\ncompatibility of the schema of the resulting DataFrame.>>> df.summary().show()+-------+------------------+-----+|summary| age| name|+-------+------------------+-----+| count| 2| 2|| mean| 3.5| null|| stddev|2.1213203435596424| null|| min| 2|Alice|| 25%| 2| null|| 50%| 2| null|| 75%| 5| null|| max| 5| Bob|", "reference": "Explanation:\nCorrect: You use the following PySpark expression:\ndf.summary()\nIncorrect:\n\n* df.describe().show()\n* df.show()\n* df.explain().show()\n* df.explain()\nexplain(extended=False)[source]Prints the (logical and physical) plans to the console for debu gging purpose.\nParameters: extended – boolean, default False. If False, prints only the physical plan.\n>>> df.explain()== Physical Plan ==Scan ExistingRDD[age#0,name#1]>>> df.explain(True)== Parsed Logical Plan ==...== Analyzed Logical Plan ==...== Optimized Logical Plan ==...== Physical Plan ==\nNote:\nsummary(*statistics)[source]Computes specified statistics for numeric and string columns. A vailable statistics are: - count - mean -\nstddev - min - max - arbitrary approximate percentiles specifie d as a percentage (eg, 75%)\nIf no statistics are given, this function computes count, mean, stddev, min, approximate quartiles\n(percentiles at 25%, 50%, and 75%), and max.\nNote This function is meant for exploratory data analysis, as w e make no guarantee about the backward\ncompatibility of the schema of the resulting DataFrame.>>> df.summary().show()+-------+------------------+-----+|summary| age| name|+-------+------------------+-----+| count| 2| 2|| mean| 3.5| null|| stddev|2.1213203435596424| null|| min| 2|Alice|| 25%| 2| null|| 50%| 2| null|| 75%| 5| null|| max| 5| Bob|\nReference:\nhttps://spark.apache.org/docs/2.3.0/api/python/pyspark.sql.html", "boxes": [], "image": "images/p223.jpg"}, {"id": 180, "shown": "40", "type": "mc", "stem": "Note: This question is part of a series of questions that prese nt the same scenario. Each question\nin the series contains a unique solution that might meet the st ated goals. Some question sets\nmight have more than one correct solution, while others might n ot have a correct solution.\nAfter you answer a question in this section, you will NOT be ab le to return to it. As a result, these\nquestions will not appear in the review screen.\nYou have a Fabric tenant that contains a new semantic model in OneLake.\nYou use a Fabric notebook to read the data into a Spark DataFra me.\nYou need to evaluate the data to calculate the min, max, mean, and standard deviation values for all the\nstring and numeric columns.\nSolution: You use the following PySpark expression:\ndf.show()\nDoes this meet the goal?", "options": [{"key": "A", "text": "Yes"}, {"key": "B", "text": "No"}], "correct": "B", "multi": false, "explanation": "Correct Solution: You use the following PySpark expression:\ndf.summary()\nsummary(*statistics)[source]\nComputes specified statistics for numeric and string columns. A vailable statistics are: - count - mean -\nstddev - min - max - arbitrary approximate percentiles specifie d as a percentage (eg, 75%)\nIf no statistics are given, this function computes count, mean, stddev, min, approximate quartiles\n(percentiles at 25%, 50%, and 75%), and max.\nNote This function is meant for exploratory data analysis, as w e make no guarantee about the backward\ncompatibility of the schema of the resulting DataFrame.>>> df.summary().show()+-------+------------------+-----+|summary| age| name|+-------+------------------+-----+| count| 2| 2|| mean| 3.5| null|| stddev|2.1213203435596424| null|| min| 2|Alice|| 25%| 2| null|| 50%| 2| null|| 75%| 5| null|| max| 5| Bob|\nIncorrect:\n* df.show()\n* df.explain().show()\n* df.explain()\nexplain(extended=False)[source]Prints the (logical and physical) plans to the console for debu gging purpose.\nParameters: extended – boolean, default False. If False, prints only the physical plan.\n>>> df.explain()== Physical Plan ==Scan ExistingRDD[age#0,name#1]>>> df.explain(True)== Parsed Logical Plan ==...== Analyzed Logical Plan ==...== Optimized Logical Plan ==...\n\n== Physical Plan ==", "reference": "Explanation:\nCorrect Solution: You use the following PySpark expression:\ndf.summary()\nsummary(*statistics)[source]\nComputes specified statistics for numeric and string columns. A vailable statistics are: - count - mean -\nstddev - min - max - arbitrary approximate percentiles specifie d as a percentage (eg, 75%)\nIf no statistics are given, this function computes count, mean, stddev, min, approximate quartiles\n(percentiles at 25%, 50%, and 75%), and max.\nNote This function is meant for exploratory data analysis, as w e make no guarantee about the backward\ncompatibility of the schema of the resulting DataFrame.>>> df.summary().show()+-------+------------------+-----+|summary| age| name|+-------+------------------+-----+| count| 2| 2|| mean| 3.5| null|| stddev|2.1213203435596424| null|| min| 2|Alice|| 25%| 2| null|| 50%| 2| null|| 75%| 5| null|| max| 5| Bob|\nIncorrect:\n* df.show()\n* df.explain().show()\n* df.explain()\nexplain(extended=False)[source]Prints the (logical and physical) plans to the console for debu gging purpose.\nParameters: extended – boolean, default False. If False, prints only the physical plan.\n>>> df.explain()== Physical Plan ==Scan ExistingRDD[age#0,name#1]>>> df.explain(True)== Parsed Logical Plan ==...== Analyzed Logical Plan ==...== Optimized Logical Plan ==...\n\n== Physical Plan ==\nReference:\nhttps://spark.apache.org/docs/2.3.0/api/python/pyspark.sql.html", "boxes": []}, {"id": 181, "shown": "41", "type": "mc", "stem": "Note: This question is part of a series of questions that prese nt the same scenario. Each question\nin the series contains a unique solution that might meet the st ated goals. Some question sets\nmight have more than one correct solution, while others might n ot have a correct solution.\nAfter you answer a question in this section, you will NOT be ab le to return to it. As a result, these\nquestions will not appear in the review screen.\nYou have a Fabric tenant that contains a new semantic model in OneLake.\nYou use a Fabric notebook to read the data into a Spark DataFra me.\nYou need to evaluate the data to calculate the min, max, mean, and standard deviation values for all the\nstring and numeric columns.\nSolution: You use the following PySpark expression:\ndf.summary()\nDoes this meet the goal?", "options": [{"key": "A", "text": "Yes"}, {"key": "B", "text": "No"}], "correct": "A", "multi": false, "explanation": "Correct Solution: You use the following PySpark expression:\ndf.summary()\nsummary(*statistics)[source]\nComputes specified statistics for numeric and string columns. A vailable statistics are: - count - mean -\nstddev - min - max - arbitrary approximate percentiles specifie d as a percentage (eg, 75%)\nIf no statistics are given, this function computes count, mean, stddev, min, approximate quartiles\n(percentiles at 25%, 50%, and 75%), and max.\nNote This function is meant for exploratory data analysis, as w e make no guarantee about the backward\ncompatibility of the schema of the resulting DataFrame.>>> df.summary().show()+-------+------------------+-----+|summary| age| name|+-------+------------------+-----+| count| 2| 2|| mean| 3.5| null|| stddev|2.1213203435596424| null|| min| 2|Alice|| 25%| 2| null|| 50%| 2| null|| 75%| 5| null|| max| 5| Bob|\nIncorrect:\n* df.show()\n\n* df.explain().show()\n* df.explain()\nexplain(extended=False)[source]Prints the (logical and physical) plans to the console for debu gging purpose.\nParameters: extended – boolean, default False. If False, prints only the physical plan.\n>>> df.explain()== Physical Plan ==Scan ExistingRDD[age#0,name#1]>>> df.explain(True)== Parsed Logical Plan ==...== Analyzed Logical Plan ==...== Optimized Logical Plan ==...== Physical Plan ==", "reference": "Explanation:\nCorrect Solution: You use the following PySpark expression:\ndf.summary()\nsummary(*statistics)[source]\nComputes specified statistics for numeric and string columns. A vailable statistics are: - count - mean -\nstddev - min - max - arbitrary approximate percentiles specifie d as a percentage (eg, 75%)\nIf no statistics are given, this function computes count, mean, stddev, min, approximate quartiles\n(percentiles at 25%, 50%, and 75%), and max.\nNote This function is meant for exploratory data analysis, as w e make no guarantee about the backward\ncompatibility of the schema of the resulting DataFrame.>>> df.summary().show()+-------+------------------+-----+|summary| age| name|+-------+------------------+-----+| count| 2| 2|| mean| 3.5| null|| stddev|2.1213203435596424| null|| min| 2|Alice|| 25%| 2| null|| 50%| 2| null|| 75%| 5| null|| max| 5| Bob|\nIncorrect:\n* df.show()\n\n* df.explain().show()\n* df.explain()\nexplain(extended=False)[source]Prints the (logical and physical) plans to the console for debu gging purpose.\nParameters: extended – boolean, default False. If False, prints only the physical plan.\n>>> df.explain()== Physical Plan ==Scan ExistingRDD[age#0,name#1]>>> df.explain(True)== Parsed Logical Plan ==...== Analyzed Logical Plan ==...== Optimized Logical Plan ==...== Physical Plan ==\nReference:\nhttps://spark.apache.org/docs/2.3.0/api/python/pyspark.sql.html", "boxes": []}, {"id": 182, "shown": "42", "type": "mc", "stem": "Note: This question is part of a series of questions that prese nt the same scenario. Each question\nin the series contains a unique solution that might meet the st ated goals. Some question sets\nmight have more than one correct solution, while others might n ot have a correct solution.\nAfter you answer a question in this section, you will NOT be ab le to return to it. As a result, these\nquestions will not appear in the review screen.\nYou have a Fabric tenant that contains a lakehouse named Lakeho use1. Lakehouse1 contains a Delta\ntable named Customer.\nWhen you query Customer, you discover that the query is slow to  execute. You suspect that maintenance\nwas NOT  performed on the table.\nYou need to identify whether maintenance tasks were performed o n Customer.\nSolution: You run the following Spark SQL statement:\nDESCRIBE HISTORY customer\nDoes this meet the goal?", "options": [{"key": "A", "text": "Yes"}, {"key": "B", "text": "No"}], "correct": "A", "multi": false, "explanation": "Correct Solution: You run the following Spark SQL statement:\nDESCRIBE HISTORY customer\nDESCRIBE HISTORY\nApplies to: Databricks SQL, Databricks Runtime\nReturns provenance information, including the operation, user, and so on, for each write to a table. Table\nhistory is retained for 30 days.\nSyntax\n\nDESCRIBE HISTORY table_name\nNote: Work with Delta Lake table history\nEach operation that modifies a Delta Lake table creates a new t able version. You can use history\ninformation to audit operations, rollback a table, or query a t able at a specific point in time using time travel.\nRetrieve Delta table history\nYou can retrieve information including the operations, user, an d timestamp for each write to a Delta table\nby running the history command. The operations are returned in reverse chronological order.\nDESCRIBE HISTORY '/data/events/' -- get the full histo ry of the table\nDESCRIBE HISTORY delta.`/data/events/`DESCRIBE HISTORY '/data/events/' LIMIT 1 -- get the last opera tion only\nDESCRIBE HISTORY eventsTableIncorrect:\n* DESCRIBE DETAIL customer\nDESCRIBE TABLE statement returns the basic metadata information of a table. The metadata information\nincludes column name, column type and column comment. Optionall y a partition spec or column name\nmay be specified to return the metadata pertaining to a partiti on or column respectively.\n* EXPLAIN TABLE customer\n* REFRESH TABLE\nREFRESH TABLE statement invalidates the cached entries, which i nclude data and metadata of the given\ntable or view. The invalidated cache is populated in lazy manne r when the cached table or the query\nassociated with it is executed again.\nSyntax\nREFRESH [TABLE] tableIdentifier", "reference": "Explanation:\nCorrect Solution: You run the following Spark SQL statement:\nDESCRIBE HISTORY customer\nDESCRIBE HISTORY\nApplies to: Databricks SQL, Databricks Runtime\nReturns provenance information, including the operation, user, and so on, for each write to a table. Table\nhistory is retained for 30 days.\nSyntax\n\nDESCRIBE HISTORY table_name\nNote: Work with Delta Lake table history\nEach operation that modifies a Delta Lake table creates a new t able version. You can use history\ninformation to audit operations, rollback a table, or query a t able at a specific point in time using time travel.\nRetrieve Delta table history\nYou can retrieve information including the operations, user, an d timestamp for each write to a Delta table\nby running the history command. The operations are returned in reverse chronological order.\nDESCRIBE HISTORY '/data/events/' -- get the full histo ry of the table\nDESCRIBE HISTORY delta.`/data/events/`DESCRIBE HISTORY '/data/events/' LIMIT 1 -- get the last opera tion only\nDESCRIBE HISTORY eventsTableIncorrect:\n* DESCRIBE DETAIL customer\nDESCRIBE TABLE statement returns the basic metadata information of a table. The metadata information\nincludes column name, column type and column comment. Optionall y a partition spec or column name\nmay be specified to return the metadata pertaining to a partiti on or column respectively.\n* EXPLAIN TABLE customer\n* REFRESH TABLE\nREFRESH TABLE statement invalidates the cached entries, which i nclude data and metadata of the given\ntable or view. The invalidated cache is populated in lazy manne r when the cached table or the query\nassociated with it is executed again.\nSyntax\nREFRESH [TABLE] tableIdentifier\nReference:\nhttps://learn.microsoft.com/en-us/azure/databricks/sql/language -manual/delta-describe-history\nhttps://docs.gcp.databricks.com/en/delta/history.html\nhttps://spark.apache.org/docs/3.0.0-preview/sql-ref-syntax-aux- refresh-table.html", "boxes": []}, {"id": 183, "shown": "43", "type": "mc", "stem": "Note: This question is part of a series of questions that prese nt the same scenario. Each question\nin the series contains a unique solution that might meet the st ated goals. Some question sets\nmight have more than one correct solution, while others might n ot have a correct solution.\nAfter you answer a question in this section, you will NOT be ab le to return to it. As a result, these\nquestions will not appear in the review screen.\nYou have a Fabric tenant that contains a lakehouse named Lakeho use1. Lakehouse1 contains a Delta\ntable named Customer.\nWhen you query Customer, you discover that the query is slow to  execute. You suspect that maintenance\nwas NOT  performed on the table.\nYou need to identify whether maintenance tasks were performed o n Customer.\nSolution: You run the following Spark SQL statement:\nREFRESH TABLE customer\nDoes this meet the goal?", "options": [{"key": "A", "text": "Yes"}, {"key": "B", "text": "No"}], "correct": "B", "multi": false, "explanation": "Correct Solution: You run the following Spark SQL statement:\nDESCRIBE HISTORY customer\nDESCRIBE HISTORY\nApplies to: Databricks SQL, Databricks Runtime\nReturns provenance information, including the operation, user, and so on, for each write to a table. Table\nhistory is retained for 30 days.\nSyntax\nDESCRIBE HISTORY table_name\nNote: Work with Delta Lake table history\nEach operation that modifies a Delta Lake table creates a new t able version. You can use history\ninformation to audit operations, rollback a table, or query a t able at a specific point in time using time travel.\nRetrieve Delta table history\nYou can retrieve information including the operations, user, an d timestamp for each write to a Delta table\nby running the history command. The operations are returned in reverse chronological order.\nDESCRIBE HISTORY '/data/events/' -- get the full histo ry of the table\nDESCRIBE HISTORY delta.`/data/events/`DESCRIBE HISTORY '/data/events/' LIMIT 1 -- get the last opera tion only\nDESCRIBE HISTORY eventsTableIncorrect:\n* DESCRIBE DETAIL customer\nDESCRIBE TABLE statement returns the basic metadata information of a table. The metadata information\nincludes column name, column type and column comment. Optionall y a partition spec or column name\nmay be specified to return the metadata pertaining to a partiti on or column respectively.\n* EXPLAIN TABLE customer\n* REFRESH TABLE\nREFRESH TABLE statement invalidates the cached entries, which i nclude data and metadata of the given\ntable or view. The invalidated cache is populated in lazy manne r when the cached table or the query\nassociated with it is executed again.\nSyntax\nREFRESH [TABLE] tableIdentifier", "reference": "Explanation:\nCorrect Solution: You run the following Spark SQL statement:\nDESCRIBE HISTORY customer\nDESCRIBE HISTORY\nApplies to: Databricks SQL, Databricks Runtime\nReturns provenance information, including the operation, user, and so on, for each write to a table. Table\nhistory is retained for 30 days.\nSyntax\nDESCRIBE HISTORY table_name\nNote: Work with Delta Lake table history\nEach operation that modifies a Delta Lake table creates a new t able version. You can use history\ninformation to audit operations, rollback a table, or query a t able at a specific point in time using time travel.\nRetrieve Delta table history\nYou can retrieve information including the operations, user, an d timestamp for each write to a Delta table\nby running the history command. The operations are returned in reverse chronological order.\nDESCRIBE HISTORY '/data/events/' -- get the full histo ry of the table\nDESCRIBE HISTORY delta.`/data/events/`DESCRIBE HISTORY '/data/events/' LIMIT 1 -- get the last opera tion only\nDESCRIBE HISTORY eventsTableIncorrect:\n* DESCRIBE DETAIL customer\nDESCRIBE TABLE statement returns the basic metadata information of a table. The metadata information\nincludes column name, column type and column comment. Optionall y a partition spec or column name\nmay be specified to return the metadata pertaining to a partiti on or column respectively.\n* EXPLAIN TABLE customer\n* REFRESH TABLE\nREFRESH TABLE statement invalidates the cached entries, which i nclude data and metadata of the given\ntable or view. The invalidated cache is populated in lazy manne r when the cached table or the query\nassociated with it is executed again.\nSyntax\nREFRESH [TABLE] tableIdentifier\nReference:\nhttps://learn.microsoft.com/en-us/azure/databricks/sql/language -manual/delta-describe-history\nhttps://docs.gcp.databricks.com/en/delta/history.html\nhttps://spark.apache.org/docs/3.0.0-preview/sql-ref-syntax-aux- refresh-table.html", "boxes": []}, {"id": 184, "shown": "44", "type": "mc", "stem": "Note: This question is part of a series of questions that prese nt the same scenario. Each question\nin the series contains a unique solution that might meet the st ated goals. Some question sets\nmight have more than one correct solution, while others might n ot have a correct solution.\nAfter you answer a question in this section, you will NOT be ab le to return to it. As a result, these\nquestions will not appear in the review screen.\nYou have a Fabric tenant that contains a lakehouse named Lakeho use1. Lakehouse1 contains a Delta\ntable named Customer.\nWhen you query Customer, you discover that the query is slow to  execute. You suspect that maintenance\nwas NOT  performed on the table.\nYou need to identify whether maintenance tasks were performed o n Customer.\nSolution: You run the following Spark SQL statement:\nEXPLAIN TABLE customer\nDoes this meet the goal?", "options": [{"key": "A", "text": "Yes"}, {"key": "B", "text": "No"}], "correct": "B", "multi": false, "explanation": "Correct Solution: You run the following Spark SQL statement:\nDESCRIBE HISTORY customer\nDESCRIBE HISTORY\nApplies to: Databricks SQL, Databricks Runtime\nReturns provenance information, including the operation, user, and so on, for each write to a table. Table\nhistory is retained for 30 days.\nSyntax\nDESCRIBE HISTORY table_name\nNote: Work with Delta Lake table history\nEach operation that modifies a Delta Lake table creates a new t able version. You can use history\ninformation to audit operations, rollback a table, or query a t able at a specific point in time using time travel.\nRetrieve Delta table history\nYou can retrieve information including the operations, user, an d timestamp for each write to a Delta table\nby running the history command. The operations are returned in reverse chronological order.\nDESCRIBE HISTORY '/data/events/' -- get the full histo ry of the table\nDESCRIBE HISTORY delta.`/data/events/`DESCRIBE HISTORY '/data/events/' LIMIT 1 -- get the last opera tion only\nDESCRIBE HISTORY eventsTableIncorrect:\n* DESCRIBE DETAIL customer\nDESCRIBE TABLE statement returns the basic metadata information of a table. The metadata information\nincludes column name, column type and column comment. Optionall y a partition spec or column name\nmay be specified to return the metadata pertaining to a partiti on or column respectively.\n* EXPLAIN TABLE customer\n* REFRESH TABLE\nREFRESH TABLE statement invalidates the cached entries, which i nclude data and metadata of the given\ntable or view. The invalidated cache is populated in lazy manne r when the cached table or the query\nassociated with it is executed again.\n\nSyntax\nREFRESH [TABLE] tableIdentifier", "reference": "Explanation:\nCorrect Solution: You run the following Spark SQL statement:\nDESCRIBE HISTORY customer\nDESCRIBE HISTORY\nApplies to: Databricks SQL, Databricks Runtime\nReturns provenance information, including the operation, user, and so on, for each write to a table. Table\nhistory is retained for 30 days.\nSyntax\nDESCRIBE HISTORY table_name\nNote: Work with Delta Lake table history\nEach operation that modifies a Delta Lake table creates a new t able version. You can use history\ninformation to audit operations, rollback a table, or query a t able at a specific point in time using time travel.\nRetrieve Delta table history\nYou can retrieve information including the operations, user, an d timestamp for each write to a Delta table\nby running the history command. The operations are returned in reverse chronological order.\nDESCRIBE HISTORY '/data/events/' -- get the full histo ry of the table\nDESCRIBE HISTORY delta.`/data/events/`DESCRIBE HISTORY '/data/events/' LIMIT 1 -- get the last opera tion only\nDESCRIBE HISTORY eventsTableIncorrect:\n* DESCRIBE DETAIL customer\nDESCRIBE TABLE statement returns the basic metadata information of a table. The metadata information\nincludes column name, column type and column comment. Optionall y a partition spec or column name\nmay be specified to return the metadata pertaining to a partiti on or column respectively.\n* EXPLAIN TABLE customer\n* REFRESH TABLE\nREFRESH TABLE statement invalidates the cached entries, which i nclude data and metadata of the given\ntable or view. The invalidated cache is populated in lazy manne r when the cached table or the query\nassociated with it is executed again.\n\nSyntax\nREFRESH [TABLE] tableIdentifier\nReference:\nhttps://learn.microsoft.com/en-us/azure/databricks/sql/language -manual/delta-describe-history\nhttps://docs.gcp.databricks.com/en/delta/history.html\nhttps://spark.apache.org/docs/3.0.0-preview/sql-ref-syntax-aux- refresh-table.html", "boxes": []}, {"id": 185, "shown": "45", "type": "mc", "stem": "You have a Microsoft Fabric tenant that contains a dataflow.\nYou are exploring a new semantic model.From Power Query, you need to view column information as shown in the following exhibit.\nWhich three Data view options should you select? Each correct a nswer presents part of the solution.", "options": [{"key": "A", "text": "Show column value distribution"}, {"key": "B", "text": "Enable details pane"}, {"key": "C", "text": "Enable column profile"}, {"key": "D", "text": "Show column quality details"}, {"key": "E", "text": "Show column profile in details pane"}], "correct": "ACD", "multi": true, "explanation": "", "reference": "", "boxes": [], "image": "images/p231.jpg"}, {"id": 186, "shown": "46", "type": "hotspot", "stem": "You have a Fabric tenant that contains a semantic model. The mo del contains data about retail stores.\nYou need to write a DAX query that will be executed by using th e XMLA endpoint. The query must return\nthe total amount of sales from the same period last year.\nHow should you complete the DAX expression? To answer, select t he appropriate options in the answer\narea.\nNOTE : Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "Box 1: SUMMARIZE\nSUMMARIZE Returns a summary table for the requested totals over a set of groups.\nSyntax\nSUMMARIZE (<table>, <groupBy_columnName>[, <groupBy_columnName> ]…[, <name>, <expression>]\n…)\nNote: EVALUATE is a DAX statement that is needed to execute a q uery. EVALUATE followed by any table\nexpression returns the result of the table expression. Moreover , one or more EVALUATE statements can\nbe preceded by special definitions like local tables, columns, measures, and variables that have the scope\nof the entire batch of EVALUATE statements executed together.\n-- Definition section\n\n[DEFINE { MEASURE <tableName>[<name>] = <expression> }\n { COLUMN <tableName>[<name>] = <expression> } { TABLE <tableName> = <expression> } { VAR <name> = <expression>}]-- Query expressionEVALUATE <table> -- Result modifiers[ORDER BY {<expression> [{ASC | DESC}]}[, …] [START AT {<value>|<parameter>} [, …]]]\nNote 2: Example\nSAMEPERIODLASTYEARReturns a table that contains a column of dates shifted one yea r back in time from the dates in the\nspecified dates column, in the current context.\nSyntax\nSAMEPERIODLASTYEAR(<dates>)\nParameters\ndatesA column containing dates.\nReturn value\nA single-column table of date values.\nThe following sample formula creates a measure that calculates the previous year sales of Reseller sales.\n= CALCULATE(SUM(ResellerSales_USD[SalesAmount_USD]), SAMEPERIOD LASTYEAR(DateTime\n[DateKey]))\nBox 2: _LYSales\nJust return the variable.\nNote: You can define a variable in any DAX expression by using VAR followed by RETURN. In one or\nseveral VAR sections, you individually declare the variables ne eded to compute the expression; in the\nRETURN part you provide the expression itself.\nIncorrect:* FILTER\nReturns a table that represents a subset of another table or ex pression.\nSyntax\nFILTER(<table>,<filter>) tableThe table to be filtered. The table can also be an expression t hat results in a table.\nFilter\nA Boolean expression that is to be evaluated for each row of th e table. For example, [Amount] > 0 or\n[Region] = \"France\"\n* SUMMARIZECOLUMNS\nReturns a summary table over a set of groups.\nSyntax\nSUMMARIZECOLUMNS( <groupBy_columnName> [, < groupBy_columnName >]…, [<filterTable>]…[,\n<name>, <expression>]…)\n* CALCULATETABLE\nEvaluates a table expression in a modified filter context.\n Note\nThere's also the CALCULATE function. It performs exactly the sa me functionality, except it modifies the\nfilter context applied to an expression that returns a scalar v alue.\n\nSyntax\nCALCULATETABLE(<expression>[, <filter1> [, <filter2> [, …]]])", "reference": "Explanation:\nBox 1: SUMMARIZE\nSUMMARIZE Returns a summary table for the requested totals over a set of groups.\nSyntax\nSUMMARIZE (<table>, <groupBy_columnName>[, <groupBy_columnName> ]…[, <name>, <expression>]\n…)\nNote: EVALUATE is a DAX statement that is needed to execute a q uery. EVALUATE followed by any table\nexpression returns the result of the table expression. Moreover , one or more EVALUATE statements can\nbe preceded by special definitions like local tables, columns, measures, and variables that have the scope\nof the entire batch of EVALUATE statements executed together.\n-- Definition section\n\n[DEFINE { MEASURE <tableName>[<name>] = <expression> }\n { COLUMN <tableName>[<name>] = <expression> } { TABLE <tableName> = <expression> } { VAR <name> = <expression>}]-- Query expressionEVALUATE <table> -- Result modifiers[ORDER BY {<expression> [{ASC | DESC}]}[, …] [START AT {<value>|<parameter>} [, …]]]\nNote 2: Example\nSAMEPERIODLASTYEARReturns a table that contains a column of dates shifted one yea r back in time from the dates in the\nspecified dates column, in the current context.\nSyntax\nSAMEPERIODLASTYEAR(<dates>)\nParameters\ndatesA column containing dates.\nReturn value\nA single-column table of date values.\nThe following sample formula creates a measure that calculates the previous year sales of Reseller sales.\n= CALCULATE(SUM(ResellerSales_USD[SalesAmount_USD]), SAMEPERIOD LASTYEAR(DateTime\n[DateKey]))\nBox 2: _LYSales\nJust return the variable.\nNote: You can define a variable in any DAX expression by using VAR followed by RETURN. In one or\nseveral VAR sections, you individually declare the variables ne eded to compute the expression; in the\nRETURN part you provide the expression itself.\nIncorrect:* FILTER\nReturns a table that represents a subset of another table or ex pression.\nSyntax\nFILTER(<table>,<filter>) tableThe table to be filtered. The table can also be an expression t hat results in a table.\nFilter\nA Boolean expression that is to be evaluated for each row of th e table. For example, [Amount] > 0 or\n[Region] = \"France\"\n* SUMMARIZECOLUMNS\nReturns a summary table over a set of groups.\nSyntax\nSUMMARIZECOLUMNS( <groupBy_columnName> [, < groupBy_columnName >]…, [<filterTable>]…[,\n<name>, <expression>]…)\n* CALCULATETABLE\nEvaluates a table expression in a modified filter context.\n Note\nThere's also the CALCULATE function. It performs exactly the sa me functionality, except it modifies the\nfilter context applied to an expression that returns a scalar v alue.\n\nSyntax\nCALCULATETABLE(<expression>[, <filter1> [, <filter2> [, …]]])\nReference:\nhttps://www.sqlbi.com/articles/variables-in-dax\nhttps://dax.guide/st/evaluate/\nhttps://learn.microsoft.com/en-us/dax/summarize-function-dax", "boxes": [{"box": 1, "value": "SUMMARIZE"}, {"box": 2, "value": "_LYSales"}], "images": ["images/p231.jpg", "images/p232.jpg"]}, {"id": 187, "shown": "47", "type": "mc", "stem": "Note: This question is part of a series of questions that prese nt the same scenario. Each question\nin the series contains a unique solution that might meet the st ated goals. Some question sets\nmight have more than one correct solution, while others might n ot have a correct solution.\nAfter you answer a question in this section, you will NOT be ab le to return to it. As a result, these\nquestions will not appear in the review screen.\nYou have a Fabric tenant that contains a new semantic model in OneLake.\nYou use a Fabric notebook to read the data into a Spark DataFra me.\nYou need to evaluate the data to calculate the min, max, mean, and standard deviation values for all the\nstring and numeric columns.\nSolution: You use the following PySpark expression:\ndf.explain().show()\nDoes this meet the goal?", "options": [{"key": "A", "text": "Yes"}, {"key": "B", "text": "No"}], "correct": "B", "multi": false, "explanation": "Correct Solution: You use the following PySpark expression:\ndf.summary()\nsummary(*statistics)[source]\nComputes specified statistics for numeric and string columns. A vailable statistics are: - count - mean -\nstddev - min - max - arbitrary approximate percentiles specifie d as a percentage (eg, 75%)\nIf no statistics are given, this function computes count, mean, stddev, min, approximate quartiles\n(percentiles at 25%, 50%, and 75%), and max.\nNote This function is meant for exploratory data analysis, as w e make no guarantee about the backward\ncompatibility of the schema of the resulting DataFrame.>>> df.summary().show()+-------+------------------+-----+|summary| age| name|+-------+------------------+-----+| count| 2| 2|| mean| 3.5| null|| stddev|2.1213203435596424| null|| min| 2|Alice|| 25%| 2| null|| 50%| 2| null|| 75%| 5| null|| max| 5| Bob|\n\nIncorrect:\n* df.show()\n* df.explain().show()\n* df.explain()\nexplain(extended=False)[source]Prints the (logical and physical) plans to the console for debu gging purpose.\nParameters: extended – boolean, default False. If False, prints only the physical plan.\n>>> df.explain()== Physical Plan ==Scan ExistingRDD[age#0,name#1]>>> df.explain(True)== Parsed Logical Plan ==...== Analyzed Logical Plan ==...== Optimized Logical Plan ==...== Physical Plan ==", "reference": "Explanation:\nCorrect Solution: You use the following PySpark expression:\ndf.summary()\nsummary(*statistics)[source]\nComputes specified statistics for numeric and string columns. A vailable statistics are: - count - mean -\nstddev - min - max - arbitrary approximate percentiles specifie d as a percentage (eg, 75%)\nIf no statistics are given, this function computes count, mean, stddev, min, approximate quartiles\n(percentiles at 25%, 50%, and 75%), and max.\nNote This function is meant for exploratory data analysis, as w e make no guarantee about the backward\ncompatibility of the schema of the resulting DataFrame.>>> df.summary().show()+-------+------------------+-----+|summary| age| name|+-------+------------------+-----+| count| 2| 2|| mean| 3.5| null|| stddev|2.1213203435596424| null|| min| 2|Alice|| 25%| 2| null|| 50%| 2| null|| 75%| 5| null|| max| 5| Bob|\n\nIncorrect:\n* df.show()\n* df.explain().show()\n* df.explain()\nexplain(extended=False)[source]Prints the (logical and physical) plans to the console for debu gging purpose.\nParameters: extended – boolean, default False. If False, prints only the physical plan.\n>>> df.explain()== Physical Plan ==Scan ExistingRDD[age#0,name#1]>>> df.explain(True)== Parsed Logical Plan ==...== Analyzed Logical Plan ==...== Optimized Logical Plan ==...== Physical Plan ==\nReference:\nhttps://spark.apache.org/docs/2.3.0/api/python/pyspark.sql.html", "boxes": []}, {"id": 188, "shown": "48", "type": "mc", "stem": "Note: This question is part of a series of questions that prese nt the same scenario. Each question\nin the series contains a unique solution that might meet the st ated goals. Some question sets\nmight have more than one correct solution, while others might n ot have a correct solution.\nAfter you answer a question in this section, you will NOT be ab le to return to it. As a result, these\nquestions will not appear in the review screen.\nYou have a Fabric tenant that contains a new semantic model in OneLake.\nYou use a Fabric notebook to read the data into a Spark DataFra me.\nYou need to evaluate the data to calculate the min, max, mean, and standard deviation values for all the\nstring and numeric columns.\nSolution: You use the following PySpark expression:\ndf.describe().show()\nDoes this meet the goal?", "options": [{"key": "A", "text": "Yes"}, {"key": "B", "text": "No"}], "correct": "A", "multi": false, "explanation": "", "reference": "", "boxes": []}, {"id": 189, "shown": "49", "type": "mc", "stem": "You have a Fabric tenant that contains a workspace named Worksp ace1. Workspace1 contains a single\nsemantic model that has two Microsoft Power BI reports.\nYou have a Microsoft 365 subscription that contains a data loss  prevention (DLP) policy named DLP1.\nYou need to apply DLP1 to the items in Workspace1.\nWhat should you do?", "options": [{"key": "A", "text": "Create a workspace identity."}, {"key": "B", "text": "Apply a certified endorsement to the semantic model."}, {"key": "C", "text": "Apply sensitivity labels to the semantic model and reports."}, {"key": "D", "text": "Apply a master data endorsement to the semantic model."}], "correct": "C", "multi": false, "explanation": "Data loss prevention for Power BI\nMicrosoft Purview DLP policies for Power BI\nA DLP policy for Power BI is set up in the Microsoft Purview co mpliance portal. It can detect sensitive data\nin a semantic model that's been published to a Premium workspac e in the Power BI service.\nType of sensitive data\nA DLP policy for Power BI that's set up in the Microsoft Purvie w compliance portal can be based on either\na sensitivity label or a sensitive information type.\nSensitivity label\nYou can use sensitivity labels to classify content, ranging fro m less sensitive to more sensitive.\nWhen a DLP policy for Power BI is invoked, a sensitivity label rule checks semantic models (that are\npublished to the Power BI service) for the presence of a certai n sensitivity label.", "reference": "Explanation:\nData loss prevention for Power BI\nMicrosoft Purview DLP policies for Power BI\nA DLP policy for Power BI is set up in the Microsoft Purview co mpliance portal. It can detect sensitive data\nin a semantic model that's been published to a Premium workspac e in the Power BI service.\nType of sensitive data\nA DLP policy for Power BI that's set up in the Microsoft Purvie w compliance portal can be based on either\na sensitivity label or a sensitive information type.\nSensitivity label\nYou can use sensitivity labels to classify content, ranging fro m less sensitive to more sensitive.\nWhen a DLP policy for Power BI is invoked, a sensitivity label rule checks semantic models (that are\npublished to the Power BI service) for the presence of a certai n sensitivity label.\nReference:\nhttps://learn.microsoft.com/en-us/power-bi/guidance/powerbi-imp lementation-planning-data-loss-\nprevention", "boxes": []}, {"id": 190, "shown": "50", "type": "mc", "stem": "You have a Microsoft Power BI semantic model that contains a me asure named TotalSalesAmount.\nTotalSalesAmount returns a sales revenue amount that is transla ted into a selected currency.\nYou need to ensure that the value returned by TotalSalesAmount is formatted to use the correct currency\nsymbol.\nWhat should you include in the solution?", "options": [{"key": "A", "text": "a field parameter"}, {"key": "B", "text": "a linguistic schema"}, {"key": "C", "text": "a dynamic format string"}, {"key": "D", "text": "the WINDOW DAX  function"}], "correct": "C", "multi": false, "explanation": "Power BI, Create dynamic format strings for measuresWith dynamic format strings for measures, you can determine how measures appear in visuals by\nconditionally applying a format string with a separate DAX expr ession.\nA common scenario with financial reports is showing the currenc y converted to other country currencies by\nmultiplying the base currency by an exchange rate. This also ta kes advantage of the dynamic format\nstrings of calculation items to show the converted currency als o in the correct currency format. Calculation\nitems can have the conversion DAX expression defined once that can be applied to the many measures in\n\nthe model showing currency values, without the need to edit or duplicate many measures.", "reference": "Explanation:\nPower BI, Create dynamic format strings for measuresWith dynamic format strings for measures, you can determine how measures appear in visuals by\nconditionally applying a format string with a separate DAX expr ession.\nA common scenario with financial reports is showing the currenc y converted to other country currencies by\nmultiplying the base currency by an exchange rate. This also ta kes advantage of the dynamic format\nstrings of calculation items to show the converted currency als o in the correct currency format. Calculation\nitems can have the conversion DAX expression defined once that can be applied to the many measures in\n\nthe model showing currency values, without the need to edit or duplicate many measures.\nReference:\nhttps://learn.microsoft.com/en-us/power-bi/create-reports/deskt op-dynamic-format-strings\nhttps://powerbi.microsoft.com/en-us/blog/deep-dive-into-the-mod el-explorer-with-calculation-group-\nauthoring-and-creating-relationships-in-the-properties-pane/", "boxes": []}, {"id": 191, "shown": "51", "type": "dragdrop", "stem": "You have a Fabric tenant that contains a workspace named Worksp ace1. Workspace1 uses the Pro\nlicense mode and contains a semantic model named Model1.\nYou have an Azure DevOps organization.You need to enable version control for Workspace1. The solution  must ensure that Model1 is added to the\nrepository.\nWhich three actions should you perform in sequence? To answer, move the appropriate actions from the\nlist of actions to the answer area and arrange them in the corr ect order.\nSelect and Place:", "options": [], "correct": "", "multi": false, "explanation": "Step 1: Assign Workspace1 to a Fabric capacity\nFabric prerequisitesTo access the Git integration feature, you need a Fabric capaci ty.\nStep 2: Connect Workspace1 to a Git provider.\nConnect a workspace to a Git repoConnect to a Git repoOnly a workspace admin can connect a workspace to a repository, but once connected, anyone with\npermission can work in the workspace. If you're not an admin, a sk your admin for help with connecting. To\nconnect a workspace to an Azure or GitHub Repo, follow these st eps:\n1. Sign into Fabric and navigate to the workspace you want to c onnect with.\n2. Go to Workspace settings3. Select Git integration.4. Select your Git provider. Currently, Azure DevOps and GitHub are supported.\nStep 3: Sync Workspace1 with the repository.\nConnect to a workspace\nWith Azure Devops branch connect.1. From the dropdown menu, specify the following details about the branch you want to connect to.\nOrganizationProjectGit repository.Branch (Select an existing branch using the drop-down menu, or select + New Branch to create a new\nbranch. You can only connect to one branch at a time.)Folder (Type in the name of an existing folder or enter a name to create a new folder. If you leave the\nfolder name blank, content will be created in the root folder. You can only connect to one folder at a time.)\n\n2. Select Connect and sync.\nDuring the initial sync, if either the workspace or Git branch is empty, content is copied from the nonempty\nlocation to the empty one. If both the workspace and Git branch have content, you’re asked which direction\nthe sync should go.\nAfter you connect, the Workspace displays information about sou rce control that allows the user to view\nthe connected branch, the status of each item in the branch and the time of the last sync.", "reference": "Explanation:\nStep 1: Assign Workspace1 to a Fabric capacity\nFabric prerequisitesTo access the Git integration feature, you need a Fabric capaci ty.\nStep 2: Connect Workspace1 to a Git provider.\nConnect a workspace to a Git repoConnect to a Git repoOnly a workspace admin can connect a workspace to a repository, but once connected, anyone with\npermission can work in the workspace. If you're not an admin, a sk your admin for help with connecting. To\nconnect a workspace to an Azure or GitHub Repo, follow these st eps:\n1. Sign into Fabric and navigate to the workspace you want to c onnect with.\n2. Go to Workspace settings3. Select Git integration.4. Select your Git provider. Currently, Azure DevOps and GitHub are supported.\nStep 3: Sync Workspace1 with the repository.\nConnect to a workspace\nWith Azure Devops branch connect.1. From the dropdown menu, specify the following details about the branch you want to connect to.\nOrganizationProjectGit repository.Branch (Select an existing branch using the drop-down menu, or select + New Branch to create a new\nbranch. You can only connect to one branch at a time.)Folder (Type in the name of an existing folder or enter a name to create a new folder. If you leave the\nfolder name blank, content will be created in the root folder. You can only connect to one folder at a time.)\n\n2. Select Connect and sync.\nDuring the initial sync, if either the workspace or Git branch is empty, content is copied from the nonempty\nlocation to the empty one. If both the workspace and Git branch have content, you’re asked which direction\nthe sync should go.\nAfter you connect, the Workspace displays information about sou rce control that allows the user to view\nthe connected branch, the status of each item in the branch and the time of the last sync.\nReference:\nhttps://learn.microsoft.com/en-us/fabric/cicd/git-integration/g it-get-started?", "boxes": [], "images": ["images/p237.jpg", "images/p239.jpg"]}, {"id": 192, "shown": "52", "type": "mc", "stem": "You have a Fabric workspace named Workspace1.\nWorkspace1 contains multiple semantic models, including a model  named Model1. Model1 is updated by\nusing an XMLA endpoint.\nYou need to increase the speed of the write operations of the X MLA endpoint.\nWhat should you do?", "options": [{"key": "A", "text": "Delete any unused semantic models from Workspace1."}, {"key": "B", "text": "Select Large semantic model storage format for Workspace1."}, {"key": "C", "text": "Configure Model 1 to use the Direct Lake storage format."}, {"key": "D", "text": "Delete any unused columns from Model1."}], "correct": "C", "multi": false, "explanation": "The Direct Lake storage format in Microsoft Fabric allows semantic models to r ead data directly from\nOneLake without requiring data movement or import. This significantly improves the performance of write\noperations when updating a model via the XMLA endpoint by eliminating the need for data duplication or\ntransformation.", "reference": "Explanation:\nThe Direct Lake storage format in Microsoft Fabric allows semantic models to r ead data directly from\nOneLake without requiring data movement or import. This significantly improves the performance of write\noperations when updating a model via the XMLA endpoint by eliminating the need for data duplication or\ntransformation.", "boxes": [], "image": "images/p240.jpg"}, {"id": 193, "shown": "53", "type": "mc", "stem": "You have a Fabric workspace named Workspace1.\nYou need to create a semantic model named Model1 and publish Mo del1 to Workspace1. The solution\nmust meet the following requirements:\nCan revert to previous versions of Model1 as required.\nIdentifies differences between saved versions of Model1.\nUses Microsoft Power BI Desktop to publish to Workspace1.\nCan edit item definition files by using Microsoft Visual Studio  Code.\nWhich two actions should you perform? Each correct answer prese nts part of the solution.\nNOTE : Each correct selection is worth one point.", "options": [{"key": "A", "text": "Enable Git integration for Workspace1."}, {"key": "B", "text": "Save Model1 in Power BI Desktop as a PBIT file."}, {"key": "C", "text": "Enable users to edit data models in the Power BI service."}, {"key": "D", "text": "Save Model1 in Power BI Desktop as a PBIP file."}], "correct": "AD", "multi": true, "explanation": "Enable Git integration for Workspace1\nGit integration allows version control , enabling users to revert to previous versions and identify\ndifferences between saved versions of the semantic model.\nSave Model1 in Power BI Desktop as a PBIP file\nA PBIP (Power BI Project) file is a folder-based format that allows for direct editing of item definition\nfiles in Microsoft Visual Studio Code .\nPBIP files are better suited for collaborative development and version control , making them the\npreferred choice for source control systems like Git.", "reference": "Explanation:\nEnable Git integration for Workspace1\nGit integration allows version control , enabling users to revert to previous versions and identify\ndifferences between saved versions of the semantic model.\nSave Model1 in Power BI Desktop as a PBIP file\nA PBIP (Power BI Project) file is a folder-based format that allows for direct editing of item definition\nfiles in Microsoft Visual Studio Code .\nPBIP files are better suited for collaborative development and version control , making them the\npreferred choice for source control systems like Git.", "boxes": [], "image": "images/p240.jpg"}, {"id": 194, "shown": "54", "type": "mc", "stem": "You have a Fabric workspace named Workspace1 that is assigned t o a newly created Fabric capacity\nnamed Capacity1.\nYou create a semantic model named Model1 and deploy Model1 to W orkspace1.\nYou need to publish changes to Model1 directly from Tabular Edi tor.\nWhat should you do?", "options": [{"key": "A", "text": "For Workspace1, enable Git integration."}, {"key": "B", "text": "For Model1, enable external sharing."}, {"key": "C", "text": "For Workspace1, create a managed private endpoint."}, {"key": "D", "text": "For Capacity1, set XMLA Endpoint to Read Write."}], "correct": "D", "multi": false, "explanation": "To publish changes to Model1 directly from Tabular Editor , the XMLA Endpoint must be set to Read\nWrite in Fabric Capacity settings . This allows external tools like Tabular Editor , SSMS , and Power BI\nALM Toolkit to connect, modify, and publish changes to the semantic model.", "reference": "Explanation:\nTo publish changes to Model1 directly from Tabular Editor , the XMLA Endpoint must be set to Read\nWrite in Fabric Capacity settings . This allows external tools like Tabular Editor , SSMS , and Power BI\nALM Toolkit to connect, modify, and publish changes to the semantic model.", "boxes": [], "image": "images/p241.jpg"}, {"id": 195, "shown": "55", "type": "hotspot", "stem": "You are creating a report and a semantic model in Microsoft Pow er BI Desktop.\nThe Value measure has the expression shown in the following exh ibit.\nUse the drop-down menus to select the answer choice that comple tes each statement based on the\ninformation presented in the graphic.\nNOTE : Each correct selection is worth one point.\nHot Area:", "options": [], "correct": "", "multi": false, "explanation": "A dynamic format string was added to the Value measure.\nThe Value measure can return values formatted as percentages or whole numbers.\nDynamic Format String:\nThe DAX formula uses SWITCH(SELECTEDVALUE(Metric[Metric]), … ) to dynamically change the\nformat of the Value measure. This is a dynamic format string , which allows the measure to return either\nwhole numbers ( #) or percentages ( 0.00% ), depending on the selected metric.\nFormatted as Percentages or Whole Numbers:\nThe formula specifies that if the metric is \"# of Customers\" , the output is formatted as a whole number\n(#,### ). If the metric is \"Gross Margin %\" , the output is formatted as a percentage ( 0.00% ). Since the\nmeasure can return both whole numbers and percentages, the corr ect answer is \"percentages or whole\nnumbers.\"", "reference": "Explanation:\nA dynamic format string was added to the Value measure.\nThe Value measure can return values formatted as percentages or whole numbers.\nDynamic Format String:\nThe DAX formula uses SWITCH(SELECTEDVALUE(Metric[Metric]), … ) to dynamically change the\nformat of the Value measure. This is a dynamic format string , which allows the measure to return either\nwhole numbers ( #) or percentages ( 0.00% ), depending on the selected metric.\nFormatted as Percentages or Whole Numbers:\nThe formula specifies that if the metric is \"# of Customers\" , the output is formatted as a whole number\n(#,### ). If the metric is \"Gross Margin %\" , the output is formatted as a percentage ( 0.00% ). Since the\nmeasure can return both whole numbers and percentages, the corr ect answer is \"percentages or whole\nnumbers.\"", "boxes": [], "image": "images/p241.jpg"}, {"id": 196, "shown": "56", "type": "mc", "stem": "You have a Microsoft Power BI semantic model.\nYou need to identify any surrogate key columns in the model tha t have the Summarize By property set to a\nvalue other than to None. The solution must minimize effort.\nWhat should you use?", "options": [{"key": "A", "text": "Performance Analyzer in Power BI Desktop"}, {"key": "B", "text": "Model explorer in Microsoft Power BI Desktop"}, {"key": "C", "text": "Model view in Microsoft Power BI Desktop"}, {"key": "D", "text": "DAX Formatter in DAX Studio"}], "correct": "C", "multi": false, "explanation": "Correct:\n* Best Practice Analyzer in Tabular Editor [Best]\n* Model view in Microsoft Power BI Desktop [Next best, choose o nly if BPA not an option]\nIncorrect:\n* DAX Formatter in DAX Studio\n* Model explorer in Microsoft Power BI Desktop\n* Performance Analyzer in Power BI Desktop\nNote:\nTo identify surrogate key columns with \"Summarize By\" set to a value other than \"None\" in a Power BI\nsemantic model, you can use the BPA rules or manually check the \"Summarize By\" property of surrogate\nkeys in the Power BI Desktop model view.\n* Best Practice Analyzer in Tabular Editor\nBPA lets you define rules on the metadata of your model, to enc ourage certain conventions and best\npractices while developing in SSAS Tabular.\nClicking one of the rules in the top list, will show you all ob jects that satisfy the conditions of the given rule\nin the bottom list:\nNote: The Best Practice Analyzer (BPA) lets you define rules on the metadata of your model, to encourage\ncertain conventions and best practices while developing your Po wer BI or Analysis Services Model.\nPBA Overview\nThe BPA overview shows you all the rules defined in your model that are currently being broken:\n\nIncorrect:\n* DAX Formatter in DAX Studio\nDAX Formatter can be used within DAX Studio to align parenthese s with their associated functions.\n* Model explorer in Microsoft Power BI Desktop\nWith Model explorer in the Model view in Power BI, you can view and work with complex semantic models\nwith many tables, relationships, measures, roles, calculation g roups, translations, and perspectives.\n* Model view in Microsoft Power BI Desktop\nModel view shows all of the tables, columns, and relationships in your model. This view can be especially\nhelpful when your model has complex relationships between many tables.", "reference": "Explanation:\nCorrect:\n* Best Practice Analyzer in Tabular Editor [Best]\n* Model view in Microsoft Power BI Desktop [Next best, choose o nly if BPA not an option]\nIncorrect:\n* DAX Formatter in DAX Studio\n* Model explorer in Microsoft Power BI Desktop\n* Performance Analyzer in Power BI Desktop\nNote:\nTo identify surrogate key columns with \"Summarize By\" set to a value other than \"None\" in a Power BI\nsemantic model, you can use the BPA rules or manually check the \"Summarize By\" property of surrogate\nkeys in the Power BI Desktop model view.\n* Best Practice Analyzer in Tabular Editor\nBPA lets you define rules on the metadata of your model, to enc ourage certain conventions and best\npractices while developing in SSAS Tabular.\nClicking one of the rules in the top list, will show you all ob jects that satisfy the conditions of the given rule\nin the bottom list:\nNote: The Best Practice Analyzer (BPA) lets you define rules on the metadata of your model, to encourage\ncertain conventions and best practices while developing your Po wer BI or Analysis Services Model.\nPBA Overview\nThe BPA overview shows you all the rules defined in your model that are currently being broken:\n\nIncorrect:\n* DAX Formatter in DAX Studio\nDAX Formatter can be used within DAX Studio to align parenthese s with their associated functions.\n* Model explorer in Microsoft Power BI Desktop\nWith Model explorer in the Model view in Power BI, you can view and work with complex semantic models\nwith many tables, relationships, measures, roles, calculation g roups, translations, and perspectives.\n* Model view in Microsoft Power BI Desktop\nModel view shows all of the tables, columns, and relationships in your model. This view can be especially\nhelpful when your model has complex relationships between many tables.\n\nReference:\nhttps://docs.tabulareditor.com/te2/Best-Practice-Analyzer.html\nhttps://docs.tabulareditor.com/common/using-bpa.html?tabs=TE3Ru les\nhttps://learn.microsoft.com/en-us/power-bi/transform-model/mode l-explorer", "boxes": [], "images": ["images/p242.jpg", "images/p243.jpg", "images/p244.jpg"]}, {"id": 197, "shown": "57", "type": "mc", "stem": "You have a semantic model named Model1 that contains data that relates to customers and their bank\naccount balances.\nModel1 has the following tables and columns.\nA customer can have one or more accounts. Each account can be a ssociated to multiple customers.\nYou need to ensure that users can query Model1 to identify the total transaction amounts by customer.\nWhat should you add to Model1?", "options": [{"key": "A", "text": "a many-to-many relationship between FactTransaction and Dim C ustomer"}, {"key": "B", "text": "a bridge table with relationships to DimCustomer and DimAccou nt"}, {"key": "C", "text": "a bridge table with relationships to FactTransaction and DimC ustomer"}, {"key": "D", "text": "the CustomerKey column in FactTransaction and a relationship to DimCustomer"}], "correct": "B", "multi": false, "explanation": "We need to define a many-to-many relation between DimAccount an d DimCustomer. This can be done\nusing a bridge table.\nNote: Relate many-to-many dimensions\nThe classic many-to-many scenario relates two entities, for exa mple bank customers and bank accounts.\nConsider that customers can have multiple accounts, and account s can have multiple customers. When an\naccount has multiple customers, they're commonly called joint a ccount holders.\nModeling these entities is straight forward. One dimension tabl e stores accounts, and another dimension\ntable stores customers. As is characteristic of dimension table s, there's a unique identifier (ID) column in\neach table. To model the relationship between the two tables, a third table is required. This table is\ncommonly referred to as a bridging table. In this example, it's purpose is to store one row for each\ncustomer-account association.", "reference": "Explanation:\nWe need to define a many-to-many relation between DimAccount an d DimCustomer. This can be done\nusing a bridge table.\nNote: Relate many-to-many dimensions\nThe classic many-to-many scenario relates two entities, for exa mple bank customers and bank accounts.\nConsider that customers can have multiple accounts, and account s can have multiple customers. When an\naccount has multiple customers, they're commonly called joint a ccount holders.\nModeling these entities is straight forward. One dimension tabl e stores accounts, and another dimension\ntable stores customers. As is characteristic of dimension table s, there's a unique identifier (ID) column in\neach table. To model the relationship between the two tables, a third table is required. This table is\ncommonly referred to as a bridging table. In this example, it's purpose is to store one row for each\ncustomer-account association.\nReference:\nhttps://learn.microsoft.com/en-us/power-bi/guidance/relationshi ps-many-to-many", "boxes": [], "image": "images/p245.jpg"}, {"id": 198, "shown": "58", "type": "mc", "stem": "You have a Fabric tenant that contains 30 CSV files in OneLake.  The files are updated daily.\nYou create a Microsoft Power BI semantic model named Model1 tha t uses the CSV files as a data source.\nYou configure incremental refresh for Model1 and publish the mo del to an F64 capacity in the Fabric\ntenant.\nWhen you initiate a refresh of Model1, the refresh fails after running out of resources.\nWhat is a possible cause of the failure?", "options": [{"key": "A", "text": "XMLA Endpoint is set to Read Only ."}, {"key": "B", "text": "Query folding is occurring."}, {"key": "C", "text": "The data type of the column used to partition the data has ch anged."}, {"key": "D", "text": "Only refresh complete days  is selected."}, {"key": "E", "text": "Query folding is NOT  occurring."}], "correct": "E", "multi": false, "explanation": "Incremental refresh and real-time data for semantic models, Tro ubleshoot incremental refresh and real-\ntime data\nD (not B): Most problems that occur when configuring incrementa l refresh and real-time data have to do\nwith query folding. Your data source must support query folding .\nIf the incremental refresh policy includes getting real-time da ta with DirectQuery, non-folding\ntransformations can't be used.\nBecause support for query folding is different for different ty pes of data sources, verification should be\nperformed to ensure the filter logic is included in the queries being run against the data source.\nNote: Cause: Data type mismatch\nThis issue can be caused by a data type mismatch where Date/Tim e is the required data type for the\n\nRangeStart and RangeEnd parameters, but the table date column o n which the filters are applied aren't\nDate/Time data type, or vice-versa. Both the parameters data ty pe and the filtered data column must be\nDate/Time data type and the format must be the same. If not, th e query can't be folded.\nIncorrect:\nNot D: The Only refresh complete days setting ensures all rows for the entire day are included in the\nrefresh operation.", "reference": "Explanation:\nIncremental refresh and real-time data for semantic models, Tro ubleshoot incremental refresh and real-\ntime data\nD (not B): Most problems that occur when configuring incrementa l refresh and real-time data have to do\nwith query folding. Your data source must support query folding .\nIf the incremental refresh policy includes getting real-time da ta with DirectQuery, non-folding\ntransformations can't be used.\nBecause support for query folding is different for different ty pes of data sources, verification should be\nperformed to ensure the filter logic is included in the queries being run against the data source.\nNote: Cause: Data type mismatch\nThis issue can be caused by a data type mismatch where Date/Tim e is the required data type for the\n\nRangeStart and RangeEnd parameters, but the table date column o n which the filters are applied aren't\nDate/Time data type, or vice-versa. Both the parameters data ty pe and the filtered data column must be\nDate/Time data type and the format must be the same. If not, th e query can't be folded.\nIncorrect:\nNot D: The Only refresh complete days setting ensures all rows for the entire day are included in the\nrefresh operation.\nReference:\nhttps://learn.microsoft.com/en-us/power-bi/connect-data/increme ntal-refresh-troubleshoot\nhttps://learn.microsoft.com/en-us/power-bi/connect-data/increme ntal-refresh-overview", "boxes": []}];
let idx = 0;
let learnMode = false;
let timeLeft = 120 * 60;
const answers = {};

function pad(n) { return String(n).padStart(2,'0'); }

function startTimer() {
  setInterval(() => {
    if (timeLeft <= 0) return;
    timeLeft--;
    const m = Math.floor(timeLeft/60), s = timeLeft % 60;
    document.getElementById('timer').innerText = `${pad(m)}:${pad(s)}`;
    if (timeLeft === 0) alert("时间到！你可以选择提交统计。");
  }, 1000);
}

function isAnswered(q) {
  const a = answers[q.id];
  if (!a) return false;
  if (q.type === 'mc') return Array.isArray(a.choice) ? a.choice.length>0 : !!a.choice;
  if (a.boxes) return Object.values(a.boxes).some(v => (v||'').trim().length>0);
  if (a.text) return (a.text||'').trim().length>0;
  return false;
}

function updateProgress() {
  const answered = QUESTIONS.filter(q => isAnswered(q)).length;
  document.getElementById('progressText').innerText = `${answered}/${QUESTIONS.length}`;
  document.getElementById('progressBar').style.width = `${Math.round(answered/QUESTIONS.length*100)}%`;
}

function saveCurrent() {
  const q = QUESTIONS[idx];
  if (!q) return;
  if (q.type === 'mc') {
    const inputs = document.querySelectorAll('input[name="opt"]');
    const picked = [];
    inputs.forEach(el => { if (el.checked) picked.push(el.value); });
    answers[q.id] = { choice: q.multi ? picked : (picked[0] || "") };
  } else {
    const boxes = {};
    document.querySelectorAll('[data-box]').forEach(el => { boxes[el.getAttribute('data-box')] = el.value || ""; });
    const free = document.getElementById('freeText');
    answers[q.id] = { boxes, text: free ? (free.value||"") : "" };
  }
  updateProgress();
}

function openImage(src){ try{ window.open(src,'_blank'); }catch(e){} }

function render() {
  const q = QUESTIONS[idx];
  const card = document.getElementById('questionCard');
  const answeredTag = isAnswered(q) ? "✅已答" : "⬜未答";

  let h = `
    <div class="row" style="justify-content:space-between; align-items:center;">
      <div>
        <h3 class="qtitle">第 ${idx+1} 题 <span class="meta">（原标号：QUESTION ${q.shown} · ${q.type.toUpperCase()}）</span></h3>
        <div class="meta">${answeredTag}</div>
      </div>
      <div class="meta">共 ${QUESTIONS.length} 题</div>
    </div>
    <hr/>
    <div class="stem">${escapeHtml(q.stem || "")}</div>
  `;

  // 如果题目有配图（如 Hotspot/DragDrop 的 answer area、exhibit 等），在题干下方展示
  if (q.image) {
    h += `<div class="qimg-wrap"><img class="qimg" src="${q.image}" alt="题目图片" onclick="openImage(this.src)"></div>`;
  }
  if (Array.isArray(q.images) && q.images.length) {
    q.images.forEach(src => {
      h += `<div class="qimg-wrap"><img class="qimg" src="${src}" alt="题目图片" onclick="openImage(this.src)"></div>`;
    });
  }


  if (q.type === 'mc' && q.options && q.options.length) {
    h += `<div class="options">`;
    const a = answers[q.id]?.choice;
    const chosen = new Set(Array.isArray(a) ? a : [a]);
    const inputType = q.multi ? "checkbox" : "radio";
    q.options.forEach(opt => {
      const checked = chosen.has(opt.key) ? "checked" : "";
      h += `
        <label class="opt">
          <input type="${inputType}" name="opt" value="${opt.key}" ${checked}>
          <div><b>${opt.key}.</b> ${escapeHtml(opt.text)}</div>
        </label>`;
    });
    h += `</div>`;
  } else {
    const hasBoxes = q.boxes && q.boxes.length;
    const saved = answers[q.id] || {boxes:{}, text:""};
    if (hasBoxes) {
      h += `<div style="margin-top:12px" class="grid two">`;
      const uniqueBoxes = [...new Set(q.boxes.map(b => b.box))].sort((a,b)=>a-b);
      uniqueBoxes.forEach(bn => {
        const v = (saved.boxes && saved.boxes[String(bn)]) ? saved.boxes[String(bn)] : "";
        const presets = ['Yes','No','Viewer','Contributor','Member','Admin'];
        const isCustom = v && !presets.includes(v);
        h += `
          <div>
            <div class="meta">Box ${bn} 你的答案</div>
            <select data-box="${bn}">
              <option value="" ${v==="" ? "selected":""}>（未选择）</option>
              <option value="Yes" ${v==="Yes"?"selected":""}>Yes</option>
              <option value="No" ${v==="No"?"selected":""}>No</option>
              <option value="Viewer" ${v==="Viewer"?"selected":""}>Viewer</option>
              <option value="Contributor" ${v==="Contributor"?"selected":""}>Contributor</option>
              <option value="Member" ${v==="Member"?"selected":""}>Member</option>
              <option value="Admin" ${v==="Admin"?"selected":""}>Admin</option>
              <option value="__custom__">自定义...</option>
            </select>
            <input type="text" placeholder="自定义答案（可选）" data-box="${bn}" style="margin-top:8px; display:${isCustom?'block':'none'}" value="${escapeAttr(isCustom?v:'')}">
          </div>`;
      });
      h += `</div>`;
    }
    h += `
      <div style="margin-top:12px">
        <div class="meta">补充/备注（可选）</div>
        <textarea id="freeText" placeholder="写下你的思路/答案（可选）">${escapeHtml(saved.text || "")}</textarea>
      </div>
    `;
  }

  const hasBoxAnswers = q.boxes && q.boxes.length;
  const boxMap = new Map();
  if (hasBoxAnswers) q.boxes.forEach(b => { if(!boxMap.has(b.box)) boxMap.set(b.box, b.value); });
  const boxHtml = hasBoxAnswers ? `
    <div class="meta" style="margin-top:6px">Box 答案（来自解析）：</div>
    <div class="grid two" style="margin-top:6px">
      ${[...boxMap.entries()].sort((a,b)=>a[0]-b[0]).map(([bn,val]) => `
        <div class="card" style="padding:10px">
          <div class="meta">Box ${bn}</div>
          <div><b>${escapeHtml(val)}</b></div>
        </div>
      `).join('')}
    </div>` : "";

  h += `
    <div class="learn" id="learnPanel">
      <hr/>
      <div class="answer">正确答案：${escapeHtml(q.correct || "（本题未提供明确字母答案/请参考 Box 或解析）")}</div>
      ${boxHtml}
      <div class="meta" style="margin-top:10px">解析：</div>
      <div class="explain">${escapeHtml(q.explanation || "（无）")}</div>
      <div class="meta" style="margin-top:10px">Reference：</div>
      <div class="ref">${escapeHtml(q.reference || "（无）")}</div>
    </div>
  `;

  card.innerHTML = h;
  document.getElementById('learnPanel').style.display = learnMode ? 'block' : 'none';

  document.querySelectorAll('input[name="opt"]').forEach(el => el.addEventListener('change', () => saveCurrent()));
  const free = document.getElementById('freeText');
  if (free) free.addEventListener('input', () => saveCurrent());

  document.querySelectorAll('select[data-box]').forEach(sel => {
    const txt = sel.nextElementSibling;
    sel.addEventListener('change', () => {
      if (sel.value === '__custom__') {
        sel.value = '';
        if (txt) txt.style.display = 'block';
      } else {
        if (txt) { txt.style.display = 'none'; txt.value = ''; }
      }
      saveCurrent();
    });
    if (txt) {
      txt.addEventListener('input', () => saveCurrent());
      txt.addEventListener('blur', () => saveCurrent());
    }
  });

  document.getElementById('prevBtn').disabled = (idx===0);
  document.getElementById('nextBtn').disabled = (idx===QUESTIONS.length-1);
  updateProgress();
}

function nextQ() { saveCurrent(); if (idx < QUESTIONS.length-1) { idx++; render(); window.scrollTo(0,0); } }
function prevQ() { saveCurrent(); if (idx > 0) { idx--; render(); window.scrollTo(0,0); } }

function jumpPrompt() {
  saveCurrent();
  const val = prompt("输入要跳转的题号（1-" + QUESTIONS.length + "）：");
  if (!val) return;
  const n = parseInt(val,10);
  if (Number.isFinite(n) && n>=1 && n<=QUESTIONS.length) { idx = n-1; render(); window.scrollTo(0,0); }
  else alert("题号无效");
}

function submitExam() {
  saveCurrent();
  const answered = QUESTIONS.filter(q => isAnswered(q)).length;
  alert(`已作答 ${answered} 题，未作答 ${QUESTIONS.length-answered} 题`);
}

function toggleLearn() {
  learnMode = !learnMode;
  document.getElementById('learnBtn').innerText = "学习模式：" + (learnMode ? "开启" : "关闭");
  const panel = document.getElementById('learnPanel');
  if (panel) panel.style.display = learnMode ? 'block' : 'none';
}

function escapeHtml(str) { return (str||"").replaceAll("&","&amp;").replaceAll("<","&lt;").replaceAll(">","&gt;"); }
function escapeAttr(str) { return (str||"").replaceAll("&","&amp;").replaceAll('"',"&quot;").replaceAll("<","&lt;").replaceAll(">","&gt;"); }

startTimer();
render();
</script>
</body>
</html>
